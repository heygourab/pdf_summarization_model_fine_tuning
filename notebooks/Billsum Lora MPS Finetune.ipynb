{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b37ddabf",
   "metadata": {},
   "source": [
    "# Fine-tuning Flan-T5-base for Legal Document Summarization\n",
    "\n",
    "This notebook is a modified version of the original notebook by Gourab S. (@heygourab).\n",
    "Author: Gourab S. (@heygourab)\n",
    "This notebook demonstrates fine-tuning the Flan-T5-base model on the BillSum dataset using LoRA (Low-Rank Adaptation). We'll use the Hugging Face ecosystem (`transformers`, `datasets`, `peft`) for efficient fine-tuning.\n",
    "\n",
    "## Setup Overview\n",
    "\n",
    "- Base Model: google/flan-t5-base\n",
    "- Dataset: BillSum (~2000 samples)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "This notebook assumes a Colab environment with a GPU available. If you're running this locally, make sure to install the required packages and set up your GPU environment accordingly.\n",
    "[Open in Colab](https://colab.research.google.com/github/heygourab/pdf_summarization_model_fine_tuning/blob/main/notebooks/billsum_lora_finetune_colab.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e4d2ae",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, let's install the required dependencies and set up GPU monitoring.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b6b71a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# M1/M2 Mac Setup for PyTorch with MPS (Metal GPU)\n",
    "%pip install -q torch torchvision torchaudio\n",
    "%pip install -q transformers datasets accelerate evaluate peft nltk wandb omegaconf fsspec pyarrow\n",
    "# Note: bitsandbytes is not supported on M1/M2 Mac with MPS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37f3264a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS (Metal Performance Shaders) is available! Using MPS for acceleration.\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# ðŸ”¥ Device setup for M1/M2 (Metal Performance Shaders)\n",
    "import torch\n",
    "\n",
    "def get_device():\n",
    "    if torch.backends.mps.is_available():\n",
    "        print(\"MPS (Metal Performance Shaders) is available! Using MPS for acceleration.\")\n",
    "        return torch.device('mps')\n",
    "    elif torch.cuda.is_available():\n",
    "        print(\"CUDA is available! Using CUDA for acceleration.\")\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        print(\"No GPU acceleration available. Using CPU.\")\n",
    "        return torch.device('cpu')\n",
    "\n",
    "device = get_device()\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4a9ea0",
   "metadata": {},
   "source": [
    "## 2. Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a8c4ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import nltk\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import sys\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType\n",
    ")\n",
    "import wandb\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba69578d",
   "metadata": {},
   "source": [
    "## 3. Logger setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59c4567e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-20 12:42:41 â€” train_logger â€” INFO â€” Logger initialized: train_logger\n",
      "2025-05-20 12:42:41 â€” train_logger â€” INFO â€” Log file created at: /Users/gourabsarkar/Developer/college_project/pdf_summarization_model_fine_tuning/notebooks/logs/training_20250520_124241.log\n",
      "2025-05-20 12:42:41 â€” train_logger â€” INFO â€” Python version: 3.10.17 (main, Apr  8 2025, 12:10:59) [Clang 16.0.0 (clang-1600.0.26.6)]\n",
      "2025-05-20 12:42:41 â€” train_logger â€” INFO â€” Log file created at: /Users/gourabsarkar/Developer/college_project/pdf_summarization_model_fine_tuning/notebooks/logs/training_20250520_124241.log\n",
      "2025-05-20 12:42:41 â€” train_logger â€” INFO â€” Python version: 3.10.17 (main, Apr  8 2025, 12:10:59) [Clang 16.0.0 (clang-1600.0.26.6)]\n"
     ]
    }
   ],
   "source": [
    "def setup_logger(name=\"train_logger\", level=logging.INFO, log_file=None):\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level)\n",
    "    logger.propagate = False  # Avoid duplicate logs\n",
    "\n",
    "    # Clear existing handlers\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "\n",
    "    # Formatter for log messages\n",
    "    formatter = logging.Formatter(\n",
    "        fmt='%(asctime)s â€” %(name)s â€” %(levelname)s â€” %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S'\n",
    "    )\n",
    "\n",
    "    # Console handler setup\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setFormatter(formatter)\n",
    "    logger.addHandler(console_handler)\n",
    "\n",
    "    # File handler setup\n",
    "    if log_file is None:\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        log_dir = os.path.join(os.getcwd(), 'logs')  # Safe fallback to current dir\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        log_file = os.path.join(log_dir, f'training_{timestamp}.log')\n",
    "    else:\n",
    "        log_dir = os.path.dirname(log_file)\n",
    "        if log_dir:\n",
    "            os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    file_handler = logging.FileHandler(log_file)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "    # Log header\n",
    "    logger.info(f\"Logger initialized: {name}\")\n",
    "    logger.info(f\"Log file created at: {os.path.abspath(log_file)}\")\n",
    "    logger.info(f\"Python version: {sys.version}\")\n",
    "\n",
    "    return logger\n",
    "\n",
    "# Use it\n",
    "logger = setup_logger(\"train_logger\", logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712d79dc",
   "metadata": {},
   "source": [
    "## 4. Loading the required NLTK libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68fbb34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-20 12:42:42 â€” train_logger â€” INFO â€” Successfully downloaded NLTK resource: punkt\n",
      "2025-05-20 12:42:42 â€” train_logger â€” INFO â€” Successfully downloaded NLTK resource: punkt_tab\n",
      "2025-05-20 12:42:42 â€” train_logger â€” INFO â€” Successfully downloaded NLTK resource: punkt_tab\n"
     ]
    }
   ],
   "source": [
    "# Download required NLTK data\n",
    "for resource in ['punkt', 'punkt_tab']:\n",
    "    try:\n",
    "        nltk.download(resource, quiet=True)\n",
    "        logger.info(f\"Successfully downloaded NLTK resource: {resource}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error downloading {resource}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d615513",
   "metadata": {},
   "source": [
    "## 5. Memory Usage Monitoring\n",
    "\n",
    "The `print_memory_usage()` function monitors system resource utilization during model training:\n",
    "\n",
    "- Tracks RAM usage by getting the Resident Set Size (RSS) of current process in GB\n",
    "- For GPU-enabled systems:\n",
    "  - Reports allocated GPU memory\n",
    "  - Shows total available GPU memory\n",
    "  - Calculates percentage of GPU memory utilization\n",
    "  - Resets peak memory tracking statistics\n",
    "\n",
    "This helps identify potential memory bottlenecks and optimize resource usage during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "393f44a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-20 12:42:42 â€” train_logger â€” INFO â€” RAM usage: 0.46 GB\n",
      "2025-05-20 12:42:42 â€” train_logger â€” INFO â€” Total system RAM: 8.59 GB\n",
      "2025-05-20 12:42:42 â€” train_logger â€” INFO â€” Using MPS (Metal Performance Shaders) - Memory stats not available\n",
      "2025-05-20 12:42:42 â€” train_logger â€” INFO â€” Total system RAM: 8.59 GB\n",
      "2025-05-20 12:42:42 â€” train_logger â€” INFO â€” Using MPS (Metal Performance Shaders) - Memory stats not available\n"
     ]
    }
   ],
   "source": [
    "def print_memory_usage():\n",
    "    process = psutil.Process(os.getpid()) # Get the current process\n",
    "\n",
    "    ram_gb = process.memory_info().rss / 1e9 # Convert bytes to GB\n",
    "    total_gb = psutil.virtual_memory().total / 1e9 # Total system RAM in GB\n",
    "\n",
    "    logger.info(f\"RAM usage: {ram_gb:.2f} GB\") # Current process RAM usage\n",
    "    logger.info(f\"Total system RAM: {total_gb:.2f} GB\") # Total system RAM\n",
    "\n",
    "    # Check for CUDA device\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "        gpu_mem = torch.cuda.memory_allocated() / 1e9\n",
    "        gpu_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        peak_gpu_mem = torch.cuda.max_memory_allocated() / 1e9\n",
    "\n",
    "        logger.info(f\"GPU memory usage: {gpu_mem:.2f}/{gpu_total:.2f} GB ({gpu_mem/gpu_total*100:.1f}%)\")\n",
    "        logger.info(f\"Peak GPU memory: {peak_gpu_mem:.2f} GB\")\n",
    "\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    # Check for MPS device\n",
    "    elif torch.backends.mps.is_available():\n",
    "        # MPS doesn't have built-in memory tracking like CUDA\n",
    "        # We can only log that we're using MPS\n",
    "        logger.info(\"Using MPS (Metal Performance Shaders) - Memory stats not available\")\n",
    "\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9070a4ca",
   "metadata": {},
   "source": [
    "## 6. Configuration Parameters\n",
    "\n",
    "all hyperparameters and configuration settings for the model, dataset, LoRA, and training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "580b1886",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import TaskType \n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "CONFIG = {\n",
    "    # ====== MODEL & ARCHITECTURE ======\n",
    "    \"model_name\": \"google/flan-t5-base\",  # Lightweight encoder-decoder model\n",
    "    \"model_type\": \"encoder-decoder\",       # Needed for proper Trainer setup\n",
    "\n",
    "    # ====== DATASET CONFIG ======\n",
    "    \"dataset_name\": \"billsum\",\n",
    "    \"text_col\": \"text\",\n",
    "    \"summary_col\": \"summary\",\n",
    "    \"max_input_tokens\": 512,\n",
    "    \"max_target_tokens\": 256,\n",
    "\n",
    "    \"sample_size\": 2000,  # Increased sample size to stabilize training\n",
    "    \"filter_by_length\": True,\n",
    "    \"split_train_frac\": 0.9,  # Give model more data to learn from\n",
    "\n",
    "    # ====== PROMPT INJECTION ======\n",
    "    \"prompt_prefix\": \"Summarize the following legal bill: \",\n",
    "\n",
    "    # ====== LoRA CONFIG ======\n",
    "    \"lora_r\": 8,  # Reduced rank for MPS stability\n",
    "    \"lora_alpha\": 16,  # Lower alpha to reduce overfitting\n",
    "    \"lora_target_modules\": [\"q\", \"k\", \"v\"],  # Cover more layers (T5 specifics)\n",
    "    \"lora_dropout\": 0.1,  # Slightly increased to help generalization\n",
    "    \"lora_bias\": \"none\",\n",
    "    \"lora_task_type\": TaskType.SEQ_2_SEQ_LM,\n",
    "\n",
    "    # ====== TRAINING CONFIG ======\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": True,\n",
    "    \"num_train_epochs\": 3,  # 3-5 epochs is safe for small datasets\n",
    "    \"per_device_train_batch_size\": 1,  # Lower batch size for stability on MPS\n",
    "    \"per_device_eval_batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 8,  # Effective batch size = 8\n",
    "\n",
    "    \"learning_rate\": 1e-4,  # Reduced LR to avoid erratic weight updates\n",
    "    \"weight_decay\": 0.1,   # Lower decay to avoid underfitting\n",
    "    \"warmup_steps\": 20,     # Faster warmup for small dataset\n",
    "\n",
    "    \"fp16\": False,  # MPS doesnâ€™t support it\n",
    "    \"bf16\": False,\n",
    "    \"torch_compile\": False,  # Not stable on MPS\n",
    "    \"gradient_checkpointing\": True,  # Saves memory\n",
    "\n",
    "    \"optim\": \"adamw_torch\",  # MPS-safe optimizer\n",
    "\n",
    "    # ====== EVALUATION & LOGGING ======\n",
    "    \"logging_steps\": 10,\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"eval_steps\": 50,  # ðŸ”¥ Slightly slower eval to reduce jitter\n",
    "    \"save_strategy\": \"steps\",\n",
    "    \"save_steps\": 100,\n",
    "    \"save_total_limit\": 1,\n",
    "\n",
    "    \"load_best_model_at_end\": True,\n",
    "    \"metric_for_best_model\": \"rougeL\",  # Logical for summarization\n",
    "    \"greater_is_better\": True,\n",
    "    \"report_to\": \"wandb\",  # Replace with \"none\" if not using WandB\n",
    "    \"overwrite_output_dir\": True,\n",
    "\n",
    "    # ====== GENERATION CONFIG ======\n",
    "    \"gen_num_beams\": 4,         # Slightly cheaper beam search\n",
    "    \"gen_length_penalty\": 0.8,\n",
    "    \"gen_early_stopping\": True,\n",
    "\n",
    "    # ====== MISC ======\n",
    "    \"seed\": 42,\n",
    "\n",
    "    # ====== GOOGLE DRIVE SUPPORT (COLAB) ======\n",
    "    \"mount_drive\": False,\n",
    "    \"drive_path\": \"MyDrive/ML_models/pdf_summarization\",\n",
    "    \"training_report_filename\": \"training_report.json\",\n",
    "    \"lora_adapter_name\": \"lora_billsum_legal\",\n",
    "}\n",
    "\n",
    "# ====== OUTPUT DIR HANDLER ======\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "CONFIG[\"output_dir\"] = f\"{CONFIG['lora_adapter_name']}_{timestamp}\"\n",
    "\n",
    "if CONFIG[\"mount_drive\"]:\n",
    "    CONFIG[\"gdrive_output_dir\"] = f\"/content/drive/{CONFIG['drive_path']}/{CONFIG['output_dir']}\"\n",
    "\n",
    "# ====== SANITY LOG ======\n",
    "logger.info(\"ðŸ”§ MPS CONFIG DUMP:\")\n",
    "for k, v in CONFIG.items():\n",
    "    logger.info(f\"  {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a0224a",
   "metadata": {},
   "source": [
    "## 6. Login to Hugging Face Hub and Weights & Biases\n",
    "\n",
    "You'll need to log in to Hugging Face to download models/datasets and to Weights & Biases for experiment tracking.\n",
    "You can get your Hugging Face token from [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
    "and your W&B API key from [https://wandb.ai/authorize](https://wandb.ai/authorize).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49076cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgoheygourab\u001b[0m (\u001b[33mgoheygourab-self\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/gourabsarkar/Developer/college_project/pdf_summarization_model_fine_tuning/notebooks/wandb/run-20250520_124243-vp2zd3fz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/goheygourab-self/flan-t5-billsum-lora/runs/vp2zd3fz' target=\"_blank\">run_20250520_124243</a></strong> to <a href='https://wandb.ai/goheygourab-self/flan-t5-billsum-lora' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/goheygourab-self/flan-t5-billsum-lora' target=\"_blank\">https://wandb.ai/goheygourab-self/flan-t5-billsum-lora</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/goheygourab-self/flan-t5-billsum-lora/runs/vp2zd3fz' target=\"_blank\">https://wandb.ai/goheygourab-self/flan-t5-billsum-lora/runs/vp2zd3fz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import HfFolder, notebook_login\n",
    "\n",
    "try:\n",
    "    if HfFolder.get_token() is None:\n",
    "        logger.info(\"Hugging Face token not found. Please log in.\")\n",
    "        notebook_login()\n",
    "    else:\n",
    "        logger.info(\"Already logged in to Hugging Face Hub.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"An error occurred during Hugging Face login check: {e}\")\n",
    "    logger.info(\"Attempting login...\")\n",
    "    notebook_login()\n",
    "\n",
    "# Login to Weights & Biases\n",
    "try:\n",
    "    wandb.login()\n",
    "    # Use a different run name than output_dir\n",
    "    run_name = f\"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    wandb.init(\n",
    "        project=\"flan-t5-billsum-lora\", \n",
    "        name=run_name,  # Use different run name\n",
    "        config=CONFIG\n",
    "    )\n",
    "    logger.info(\"Successfully logged in to W&B and initialized experiment.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not login to W&B: {e}. Ensure you have run `wandb login` or set WANDB_API_KEY.\")\n",
    "    CONFIG[\"report_to\"] = \"tensorboard\" # Fallback to tensorboard\n",
    "    logger.info(\"Falling back to TensorBoard for logging.\")\n",
    "    # No explicit init for tensorboard here, Trainer handles it via TrainingArguments\n",
    "# Note: Use environment variables or notebook secrets to store your tokens securely\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e96255",
   "metadata": {},
   "source": [
    "## 7. Mount Google Drive (Optional)\n",
    "\n",
    "If you want to save your model checkpoints and outputs to Google Drive, mount it here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faa3daab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_colab():\n",
    "    \"\"\"Check if the current environment is Google Colab.\"\"\"\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "        \n",
    "if is_colab() and CONFIG[\"mount_drive\"]:\n",
    "    from google.colab import drive\n",
    "    try:\n",
    "        drive.mount('/content/drive')\n",
    "        logger.info(\"Google Drive mounted successfully.\")\n",
    "        # Create the output directory on Drive if it doesn't exist\n",
    "        if not os.path.exists(CONFIG[\"gdrive_output_dir\"]):\n",
    "            os.makedirs(CONFIG[\"gdrive_output_dir\"], exist_ok=True)\n",
    "            logger.info(f\"Created Google Drive output directory: {CONFIG['gdrive_output_dir']}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to mount Google Drive: {e}\")\n",
    "        logger.info(\"Proceeding without Google Drive. Outputs will be saved to Colab ephemeral storage.\")\n",
    "        CONFIG[\"mount_drive\"] = False # Disable drive features if mount fails\n",
    "else:\n",
    "    logger.info(\"Not running in Colab or Google Drive is disabled in the configuration.\")\n",
    "    CONFIG[\"mount_drive\"] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559ed30a",
   "metadata": {},
   "source": [
    "## 8. Load Model, Tokenizer and Configure LoRA\n",
    "\n",
    "Loads the Flan-T5-base model and tokenizer from Hugging Face, configures the model for LoRA training and returns the model and tokenizer objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfe80f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gourabsarkar/Developer/college_project/pdf_summarization_model_fine_tuning/.venv/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,327,104 || all params: 248,904,960 || trainable%: 0.5332\n"
     ]
    }
   ],
   "source": [
    "# Install latest bitsandbytes and required dependencies\n",
    "%pip install -U bitsandbytes --no-cache-dir -q\n",
    "%pip install accelerate --upgrade -q\n",
    "%pip install transformers --upgrade -q\n",
    "\n",
    "# Don't install bitsandbytes on Mac M1 as it's not compatible with MPS\n",
    "\n",
    "# Import required libraries\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, BitsAndBytesConfig\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "# Print versions and available devices \n",
    "def check_environment():\n",
    "    logger.info(f\"PyTorch version: {torch.__version__}\")\n",
    "    logger.info(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
    "    logger.info(f\"MPS built: {torch.backends.mps.is_built()}\")\n",
    "    logger.info(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "check_environment()\n",
    "\n",
    "# Function to verify CUDA and bitsandbytes setup\n",
    "def verify_installation():\n",
    "    logger.info(f\"PyTorch version: {torch.__version__}\")\n",
    "    logger.info(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    logger.info(f\"bitsandbytes version: {bnb.__version__}\")\n",
    "    \n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"CUDA is not available. 4-bit quantization requires a CUDA-enabled GPU.\")\n",
    "    \n",
    "    # Test BitsAndBytes CUDA kernels\n",
    "    try:\n",
    "        _ = bnb.matmul(torch.zeros(2, 2).cuda(), torch.zeros(2, 2).cuda())\n",
    "        logger.info(\"BitsAndBytes CUDA kernels working correctly\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"BitsAndBytes CUDA test failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Verify installation\n",
    "# verify_installation()\n",
    "\n",
    "# Load tokenizer\n",
    "logger.info(f\"Loading tokenizer for model: {CONFIG['model_name']}\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        CONFIG[\"model_name\"],\n",
    "        use_fast=True,\n",
    "        padding_side=\"right\",\n",
    "        model_max_length=CONFIG[\"max_input_tokens\"]\n",
    "    )\n",
    "    logger.info(\"Tokenizer loaded successfully\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load tokenizer: {e}\")\n",
    "    raise\n",
    "\n",
    "# Load model for MPS\n",
    "logger.info(f\"Loading base model: {CONFIG['model_name']} for MPS...\")\n",
    "try:\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        CONFIG[\"model_name\"],\n",
    "        device_map=None,  # Don't use auto device mapping on MPS\n",
    "        torch_dtype=torch.float32,  # Use FP32 on MPS for better compatibility\n",
    "        use_cache=False  # Disable KV cache to avoid past_key_values warning\n",
    "    )\n",
    "    # Move model to MPS device after loading\n",
    "    if torch.backends.mps.is_available():\n",
    "        model = model.to(device)\n",
    "    logger.info(f\"Model loaded successfully and moved to {device}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load model: {e}\")\n",
    "    raise\n",
    "\n",
    "# Prepare model for training with proper error handling\n",
    "logger.info(\"Configuring model for LoRA training...\")\n",
    "try:\n",
    "    # Configure LoRA for the model\n",
    "    lora_config = LoraConfig(\n",
    "        r=CONFIG[\"lora_r\"],\n",
    "        lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "        target_modules=CONFIG[\"lora_target_modules\"],\n",
    "        lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "        bias=CONFIG[\"lora_bias\"],\n",
    "        task_type=CONFIG[\"lora_task_type\"],\n",
    "    )\n",
    "    \n",
    "    # Enable gradients before applying LoRA\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    # Explicitly verify trainable parameters\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    logger.info(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")\n",
    "    \n",
    "    logger.info(\"LoRA adapter applied successfully\")\n",
    "    model.print_trainable_parameters()\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to apply LoRA adapter: {e}\")\n",
    "    raise\n",
    "\n",
    "# Show memory usage\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b15b44bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSeq2SeqLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): T5ForConditionalGeneration(\n",
       "      (shared): Embedding(32128, 768)\n",
       "      (encoder): T5Stack(\n",
       "        (embed_tokens): Embedding(32128, 768)\n",
       "        (block): ModuleList(\n",
       "          (0): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (k): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (v): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (relative_attention_bias): Embedding(32, 12)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1-11): 11 x T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (k): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (v): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): T5LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (decoder): T5Stack(\n",
       "        (embed_tokens): Embedding(32128, 768)\n",
       "        (block): ModuleList(\n",
       "          (0): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (k): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (v): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (relative_attention_bias): Embedding(32, 12)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (k): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (v): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1-11): 11 x T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (k): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (v): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (k): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (v): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): T5LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9673f4b4",
   "metadata": {},
   "source": [
    "## 9. Load and Preprocess Dataset\n",
    "\n",
    "Load the BillSum dataset, preprocess it for Flan-T5, and split into training and evaluation sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd47db1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    text = text.strip()\n",
    "    text = \" \".join(text.split())\n",
    "    # Only strip metadata if required\n",
    "    text = re.sub(r'\\s*\\([^\\)]{0,40}\\)\\s*', ' ', text)  # remove very short inlines\n",
    "    text = re.sub(r'\\s*\\[[^\\]]{0,40}\\]\\s*', ' ', text)\n",
    "    return text\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    try:\n",
    "        input_texts = examples.get(CONFIG[\"text_col\"], [])\n",
    "        summary_texts = examples.get(CONFIG[\"summary_col\"], [])\n",
    "\n",
    "        if not input_texts:\n",
    "            raise ValueError(\"Empty input text\")\n",
    "\n",
    "        cleaned_inputs = [clean_text(doc) for doc in input_texts]\n",
    "\n",
    "        prompts = [f'{CONFIG[\"prompt_prefix\"]}{doc}' for doc in cleaned_inputs]\n",
    "\n",
    "        print(f\"Raw: {input_texts[0][:150]}...\")\n",
    "        print(f\"Cleaned: {cleaned_inputs[0][:150]}...\")\n",
    "\n",
    "        model_inputs = tokenizer(\n",
    "            prompts,\n",
    "            max_length=CONFIG[\"max_input_tokens\"] - 32,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        # Use plain summaries â€” no extra formatting\n",
    "        summaries = [s if s else \"No summary provided.\" for s in summary_texts]\n",
    "\n",
    "        labels = tokenizer(\n",
    "            summaries,\n",
    "            max_length=CONFIG[\"max_target_tokens\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Preprocessing failed: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b54f4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to update some libraries.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Library update attempts finished. If issues persist, ensure runtime was restarted after updates.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07784aaca01047f6834d77c9a338cf3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw: SECTION 1. SHORT TITLE.\n",
      "\n",
      "    This Act may be cited as the ``Pentagon 9/11 Memorial Commemorative \n",
      "Coin Act of 2005''.\n",
      "\n",
      "SEC. 2. FINDINGS.\n",
      "\n",
      "    The Cong...\n",
      "Cleaned: SECTION 1. SHORT TITLE. This Act may be cited as the ``Pentagon 9/11 Memorial Commemorative Coin Act of 2005''. SEC. 2. FINDINGS. The Congress finds a...\n",
      "Raw: SECTION 1. SHORT TITLE.\n",
      "\n",
      "    This Act may be cited as the ``Children's Hope Act of 2003''.\n",
      "\n",
      "SEC. 2. TAX CREDIT FOR CONTRIBUTIONS TO EDUCATION INVESTME...\n",
      "Cleaned: SECTION 1. SHORT TITLE. This Act may be cited as the ``Children's Hope Act of 2003''. SEC. 2. TAX CREDIT FOR CONTRIBUTIONS TO EDUCATION INVESTMENT ORG...\n",
      "Raw: SECTION 1. SHORT TITLE.\n",
      "\n",
      "    This Act may be cited as the ``Children's Hope Act of 2003''.\n",
      "\n",
      "SEC. 2. TAX CREDIT FOR CONTRIBUTIONS TO EDUCATION INVESTME...\n",
      "Cleaned: SECTION 1. SHORT TITLE. This Act may be cited as the ``Children's Hope Act of 2003''. SEC. 2. TAX CREDIT FOR CONTRIBUTIONS TO EDUCATION INVESTMENT ORG...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "facf4c22e7d74a898e7fba3cdbc72ebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw: SECTION 1. SHORT TITLE.\n",
      "\n",
      "    This Act may be cited as the ``Social Security Account Number Anti-\n",
      "Fraud Act''.\n",
      "\n",
      "SEC. 2. STATEMENT OF PURPOSE.\n",
      "\n",
      "    The ...\n",
      "Cleaned: SECTION 1. SHORT TITLE. This Act may be cited as the ``Social Security Account Number Anti- Fraud Act''. SEC. 2. STATEMENT OF PURPOSE. The purposes of...\n"
     ]
    }
   ],
   "source": [
    "logger.warning(\"Attempting to update some libraries.\")\n",
    "%pip install datasets --upgrade -q\n",
    "%pip install fsspec --upgrade -q\n",
    "%pip install pyarrow --upgrade -q\n",
    "logger.warning(\"Library update attempts finished. If issues persist, ensure runtime was restarted after updates.\")\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "logger.info(f\"Starting dataset loading and processing for: {CONFIG['dataset_name']}\")\n",
    "\n",
    "dataset = None\n",
    "load_dataset_kwargs = {\n",
    "    \"path\": CONFIG[\"dataset_name\"],\n",
    "    \"split\": f\"train[:{CONFIG['sample_size']}]\",\n",
    "}\n",
    "\n",
    "try:\n",
    "    logger.info(f\"Loading dataset with streaming=False and split sample size: {CONFIG['sample_size']}\")\n",
    "    dataset = load_dataset(\n",
    "        **load_dataset_kwargs,\n",
    "    )\n",
    "    logger.info(\"Dataset loaded successfully with streaming=False\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Attempt 1 failed: {str(e)}\")\n",
    "    logger.info(\"Attempting alternative loading method using pandas...\")\n",
    "    try:\n",
    "        # Define splits and paths\n",
    "        splits = {\n",
    "            'train': 'data/train-00000-of-00001.parquet',\n",
    "            'test': 'data/test-00000-of-00001.parquet',\n",
    "            'ca_test': 'data/ca_test-00000-of-00001.parquet'\n",
    "        }\n",
    "        \n",
    "        # Load training data using pandas\n",
    "        df = pd.read_parquet(f\"hf://datasets/FiscalNote/billsum/{splits['train']}\")\n",
    "        \n",
    "        # Convert to Dataset format\n",
    "        from datasets import Dataset\n",
    "        dataset = Dataset.from_pandas(df)\n",
    "        \n",
    "        # Sample if needed\n",
    "        if CONFIG['sample_size'] and CONFIG['sample_size'] < len(dataset):\n",
    "            dataset = dataset.shuffle(seed=CONFIG['seed']).select(range(CONFIG['sample_size']))\n",
    "            \n",
    "        logger.info(\"Dataset loaded successfully using pandas alternative method\")\n",
    "        \n",
    "    except Exception as e2:\n",
    "        logger.error(f\"Alternative loading method also failed: {str(e2)}\")\n",
    "        raise RuntimeError(f\"Both loading methods failed. Original error: {str(e)}, Alternative error: {str(e2)}\")\n",
    "        \n",
    "if dataset is None:\n",
    "    logger.critical(\"FATAL: Dataset could not be loaded by any implemented method.\")\n",
    "    raise RuntimeError(\"Dataset loading failed. Please check the dataset name and parameters.\")\n",
    "\n",
    "\n",
    "logger.info(f\"Dataset loaded. Original columns: {dataset.column_names}\")\n",
    "logger.info(f\"Number of samples in loaded dataset: {len(dataset)}\")\n",
    "\n",
    "if 'text' in dataset.column_names:\n",
    "    dataset = dataset.rename_column('text', 'article')\n",
    "    logger.info(\"Renamed dataset column: 'text' -> 'article'\")\n",
    "    CONFIG['text_col'] = 'article'\n",
    "    logger.info(f\"Updated CONFIG['text_col'] to '{CONFIG['text_col']}'\")\n",
    "else:\n",
    "    logger.warning(f\"'text' column not found in dataset columns: {dataset.column_names}. Skipping rename. Current text column in CONFIG: {CONFIG['text_col']}\")\n",
    "\n",
    "logger.info(\"Creating dataset splits...\")\n",
    "try:\n",
    "    total_size = len(dataset)\n",
    "    logger.info(f\"Total dataset size for splitting: {total_size}\")\n",
    "\n",
    "    train_size = int(total_size * CONFIG[\"split_train_frac\"])\n",
    "    eval_size = total_size - train_size\n",
    "\n",
    "    if train_size <= 0 or eval_size <= 0:\n",
    "        logger.error(f\"Calculated train_size ({train_size}) or eval_size ({eval_size}) is non-positive. Aborting split.\")\n",
    "        raise ValueError(\"Train or evaluation set size is not positive. Check dataset size and split_train_frac.\")\n",
    "\n",
    "    dataset_shuffled = dataset.shuffle(seed=CONFIG[\"seed\"]) # Shuffle before selecting\n",
    "    train_dataset = dataset_shuffled.select(range(train_size))\n",
    "    eval_dataset = dataset_shuffled.select(range(train_size, total_size))\n",
    "\n",
    "    logger.info(f\"Created splits - Train: {len(train_dataset)}, Eval: {len(eval_dataset)}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error creating dataset splits: {e}\")\n",
    "    raise\n",
    "\n",
    "logger.info(\"Processing datasets (tokenization, etc.)...\")\n",
    "try:\n",
    "    tokenized_datasets = {\n",
    "        'train': train_dataset.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            remove_columns=train_dataset.column_names\n",
    "        ),\n",
    "        'eval': eval_dataset.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            remove_columns=eval_dataset.column_names\n",
    "        )\n",
    "    }\n",
    "    logger.info(\"Dataset processing complete.\")\n",
    "    logger.info(f\"Final tokenized dataset sizes - Training samples: {len(tokenized_datasets['train'])}, Evaluation samples: {len(tokenized_datasets['eval'])}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error processing datasets: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56a9f58",
   "metadata": {},
   "source": [
    "## 10. Define Training Arguments\n",
    "\n",
    "Configure the training process using Seq2SeqTrainingArguments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85dd1a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=CONFIG[\"output_dir\"],\n",
    "    num_train_epochs=CONFIG[\"num_train_epochs\"],\n",
    "    per_device_train_batch_size=CONFIG[\"per_device_train_batch_size\"],\n",
    "    per_device_eval_batch_size=CONFIG[\"per_device_eval_batch_size\"],\n",
    "    gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n",
    "    learning_rate=CONFIG[\"learning_rate\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"],\n",
    "    warmup_steps=CONFIG[\"warmup_steps\"],\n",
    "    \n",
    "    # Optimization - modified for MPS\n",
    "    fp16=False,  # Disable FP16 for MPS\n",
    "    fp16_full_eval=False,\n",
    "    bf16=False,\n",
    "    optim=\"adamw_torch\",  # Use adamw_torch optimizer\n",
    "    \n",
    "    # Disable features that might cause issues on MPS\n",
    "    gradient_checkpointing=False,  # Disable gradient checkpointing on MPS\n",
    "    group_by_length=False,  # Disable length batching\n",
    "    dataloader_pin_memory=False,  # Disable pin_memory on MPS\n",
    "    \n",
    "    # Logging & Evaluation\n",
    "    logging_dir=f\"{CONFIG['output_dir']}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=CONFIG[\"logging_steps\"],\n",
    "    eval_strategy=CONFIG[\"evaluation_strategy\"],\n",
    "    eval_steps=CONFIG[\"eval_steps\"],\n",
    "    \n",
    "    # Saving\n",
    "    save_strategy=CONFIG[\"save_strategy\"],\n",
    "    save_steps=CONFIG[\"save_steps\"],\n",
    "    save_total_limit=CONFIG[\"save_total_limit\"],\n",
    "    \n",
    "    # Model Loading\n",
    "    load_best_model_at_end=CONFIG[\"load_best_model_at_end\"],\n",
    "    metric_for_best_model=CONFIG[\"metric_for_best_model\"],\n",
    "    greater_is_better=CONFIG[\"greater_is_better\"],\n",
    "    \n",
    "    # Generation\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=CONFIG[\"max_target_tokens\"],\n",
    "    generation_num_beams=CONFIG[\"gen_num_beams\"],\n",
    "    \n",
    "    # Other\n",
    "    report_to=CONFIG[\"report_to\"],\n",
    "    seed=CONFIG[\"seed\"],\n",
    "    remove_unused_columns=False,  # Keep all columns\n",
    "    overwrite_output_dir=CONFIG[\"overwrite_output_dir\"],\n",
    "    use_legacy_prediction_loop=True,  # Use legacy prediction loop to avoid past_key_values warning\n",
    ")\n",
    "\n",
    "logger.info(f\"Training arguments configured for MPS\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78898be",
   "metadata": {},
   "source": [
    "## 11. Define Metrics Computation\n",
    "\n",
    "Function to compute ROUGE and BLEU scores for evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4970687",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "import numpy as np\n",
    "import nltk\n",
    "from typing import Dict, List\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def get_metrics():\n",
    "    \"\"\"Load and cache evaluation metrics.\"\"\"\n",
    "    return {\n",
    "        \"rouge\": evaluate.load(\"rouge\"),\n",
    "        \"bleu\": evaluate.load(\"bleu\"),\n",
    "        \"bertscore\": evaluate.load(\"bertscore\")\n",
    "    }\n",
    "\n",
    "def process_texts(texts: List[str]) -> List[str]:\n",
    "    \"\"\"Clean and process texts for evaluation.\"\"\"\n",
    "    return [\"\\n\".join(nltk.sent_tokenize(text.strip())) for text in texts]\n",
    "\n",
    "def compute_metrics(eval_preds, batch_size: int = 32) -> Dict[str, float]:\n",
    "    \"\"\"Compute evaluation metrics with improved error handling and statistics.\"\"\"\n",
    "    try:\n",
    "        metrics = get_metrics()\n",
    "        preds, labels = eval_preds\n",
    "        if isinstance(preds, tuple):\n",
    "            preds = preds[0]\n",
    "\n",
    "        # Decode predictions and labels\n",
    "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        # Process texts\n",
    "        decoded_preds = process_texts(decoded_preds)\n",
    "        decoded_labels = process_texts(decoded_labels)\n",
    "\n",
    "        # Compute metrics\n",
    "        rouge_results = metrics[\"rouge\"].compute(\n",
    "            predictions=decoded_preds, \n",
    "            references=decoded_labels\n",
    "        )\n",
    "        \n",
    "        decoded_labels_bleu = [[label] for label in decoded_labels]\n",
    "        bleu_results = metrics[\"bleu\"].compute(\n",
    "            predictions=decoded_preds, \n",
    "            references=decoded_labels_bleu\n",
    "        )\n",
    "\n",
    "        # Compute additional statistics\n",
    "        pred_lengths = [len(p.split()) for p in decoded_preds]\n",
    "        ref_lengths = [len(r.split()) for r in decoded_labels]\n",
    "\n",
    "        results = {\n",
    "            \"rouge1\": rouge_results[\"rouge1\"],\n",
    "            \"rouge2\": rouge_results[\"rouge2\"],\n",
    "            \"rougeL\": rouge_results[\"rougeL\"],\n",
    "            \"rougeLsum\": rouge_results[\"rougeLsum\"],\n",
    "            \"bleu\": bleu_results[\"bleu\"],\n",
    "            \"avg_pred_length\": np.mean(pred_lengths),\n",
    "            \"avg_ref_length\": np.mean(ref_lengths),\n",
    "            \"compression_ratio\": np.mean([p/r for p, r in zip(pred_lengths, ref_lengths)])\n",
    "        }\n",
    "\n",
    "        # Add generation length metric\n",
    "        prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "        results[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "        return {k: round(v, 4) for k, v in results.items()}\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error computing metrics: {e}\")\n",
    "        return {\n",
    "            \"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0,\n",
    "            \"rougeLsum\": 0.0, \"bleu\": 0.0, \"gen_len\": 0,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "logger.info(\"Enhanced metrics computation function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b54a510",
   "metadata": {},
   "source": [
    "## 11. Initialize Trainer\n",
    "\n",
    "Set up the `Seq2SeqTrainer` with the model, arguments, datasets, tokenizer, and metrics function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "601cfed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bh/db9plr7n08g67qlmw3z86x340000gn/T/ipykernel_62110/1412348843.py:52: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "from transformers import EarlyStoppingCallback,TrainerCallback\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Clear unused memory before training\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    print_memory_usage()\n",
    "\n",
    "class MemoryTrackingCallback(TrainerCallback):\n",
    "    \"\"\"Callback to track memory usage during training\"\"\"\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % 100 == 0:  # Monitor every 100 steps\n",
    "            print_memory_usage()\n",
    "\n",
    "def validate_training_args(args, model):\n",
    "    \"\"\"Validate training arguments for potential issues\"\"\"\n",
    "    if args.per_device_train_batch_size * args.gradient_accumulation_steps > 32:\n",
    "        logger.warning(\"Total batch size might be too large for available memory\")\n",
    "    \n",
    "    if args.fp16 and not torch.cuda.is_available() and not torch.backends.mps.is_available():\n",
    "        raise ValueError(\"FP16 requires CUDA or MPS\")\n",
    "    \n",
    "    if args.fp16 and torch.backends.mps.is_available():\n",
    "        logger.warning(\"FP16 is not fully supported on MPS. Turning it off.\")\n",
    "        args.fp16 = False\n",
    "        args.fp16_full_eval = False\n",
    "\n",
    "# Clear memory before initialization\n",
    "clear_memory()\n",
    "\n",
    "# Initialize data collator with error handling\n",
    "try:\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer,\n",
    "        model=model,\n",
    "        label_pad_token_id=tokenizer.pad_token_id,\n",
    "        pad_to_multiple_of=None  # Changed for MPS compatibility\n",
    "    )\n",
    "    logger.info(\"Data collator initialized successfully\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to initialize data collator: {e}\")\n",
    "    raise\n",
    "\n",
    "# Validate training arguments\n",
    "validate_training_args(training_args, model)\n",
    "\n",
    "# Initialize trainer with enhanced monitoring\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"eval\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(early_stopping_patience=3),\n",
    "        MemoryTrackingCallback()\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger.info(\"Trainer initialized with enhanced monitoring\")\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d90adf",
   "metadata": {},
   "source": [
    "## 12. Train the Model\n",
    "\n",
    "Start the fine-tuning process. This will take some time depending on the dataset size and hardware. ðŸ¥³\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7fc3bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_checkpoint(checkpoint_dir):\n",
    "    \"\"\"Find most recent checkpoint in the directory\"\"\"\n",
    "    checkpoints = [d for d in os.listdir(checkpoint_dir) \n",
    "                  if d.startswith(\"checkpoint-\")]\n",
    "    if not checkpoints:\n",
    "        return None\n",
    "    return os.path.join(checkpoint_dir, \n",
    "                       sorted(checkpoints, key=lambda x: int(x.split(\"-\")[1]))[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce148d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75' max='675' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 75/675 41:22 < 5:40:03, 0.03 it/s, Epoch 0.33/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Gen Len</th>\n",
       "      <th>Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>13.281200</td>\n",
       "      <td>14.751487</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>out of range integral type conversion attempted</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gourabsarkar/Developer/college_project/pdf_summarization_model_fine_tuning/.venv/lib/python3.10/site-packages/transformers/trainer_pt_utils.py:477: FutureWarning: DistributedTensorGatherer is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Error computing metrics: out of range integral type conversion attempted\n",
      "Error computing metrics: out of range integral type conversion attempted\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- Batch size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mper_device_train_batch_size\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Execute training\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_checkpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Log final metrics\u001b[39;00m\n\u001b[1;32m     23\u001b[0m metrics \u001b[38;5;241m=\u001b[39m train_result\u001b[38;5;241m.\u001b[39mmetrics\n",
      "File \u001b[0;32m~/Developer/college_project/pdf_summarization_model_fine_tuning/.venv/lib/python3.10/site-packages/transformers/trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/college_project/pdf_summarization_model_fine_tuning/.venv/lib/python3.10/site-packages/transformers/trainer.py:2560\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2553\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2554\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2555\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2556\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2557\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2558\u001b[0m )\n\u001b[1;32m   2559\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2560\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2563\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2564\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2565\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2566\u001b[0m ):\n\u001b[1;32m   2567\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2568\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/Developer/college_project/pdf_summarization_model_fine_tuning/.venv/lib/python3.10/site-packages/transformers/trainer.py:3782\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3779\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[1;32m   3780\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 3782\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3784\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/Developer/college_project/pdf_summarization_model_fine_tuning/.venv/lib/python3.10/site-packages/accelerate/accelerator.py:2473\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2471\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2472\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2473\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/college_project/pdf_summarization_model_fine_tuning/.venv/lib/python3.10/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/college_project/pdf_summarization_model_fine_tuning/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/college_project/pdf_summarization_model_fine_tuning/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logger.info(\"Starting training...\")\n",
    "try:\n",
    "    # Setup checkpoint directory\n",
    "    checkpoint_dir = os.path.join(CONFIG[\"output_dir\"], \"checkpoints\")\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # Check for existing checkpoint\n",
    "    resume_checkpoint = get_latest_checkpoint(checkpoint_dir)\n",
    "    if resume_checkpoint:\n",
    "        logger.info(f\"Resuming from checkpoint: {resume_checkpoint}\")\n",
    "    \n",
    "    # Print training info\n",
    "    logger.info(\"Training Configuration:\")\n",
    "    logger.info(f\"- Number of training examples: {len(trainer.train_dataset)}\")\n",
    "    logger.info(f\"- Number of validation examples: {len(trainer.eval_dataset)}\")\n",
    "    logger.info(f\"- Training Epochs: {CONFIG['num_train_epochs']}\")\n",
    "    logger.info(f\"- Batch size: {CONFIG['per_device_train_batch_size']}\")\n",
    "    \n",
    "    # Execute training\n",
    "    train_result = trainer.train(resume_from_checkpoint=resume_checkpoint)\n",
    "    \n",
    "    # Log final metrics\n",
    "    metrics = train_result.metrics\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "    \n",
    "    logger.info(\"Training completed successfully!\")\n",
    "    logger.info(f\"Final Training Loss: {metrics.get('train_loss', 'N/A')}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Training failed: {e}\")\n",
    "    if wandb.run:\n",
    "        wandb.log({\"training_error\": str(e)})\n",
    "        wandb.run.finish(exit_code=1)\n",
    "    raise e\n",
    "\n",
    "finally:\n",
    "    print_memory_usage()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0458a99f",
   "metadata": {},
   "source": [
    "## 13. Evaluate the Model\n",
    "\n",
    "Evaluate the fine-tuned model on the evaluation set to get final performance metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e88d6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.log(\"Evaluating model...\")\n",
    "eval_metrics = trainer.evaluate()\n",
    "\n",
    "logger.info(\"Evaluation metrics:\")\n",
    "for key, value in eval_metrics.items():\n",
    "    logger.info(f\"{key}: {value}\")\n",
    "\n",
    "# Log evaluation metrics\n",
    "trainer.log_metrics(\"eval\", eval_metrics)\n",
    "trainer.save_metrics(\"eval\", eval_metrics) # Saves to all_results.json\n",
    "\n",
    "# Prepare the training_report.json\n",
    "training_report = {\n",
    "    \"model_name\": CONFIG[\"model_name\"],\n",
    "    \"dataset_name\": CONFIG[\"dataset_name\"],\n",
    "    \"lora_adapter_name\": CONFIG[\"lora_adapter_name\"],\n",
    "    \"output_directory\": CONFIG[\"output_dir\"],\n",
    "    \"training_arguments\": {k: str(v) if isinstance(v, (torch.device, BitsAndBytesConfig)) else v for k, v in training_args.to_dict().items()}, # Convert non-serializable items\n",
    "    \"train_metrics\": trainer.state.log_history[:-1], # All logged steps except final eval\n",
    "    \"eval_metrics\": eval_metrics,\n",
    "    \"final_training_loss\": trainer.state.log_history[-2].get('loss') if len(trainer.state.log_history) > 1 and 'loss' in trainer.state.log_history[-2] else trainer.state.log_history[-1].get('train_loss', 'N/A')\n",
    "}\n",
    "\n",
    "\n",
    "# Add ROUGE and BLEU from eval_metrics to the top level for easier access\n",
    "for metric_key in [\"eval_rouge1\", \"eval_rouge2\", \"eval_rougeL\", \"eval_bleu\"]:\n",
    "    if metric_key in eval_metrics:\n",
    "        training_report[metric_key.replace(\"eval_\", \"\")] = eval_metrics[metric_key]\n",
    "\n",
    "\n",
    "# Save training_report.json locally\n",
    "report_path = os.path.join(CONFIG[\"output_dir\"], CONFIG[\"training_report_filename\"])\n",
    "with open(report_path, \"w\") as f:\n",
    "    json.dump(training_report, f, indent=4)\n",
    "logger.info(f\"Training report saved to {report_path}\")\n",
    "\n",
    "if wandb.run:\n",
    "    wandb.log(eval_metrics) # Log final eval metrics\n",
    "    wandb.save(report_path) # Save report to W&B artifacts\n",
    "    logger.info(\"Evaluation metrics and report logged to W&B.\")\n",
    "\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305442e8",
   "metadata": {},
   "source": [
    "## 14. Save Model and LoRA Adapter\n",
    "\n",
    "Save the fine-tuned LoRA adapter and the full model if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7083fe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the LoRA adapter\n",
    "lora_adapter_path = os.path.join(CONFIG[\"output_dir\"], CONFIG[\"lora_adapter_name\"])\n",
    "model.save_pretrained(lora_adapter_path) # Saves only the LoRA adapter\n",
    "tokenizer.save_pretrained(lora_adapter_path) # Save tokenizer with adapter\n",
    "logger.info(f\"LoRA adapter and tokenizer saved to {lora_adapter_path}\")\n",
    "\n",
    "# To save the full model (optional, requires more space)\n",
    "# merged_model_path = os.path.join(CONFIG[\"output_dir\"], \"merged_model_flan_t5_base_billsum\")\n",
    "# try:\n",
    "#     # Merge LoRA weights with the base model\n",
    "#     merged_model = model.merge_and_unload()\n",
    "#     merged_model.save_pretrained(merged_model_path)\n",
    "#     tokenizer.save_pretrained(merged_model_path)\n",
    "#     logger.info(f\"Full merged model saved to {merged_model_path}\")\n",
    "# except Exception as e:\n",
    "#     logger.error(f\"Could not merge and save full model: {e}. This might happen if the base model is not fully on CPU or due to memory constraints.\")\n",
    "#     logger.info(\"Only the LoRA adapter was saved.\")\n",
    "\n",
    "\n",
    "# If Google Drive is mounted, copy outputs there\n",
    "if CONFIG[\"mount_drive\"] and os.path.exists(CONFIG[\"gdrive_output_dir\"]):\n",
    "    logger.info(f\"Copying outputs to Google Drive: {CONFIG['gdrive_output_dir']}\")\n",
    "    # Copy LoRA adapter\n",
    "    gdrive_lora_path = os.path.join(CONFIG[\"gdrive_output_dir\"], CONFIG[\"lora_adapter_name\"])\n",
    "    if os.path.exists(gdrive_lora_path):\n",
    "        logger.info(f\"Removing existing LoRA adapter from GDrive: {gdrive_lora_path}\")\n",
    "        os.system(f\"rm -rf '{gdrive_lora_path}'\") # Use os.system for `rm -rf`\n",
    "    os.system(f\"cp -r '{lora_adapter_path}' '{CONFIG['gdrive_output_dir']}/'\")\n",
    "    logger.info(f\"LoRA adapter copied to {gdrive_lora_path}\")\n",
    "\n",
    "    # Copy training report\n",
    "    gdrive_report_path = os.path.join(CONFIG[\"gdrive_output_dir\"], CONFIG[\"training_report_filename\"])\n",
    "    os.system(f\"cp '{report_path}' '{gdrive_report_path}'\")\n",
    "    logger.info(f\"Training report copied to {gdrive_report_path}\")\n",
    "\n",
    "    # Copy all_results.json (contains eval metrics)\n",
    "    all_results_path = os.path.join(CONFIG[\"output_dir\"], \"all_results.json\")\n",
    "    if os.path.exists(all_results_path):\n",
    "        gdrive_all_results_path = os.path.join(CONFIG[\"gdrive_output_dir\"], \"all_results.json\")\n",
    "        os.system(f\"cp '{all_results_path}' '{gdrive_all_results_path}'\")\n",
    "        logger.info(f\"all_results.json copied to {gdrive_all_results_path}\")\n",
    "\n",
    "    # If merged model was saved and exists, copy it too\n",
    "    # if 'merged_model' in locals() and os.path.exists(merged_model_path):\n",
    "    #     gdrive_merged_model_path = os.path.join(CONFIG[\"gdrive_output_dir\"], \"merged_model_flan_t5_base_billsum\")\n",
    "    #     if os.path.exists(gdrive_merged_model_path):\n",
    "    #         logger.info(f\"Removing existing merged model from GDrive: {gdrive_merged_model_path}\")\n",
    "    #         os.system(f\"rm -rf '{gdrive_merged_model_path}'\")\n",
    "    #     os.system(f\"cp -r '{merged_model_path}' '{CONFIG['gdrive_output_dir']}/'\")\n",
    "    #     logger.info(f\"Full merged model copied to {gdrive_merged_model_path}\")\n",
    "else:\n",
    "    logger.warning(\"Google Drive not mounted or GDrive output path does not exist. Outputs saved locally.\")\n",
    "\n",
    "if wandb.run:\n",
    "    # Log LoRA adapter as artifact if desired\n",
    "    # lora_artifact = wandb.Artifact(CONFIG[\"lora_adapter_name\"], type=\"model\")\n",
    "    # lora_artifact.add_dir(lora_adapter_path)\n",
    "    # wandb.log_artifact(lora_artifact)\n",
    "    # logger.info(f\"LoRA adapter logged as W&B artifact: {CONFIG['lora_adapter_name']}\")\n",
    "    wandb.finish()\n",
    "\n",
    "logger.info(\"Script finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5720cd",
   "metadata": {},
   "source": [
    "## Test the model with a sample input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed36bdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test document - Cybersecurity and Privacy Protection Act\n",
    "test_document = \"\"\"\n",
    "CYBERSECURITY AND PRIVACY PROTECTION ACT OF 2025\n",
    "\n",
    "SECTION 1. SHORT TITLE AND PURPOSE\n",
    "\n",
    "    (a) This Act may be cited as the 'Cybersecurity and Privacy Protection Act of 2025'.\n",
    "    (b) The purpose of this Act is to enhance cybersecurity measures and protect individual privacy in the digital age.\n",
    "\n",
    "SECTION 2. DEFINITIONS\n",
    "\n",
    "In this Act:\n",
    "    (1) 'Personal Data' means any information relating to an identified or identifiable natural person.\n",
    "    (2) 'Data Controller' means any entity that determines the purposes and means of processing personal data.\n",
    "    (3) 'Critical Infrastructure' means systems and assets vital to national security.\n",
    "\n",
    "SECTION 3. CYBERSECURITY REQUIREMENTS\n",
    "\n",
    "    (a) MANDATORY SECURITY MEASURES.â€”\n",
    "        (1) All Data Controllers shall implement:\n",
    "            (A) End-to-end encryption for data transmission\n",
    "            (B) Multi-factor authentication for system access\n",
    "            (C) Regular security audits and vulnerability assessments\n",
    "\n",
    "    (b) INCIDENT REPORTING.â€”\n",
    "        (1) Data Controllers shall report any security breach within 48 hours.\n",
    "        (2) Penalties for non-compliance shall be up to $500,000 per incident.\n",
    "\n",
    "SECTION 4. PRIVACY PROTECTIONS\n",
    "\n",
    "    (a) CONSENT REQUIREMENTS.â€”\n",
    "        (1) Explicit consent required for data collection\n",
    "        (2) Right to access and delete personal data\n",
    "        (3) Annual privacy impact assessments\n",
    "\n",
    "    (b) CHILDREN'S PRIVACY.â€”\n",
    "        (1) Enhanced protections for users under 13\n",
    "        (2) Parental consent requirements\n",
    "\n",
    "SECTION 5. ENFORCEMENT\n",
    "\n",
    "    (a) The Federal Trade Commission shall enforce this Act.\n",
    "    (b) State Attorneys General may bring civil actions.\n",
    "\n",
    "SECTION 6. AUTHORIZATION OF APPROPRIATIONS\n",
    "\n",
    "    There is authorized to be appropriated $275,000,000 for fiscal year 2026 to carry out this Act.\n",
    "\"\"\"\n",
    "\n",
    "# Test the model with the adapter\n",
    "try:\n",
    "    # Load the trained model with LoRA adapter\n",
    "    logger.info(f\"Loading model with LoRA adapter from {lora_adapter_path}...\")\n",
    "    \n",
    "    # Get the device\n",
    "    current_device = get_device()\n",
    "    \n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        lora_adapter_path,\n",
    "        torch_dtype=torch.float32  # Use float32 for MPS compatibility\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.load_adapter(lora_adapter_path, CONFIG[\"lora_adapter_name\"])\n",
    "    model.set_adapter(CONFIG[\"lora_adapter_name\"])\n",
    "    model.eval()\n",
    "    model.to(current_device)  # Move the model to the appropriate device\n",
    "    \n",
    "    # Process the test document\n",
    "    logger.info(\"Tokenizing test document...\")\n",
    "    prompt = f\"{CONFIG['prompt_prefix']}{test_document}\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(current_device)\n",
    "    \n",
    "    # Generate summary\n",
    "    logger.info(\"Generating summary...\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs, \n",
    "            max_length=CONFIG[\"max_target_tokens\"],\n",
    "            num_beams=CONFIG[\"gen_num_beams\"],\n",
    "            length_penalty=CONFIG[\"gen_length_penalty\"],\n",
    "            early_stopping=CONFIG[\"gen_early_stopping\"]\n",
    "        )\n",
    "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(\"\\nGenerated Summary:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(summary)\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during inference: {e}\")\n",
    "    print(f\"An error occurred during inference: {str(e)}\")\n",
    "\n",
    "print_memory_usage()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
