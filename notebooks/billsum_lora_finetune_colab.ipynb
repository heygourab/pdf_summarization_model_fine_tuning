{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b37ddabf",
   "metadata": {},
   "source": [
    "# Fine-tuning Flan-T5-base for Legal Document Summarization - Colab GPU Version\n",
    "\n",
    "Author: Gourab S. (@heygourab), <a href=\"https://github.com/heygourab\"><img src=\"https://github.com/fluidicon.png\" width=\"20\" height=\"20\" alt=\"github\" /> @heygourab</a>\n",
    "\n",
    "This notebook fine-tunes Flan-T5-base on the BillSum dataset using LoRA with 4-bit quantization on Google Colab's GPU (T4, 16GB VRAM). Optimized for stability, fixes NaN loss, and handles memory constraints.\n",
    "\n",
    "## Setup Overview\n",
    "\n",
    "- Base Model: google/flan-t5-base\n",
    "- Dataset: BillSum (~1000 samples)\n",
    "- Hardware: Colab GPU (T4, 16GB VRAM)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Colab environment with GPU enabled\n",
    "- Install dependencies: `pip install transformers datasets accelerate evaluate peft bitsandbytes nltk wandb torch psutil`\n",
    "\n",
    "<a href=\"https://colab.research.google.com\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e4d2ae",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "**What it does**: Installs specific versions of required packages to avoid version mismatches and ensure bitsandbytes works with CUDA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2475d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement bitsandbytes==0.44.1 (from versions: 0.31.8, 0.32.0, 0.32.1, 0.32.2, 0.32.3, 0.33.0, 0.33.1, 0.34.0, 0.35.0, 0.35.1, 0.35.2, 0.35.3, 0.35.4, 0.36.0, 0.36.0.post1, 0.36.0.post2, 0.37.0, 0.37.1, 0.37.2, 0.38.0, 0.38.0.post1, 0.38.0.post2, 0.38.1, 0.39.0, 0.39.1, 0.40.0, 0.40.0.post1, 0.40.0.post2, 0.40.0.post3, 0.40.0.post4, 0.40.1, 0.40.1.post1, 0.40.2, 0.41.0, 0.41.1, 0.41.2, 0.41.2.post1, 0.41.2.post2, 0.41.3, 0.41.3.post1, 0.41.3.post2, 0.42.0)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for bitsandbytes==0.44.1\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Dependencies installed. Restart runtime if prompted.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q torch --force-reinstall\n",
    "%pip install -q bitsandbytes accelerate datasets evaluate peft transformers nltk wandb psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4a9ea0",
   "metadata": {},
   "source": [
    "## 2. Import Libraries\n",
    "\n",
    "**What it does**: Imports all necessary Python libraries for fine-tuning, quantization, and logging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8c4ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import nltk\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainerCallback\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "import wandb\n",
    "import psutil\n",
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba69578d",
   "metadata": {},
   "source": [
    "## 3. Logger setup\n",
    "\n",
    "**What it does**: Configures a logger to output to console and a timestamped log file for debugging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c4567e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:52:07 — train_logger — INFO — Logger initialized: train_logger\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:52:07 — train_logger — INFO — Log file created at: /Users/gourabsarkar/Developer/college_project/pdf_summarization_model_fine_tuning/notebooks/logs/training_20250519_135207.log\n",
      "2025-05-19 13:52:07 — train_logger — INFO — Python version: 3.10.17 (main, Apr  8 2025, 12:10:59) [Clang 16.0.0 (clang-1600.0.26.6)]\n"
     ]
    }
   ],
   "source": [
    "def setup_logger(name=\"train_logger\", level=logging.INFO):\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level)\n",
    "    logger.propagate = False\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "    formatter = logging.Formatter(\n",
    "        fmt='%(asctime)s — %(name)s — %(levelname)s — %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S'\n",
    "    )\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setFormatter(formatter)\n",
    "    logger.addHandler(console_handler)\n",
    "    log_dir = os.path.join(os.getcwd(), 'logs')\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    log_file = os.path.join(log_dir, f'training_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log')\n",
    "    file_handler = logging.FileHandler(log_file)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.info(f\"Logger initialized: {name}\")\n",
    "    logger.info(f\"Log file: {os.path.abspath(log_file)}\")\n",
    "    return logger\n",
    "\n",
    "logger = setup_logger(\"train_logger\", logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712d79dc",
   "metadata": {},
   "source": [
    "## 4. NLTK Setup\n",
    "\n",
    "**What it does**: Downloads NLTK resources for sentence tokenization used in metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fbb34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:52:31 — train_logger — INFO — Successfully downloaded NLTK resource: punkt\n",
      "2025-05-19 13:52:33 — train_logger — INFO — Successfully downloaded NLTK resource: punkt_tab\n"
     ]
    }
   ],
   "source": [
    "for resource in ['punkt', 'punkt_tab']:\n",
    "    try:\n",
    "        nltk.download(resource, quiet=True)\n",
    "        logger.info(f\"Downloaded NLTK resource: {resource}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error downloading {resource}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d615513",
   "metadata": {},
   "source": [
    "## 5. Memory Usage Monitoring\n",
    "\n",
    "The `print_memory_usage()` function monitors system resource utilization during model training:\n",
    "\n",
    "- Tracks RAM usage by getting the Resident Set Size (RSS) of current process in GB\n",
    "- For GPU-enabled systems:\n",
    "  - Reports allocated GPU memory\n",
    "  - Shows total available GPU memory\n",
    "  - Calculates percentage of GPU memory utilization\n",
    "  - Resets peak memory tracking statistics\n",
    "\n",
    "This helps identify potential memory bottlenecks and optimize resource usage during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393f44a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:52:43 — train_logger — INFO — RAM usage: 0.04 GB\n",
      "2025-05-19 13:52:43 — train_logger — INFO — Total system RAM: 8.59 GB\n"
     ]
    }
   ],
   "source": [
    "def print_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    ram_gb = process.memory_info().rss / 1e9\n",
    "    total_ram_gb = psutil.virtual_memory().total / 1e9\n",
    "    logger.info(f\"RAM usage: {ram_gb:.2f} GB / {total_ram_gb:.2f} GB ({ram_gb/total_ram_gb*100:.1f}%)\")\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "        gpu_mem = torch.cuda.memory_allocated() / 1e9\n",
    "        gpu_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        logger.info(f\"GPU memory: {gpu_mem:.2f}/{gpu_total:.2f} GB ({gpu_mem/gpu_total*100:.1f}%)\")\n",
    "\n",
    "def clear_memory():\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    print_memory_usage()\n",
    "\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9070a4ca",
   "metadata": {},
   "source": [
    "## 6. Configuration Parameters\n",
    "\n",
    "all hyperparameters and configuration settings for the model, dataset, LoRA, and training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580b1886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gourabsarkar/Developer/college_project/pdf_summarization_model_fine_tuning/.venv/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'BitsAndBytesConfig' from 'bitsandbytes' (/Users/gourabsarkar/Developer/college_project/pdf_summarization_model_fine_tuning/.venv/lib/python3.10/site-packages/bitsandbytes/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BitsAndBytesConfig\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TaskType\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'BitsAndBytesConfig' from 'bitsandbytes' (/Users/gourabsarkar/Developer/college_project/pdf_summarization_model_fine_tuning/.venv/lib/python3.10/site-packages/bitsandbytes/__init__.py)"
     ]
    }
   ],
   "source": [
    "CONFIG = {\n",
    "    # Model & Quantization\n",
    "    \"model_name\": \"google/flan-t5-base\",\n",
    "    \"model_type\": \"encoder-decoder\",\n",
    "    \"quantization_config\": BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True\n",
    "    ),\n",
    "    # Dataset\n",
    "    \"dataset_name\": \"billsum\",\n",
    "    \"text_col\": \"text\",\n",
    "    \"summary_col\": \"summary\",\n",
    "    \"max_input_tokens\": 512,\n",
    "    \"max_target_tokens\": 256,\n",
    "    \"sample_size\": 1000,\n",
    "    \"filter_by_length\": True,\n",
    "    \"split_train_frac\": 0.9,\n",
    "    # Prompt\n",
    "    \"prompt_prefix\": \"summarize: \",\n",
    "    # LoRA\n",
    "    \"lora_r\": 8,\n",
    "    \"lora_alpha\": 16,\n",
    "    \"lora_target_modules\": [\"q\", \"k\", \"v\"],\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \"lora_bias\": \"none\",\n",
    "    \"lora_task_type\": TaskType.SEQ_2_SEQ_LM,\n",
    "    \"lora_adapter_name\": \"lora_billsum_legal\",\n",
    "    # Training\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": True,\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"per_device_train_batch_size\": 4,\n",
    "    \"per_device_eval_batch_size\": 4,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"warmup_steps\": 50,\n",
    "    \"fp16\": True,\n",
    "    \"bf16\": False,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"optim\": \"adamw_8bit\",\n",
    "    # Logging & Checkpointing\n",
    "    \"logging_steps\": 10,\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"eval_steps\": 50,\n",
    "    \"save_strategy\": \"steps\",\n",
    "    \"save_steps\": 50,\n",
    "    \"save_total_limit\": 1,\n",
    "    \"load_best_model_at_end\": True,\n",
    "    \"metric_for_best_model\": \"eval_loss\",\n",
    "    \"greater_is_better\": False,\n",
    "    \"report_to\": \"wandb\",\n",
    "    \"overwrite_output_dir\": True,\n",
    "    # Generation\n",
    "    \"gen_num_beams\": 4,\n",
    "    \"gen_length_penalty\": 0.8,\n",
    "    \"gen_early_stopping\": True,\n",
    "    # Random seed\n",
    "    \"seed\": 42,\n",
    "    # Google Drive\n",
    "    \"mount_drive\": True,\n",
    "    \"drive_path\": \"MyDrive/ML_models/pdf_summarization\",\n",
    "    \"training_report_filename\": \"training_report.json\"\n",
    "}\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "CONFIG[\"output_dir\"] = f\"{CONFIG['lora_adapter_name']}_{timestamp}\"\n",
    "CONFIG[\"gdrive_output_dir\"] = f\"/content/drive/{CONFIG['drive_path']}/{CONFIG['output_dir']}\" if CONFIG[\"mount_drive\"] else CONFIG[\"output_dir\"]\n",
    "\n",
    "logger.info(\"CONFIG:\")\n",
    "for k, v in CONFIG.items():\n",
    "    logger.info(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a0224a",
   "metadata": {},
   "source": [
    "## 6. Login to Hugging Face Hub and Weights & Biases\n",
    "\n",
    "You'll need to log in to Hugging Face to download models/datasets and to Weights & Biases for experiment tracking.\n",
    "You can get your Hugging Face token from [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
    "and your W&B API key from [https://wandb.ai/authorize](https://wandb.ai/authorize).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49076cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder, notebook_login\n",
    "\n",
    "try:\n",
    "    if HfFolder.get_token() is None:\n",
    "        logger.info(\"Hugging Face token not found. Please log in.\")\n",
    "        notebook_login()\n",
    "    else:\n",
    "        logger.info(\"Already logged in to Hugging Face Hub.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Hugging Face login error: {e}\")\n",
    "    notebook_login()\n",
    "\n",
    "try:\n",
    "    wandb.login()\n",
    "    wandb.init(project=\"flan-t5-billsum-lora\", name=CONFIG[\"output_dir\"], config=CONFIG)\n",
    "    logger.info(\"Logged in to W&B and initialized experiment.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"W&B login failed: {e}. Falling back to TensorBoard.\")\n",
    "    CONFIG[\"report_to\"] = \"tensorboard\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e96255",
   "metadata": {},
   "source": [
    "## 7. Mount Google Drive (Optional)\n",
    "\n",
    "If you want to save your model checkpoints and outputs to Google Drive, mount it here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa3daab",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG[\"mount_drive\"]:\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        os.makedirs(CONFIG[\"gdrive_output_dir\"], exist_ok=True)\n",
    "        logger.info(f\"Google Drive mounted: {CONFIG['gdrive_output_dir']}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Google Drive mount failed: {e}. Saving locally.\")\n",
    "        CONFIG[\"mount_drive\"] = False\n",
    "else:\n",
    "    logger.info(\"Google Drive mount disabled. Saving locally.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559ed30a",
   "metadata": {},
   "source": [
    "## 8. Load Model, Tokenizer and Configure LoRA\n",
    "\n",
    "Loads the Flan-T5-base model and tokenizer from Hugging Face, configures the model for LoRA training and returns the model and tokenizer objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe80f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Loading tokenizer: {CONFIG['model_name']}\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        CONFIG[\"model_name\"],\n",
    "        use_fast=True,\n",
    "        padding_side=\"right\",\n",
    "        model_max_length=CONFIG[\"max_input_tokens\"]\n",
    "    )\n",
    "    logger.info(\"Tokenizer loaded.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Tokenizer loading failed: {e}\")\n",
    "    raise\n",
    "\n",
    "logger.info(f\"Loading model: {CONFIG['model_name']} with 4-bit quantization\")\n",
    "try:\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        CONFIG[\"model_name\"],\n",
    "        quantization_config=CONFIG[\"quantization_config\"],\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=CONFIG[\"gradient_checkpointing\"])\n",
    "    logger.info(\"Model loaded with quantization.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Model loading failed: {e}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    lora_config = LoraConfig(\n",
    "        r=CONFIG[\"lora_r\"],\n",
    "        lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "        target_modules=CONFIG[\"lora_target_modules\"],\n",
    "        lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "        bias=CONFIG[\"lora_bias\"],\n",
    "        task_type=CONFIG[\"lora_task_type\"]\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "    logger.info(\"LoRA adapter applied.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"LoRA setup failed: {e}\")\n",
    "    raise\n",
    "\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9673f4b4",
   "metadata": {},
   "source": [
    "## 9. Load and Preprocess Dataset\n",
    "\n",
    "Load the BillSum dataset, preprocess it for Flan-T5, and split into training and evaluation sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd47db1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.strip()\n",
    "    text = \" \".join(text.split())\n",
    "    text = re.sub(r'\\s*\\([^)]{0,40}\\)\\s*', ' ', text)\n",
    "    text = re.sub(r'\\s*\\[[^\\]]{0,40}\\]\\s*', ' ', text)\n",
    "    return text\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    try:\n",
    "        input_texts = examples.get(CONFIG[\"text_col\"], [])\n",
    "        summary_texts = examples.get(CONFIG[\"summary_col\"], [])\n",
    "\n",
    "        valid_pairs = [(i, s) for i, s in zip(input_texts, summary_texts) if i and s and isinstance(i, str) and isinstance(s, str)]\n",
    "        if not valid_pairs:\n",
    "            logger.warning(\"No valid input-summary pairs found.\")\n",
    "            return {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
    "\n",
    "        input_texts, summary_texts = zip(*valid_pairs)\n",
    "        cleaned_inputs = [clean_text(doc) for doc in input_texts]\n",
    "        prompts = [f'{CONFIG[\"prompt_prefix\"]}{doc}' for doc in cleaned_inputs]\n",
    "\n",
    "        logger.debug(f\"Sample raw input: {input_texts[0][:150]}...\")\n",
    "        logger.debug(f\"Sample cleaned input: {cleaned_inputs[0][:150]}...\")\n",
    "\n",
    "        model_inputs = tokenizer(\n",
    "            prompts,\n",
    "            max_length=CONFIG[\"max_input_tokens\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        summaries = [clean_text(s) if s else \"No summary provided.\" for s in summary_texts]\n",
    "        labels = tokenizer(\n",
    "            summaries,\n",
    "            max_length=CONFIG[\"max_target_tokens\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        model_inputs[\"labels\"][model_inputs[\"labels\"] == tokenizer.pad_token_id] = -100\n",
    "        return model_inputs\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Preprocessing failed: {e}\")\n",
    "        raise\n",
    "\n",
    "logger.info(f\"Loading dataset: {CONFIG['dataset_name']}\")\n",
    "try:\n",
    "    dataset = load_dataset(CONFIG[\"dataset_name\"], split=f\"train[:{CONFIG['sample_size']}]\")\n",
    "    logger.info(f\"Dataset loaded: {len(dataset)} samples\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Dataset loading failed: {e}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    total_size = len(dataset)\n",
    "    train_size = int(total_size * CONFIG[\"split_train_frac\"])\n",
    "    eval_size = total_size - train_size\n",
    "    dataset_shuffled = dataset.shuffle(seed=CONFIG[\"seed\"])\n",
    "    train_dataset = dataset_shuffled.select(range(train_size))\n",
    "    eval_dataset = dataset_shuffled.select(range(train_size, total_size))\n",
    "    logger.info(f\"Splits created - Train: {len(train_dataset)}, Eval: {len(eval_dataset)}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Dataset splitting failed: {e}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    tokenized_datasets = {\n",
    "        'train': train_dataset.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            remove_columns=train_dataset.column_names\n",
    "        ),\n",
    "        'eval': eval_dataset.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            remove_columns=eval_dataset.column_names\n",
    "        )\n",
    "    }\n",
    "    logger.info(f\"Tokenized datasets - Train: {len(tokenized_datasets['train'])}, Eval: {len(tokenized_datasets['eval'])}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Dataset tokenization failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56a9f58",
   "metadata": {},
   "source": [
    "## 10. Define Metrics Computation\n",
    "\n",
    "**What it does**: Defines a function to compute ROUGE and BLEU metrics, with error handling to prevent NaN issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dd1a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=1)\n",
    "def get_metrics():\n",
    "    \"\"\"Load and cache evaluation metrics.\n",
    "\n",
    "    Returns:\n",
    "        Dict: Dictionary containing loaded metrics for evaluation\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"rouge\": evaluate.load(\"rouge\"),\n",
    "        \"bleu\": evaluate.load(\"bleu\")\n",
    "    }\n",
    "\n",
    "def process_texts(texts):\n",
    "    \"\"\"Process and clean texts for evaluation.\n",
    "\n",
    "    Args:\n",
    "        texts (List[str]): List of texts to process\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of processed texts with sentence tokenization\n",
    "    \"\"\"\n",
    "    return [\"\\n\".join(nltk.sent_tokenize(text.strip())) \n",
    "            for text in texts if text.strip()]\n",
    "\n",
    "def compute_metrics(eval_preds: tuple):\n",
    "    \"\"\"Compute evaluation metrics for model predictions.\n",
    "\n",
    "    Args:\n",
    "        eval_preds (tuple): Tuple containing predictions and labels\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: Dictionary containing computed metrics\n",
    "    \"\"\"\n",
    "    try:\n",
    "        metrics = get_metrics()\n",
    "        preds, labels = eval_preds\n",
    "        if isinstance(preds, tuple):\n",
    "            preds = preds[0]\n",
    "\n",
    "        # Decode predictions and labels\n",
    "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        # Process texts\n",
    "        decoded_preds = process_texts(decoded_preds)\n",
    "        decoded_labels = process_texts(decoded_labels)\n",
    "\n",
    "        if not decoded_preds or not decoded_labels:\n",
    "            logger.warning(\"Empty predictions or labels. Returning default metrics.\")\n",
    "            return {\n",
    "                \"rouge1\": 0.0, \n",
    "                \"rouge2\": 0.0, \n",
    "                \"rougeL\": 0.0, \n",
    "                \"bleu\": 0.0, \n",
    "                \"gen_len\": 0\n",
    "            }\n",
    "\n",
    "        # Compute ROUGE scores\n",
    "        rouge_results = metrics[\"rouge\"].compute(\n",
    "            predictions=decoded_preds,\n",
    "            references=decoded_labels\n",
    "        )\n",
    "\n",
    "        # Compute BLEU scores\n",
    "        bleu_results = metrics[\"bleu\"].compute(\n",
    "            predictions=decoded_preds,\n",
    "            references=[[label] for label in decoded_labels]\n",
    "        )\n",
    "\n",
    "        # Calculate length statistics\n",
    "        pred_lengths = [len(p.split()) for p in decoded_preds]\n",
    "        ref_lengths = [len(r.split()) for r in decoded_labels]\n",
    "        \n",
    "        # Compile results\n",
    "        results = {\n",
    "            \"rouge1\": rouge_results[\"rouge1\"],\n",
    "            \"rouge2\": rouge_results[\"rouge2\"],\n",
    "            \"rougeL\": rouge_results[\"rougeL\"],\n",
    "            \"bleu\": bleu_results[\"bleu\"],\n",
    "            \"gen_len\": np.mean(pred_lengths) if pred_lengths else 0,\n",
    "            \"compression_ratio\": np.mean([p/r for p, r in zip(pred_lengths, ref_lengths)]) \n",
    "                               if pred_lengths and ref_lengths else 0\n",
    "        }\n",
    "\n",
    "        return {k: round(v, 4) for k, v in results.items()}\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Metrics computation failed: {e}\")\n",
    "        return {\n",
    "            \"rouge1\": 0.0,\n",
    "            \"rouge2\": 0.0,\n",
    "            \"rougeL\": 0.0,\n",
    "            \"bleu\": 0.0,\n",
    "            \"gen_len\": 0,\n",
    "            \"error\": str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78898be",
   "metadata": {},
   "source": [
    "## 11. Define Initialize Trainer\n",
    "\n",
    "**What it does**: Sets up the trainer with training arguments, data collator, and callbacks for loss and memory tracking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4970687",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossTrackingCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.training_loss = []\n",
    "        self.eval_loss = []\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None:\n",
    "            step = state.global_step\n",
    "            if \"loss\" in logs:\n",
    "                self.training_loss.append((step, logs[\"loss\"]))\n",
    "                wandb.log({\"training_loss\": logs[\"loss\"]}, step=step)\n",
    "            if \"eval_loss\" in logs:\n",
    "                self.eval_loss.append((step, logs[\"eval_loss\"]))\n",
    "                wandb.log({\"eval_loss\": logs[\"eval_loss\"]}, step=step)\n",
    "\n",
    "class MemoryTrackingCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % 50 == 0:\n",
    "            print_memory_usage()\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=CONFIG[\"output_dir\"],\n",
    "    num_train_epochs=CONFIG[\"num_train_epochs\"],\n",
    "    per_device_train_batch_size=CONFIG[\"per_device_train_batch_size\"],\n",
    "    per_device_eval_batch_size=CONFIG[\"per_device_eval_batch_size\"],\n",
    "    gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n",
    "    learning_rate=CONFIG[\"learning_rate\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"],\n",
    "    warmup_steps=CONFIG[\"warmup_steps\"],\n",
    "    fp16=CONFIG[\"fp16\"],\n",
    "    bf16=CONFIG[\"bf16\"],\n",
    "    optim=CONFIG[\"optim\"],\n",
    "    logging_dir=f\"{CONFIG['output_dir']}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=CONFIG[\"logging_steps\"],\n",
    "    eval_strategy=CONFIG[\"evaluation_strategy\"],\n",
    "    eval_steps=CONFIG[\"eval_steps\"],\n",
    "    save_strategy=CONFIG[\"save_strategy\"],\n",
    "    save_steps=CONFIG[\"save_steps\"],\n",
    "    save_total_limit=CONFIG[\"save_total_limit\"],\n",
    "    load_best_model_at_end=CONFIG[\"load_best_model_at_end\"],\n",
    "    metric_for_best_model=CONFIG[\"metric_for_best_model\"],\n",
    "    greater_is_better=CONFIG[\"greater_is_better\"],\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=CONFIG[\"max_target_tokens\"],\n",
    "    generation_num_beams=CONFIG[\"gen_num_beams\"],\n",
    "    report_to=CONFIG[\"report_to\"],\n",
    "    seed=CONFIG[\"seed\"],\n",
    "    gradient_checkpointing=CONFIG[\"gradient_checkpointing\"],\n",
    "    overwrite_output_dir=CONFIG[\"overwrite_output_dir\"]\n",
    ")\n",
    "\n",
    "try:\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer,\n",
    "        model=model,\n",
    "        label_pad_token_id=-100,\n",
    "        pad_to_multiple_of=8\n",
    "    )\n",
    "    logger.info(\"Data collator initialized.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Data collator initialization failed: {e}\")\n",
    "    raise\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"eval\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[\n",
    "        LossTrackingCallback(),\n",
    "        MemoryTrackingCallback()\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger.info(\"Trainer initialized.\")\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20932943",
   "metadata": {},
   "source": [
    "## 12. Train the Model 😁\n",
    "\n",
    "**What it does**: Runs the training loop, resumes from checkpoints if available, and logs metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d141af7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_checkpoint(checkpoint_dir):\n",
    "    checkpoints = [d for d in os.listdir(checkpoint_dir) if d.startswith(\"checkpoint-\")]\n",
    "    if not checkpoints:\n",
    "        return None\n",
    "    return os.path.join(checkpoint_dir, sorted(checkpoints, key=lambda x: int(x.split(\"-\")[1]))[-1])\n",
    "\n",
    "logger.info(\"Starting training...\")\n",
    "try:\n",
    "    checkpoint_dir = os.path.join(CONFIG[\"output_dir\"], \"checkpoints\")\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    resume_checkpoint = get_latest_checkpoint(checkpoint_dir)\n",
    "    if resume_checkpoint:\n",
    "        logger.info(f\"Resuming from checkpoint: {resume_checkpoint}\")\n",
    "\n",
    "    logger.info(f\"Training samples: {len(trainer.train_dataset)}\")\n",
    "    logger.info(f\"Validation samples: {len(trainer.eval_dataset)}\")\n",
    "    logger.info(f\"Epochs: {CONFIG['num_train_epochs']}\")\n",
    "\n",
    "    train_result = trainer.train(resume_from_checkpoint=resume_checkpoint)\n",
    "    metrics = train_result.metrics\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "\n",
    "    logger.info(\"Training completed!\")\n",
    "    logger.info(f\"Final Training Loss: {metrics.get('train_loss', 'N/A')}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Training failed: {e}\")\n",
    "    if wandb.run:\n",
    "        wandb.log({\"training_error\": str(e)})\n",
    "        wandb.run.finish(exit_code=1)\n",
    "    raise\n",
    "finally:\n",
    "    clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4c4cb1",
   "metadata": {},
   "source": [
    "## 13. Evaluate the Model\n",
    "\n",
    "**What it does**: Evaluates the model on the validation set and computes ROUGE and BLEU scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c624e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Evaluating model...\")\n",
    "eval_metrics = trainer.evaluate()\n",
    "\n",
    "logger.info(\"Evaluation metrics:\")\n",
    "for key, value in eval_metrics.items():\n",
    "    logger.info(f\"{key}: {value}\")\n",
    "\n",
    "trainer.log_metrics(\"eval\", eval_metrics)\n",
    "trainer.save_metrics(\"eval\", eval_metrics)\n",
    "\n",
    "training_report = {\n",
    "    \"model_name\": CONFIG[\"model_name\"],\n",
    "    \"dataset_name\": CONFIG[\"dataset_name\"],\n",
    "    \"lora_adapter_name\": CONFIG[\"lora_adapter_name\"],\n",
    "    \"output_directory\": CONFIG[\"output_dir\"],\n",
    "    \"training_arguments\": training_args.to_dict(),\n",
    "    \"train_metrics\": trainer.state.log_history[:-1],\n",
    "    \"eval_metrics\": eval_metrics,\n",
    "    \"final_training_loss\": trainer.state.log_history[-2].get('loss', 'N/A') if len(trainer.state.log_history) > 1 else 'N/A'\n",
    "}\n",
    "\n",
    "for metric_key in [\"eval_rouge1\", \"eval_rouge2\", \"eval_rougeL\", \"eval_bleu\"]:\n",
    "    if metric_key in eval_metrics:\n",
    "        training_report[metric_key.replace(\"eval_\", \"\")] = eval_metrics[metric_key]\n",
    "\n",
    "report_path = os.path.join(CONFIG[\"output_dir\"], CONFIG[\"training_report_filename\"])\n",
    "with open(report_path, \"w\") as f:\n",
    "    json.dump(training_report, f, indent=4)\n",
    "logger.info(f\"Training report saved to {report_path}\")\n",
    "\n",
    "if CONFIG[\"mount_drive\"] and os.path.exists(CONFIG[\"gdrive_output_dir\"]):\n",
    "    gdrive_report_path = os.path.join(CONFIG[\"gdrive_output_dir\"], CONFIG[\"training_report_filename\"])\n",
    "    os.system(f\"cp '{report_path}' '{gdrive_report_path}'\")\n",
    "    logger.info(f\"Training report copied to {gdrive_report_path}\")\n",
    "\n",
    "if wandb.run:\n",
    "    wandb.log(eval_metrics)\n",
    "    wandb.save(report_path)\n",
    "\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39df27fe",
   "metadata": {},
   "source": [
    "## 13. Save Model and LoRA Adapter\n",
    "\n",
    "**What it does**: Saves the fine-tuned model and LoRA adapter to the specified directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dd4c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_adapter_path = os.path.join(CONFIG[\"output_dir\"], CONFIG[\"lora_adapter_name\"])\n",
    "model.save_pretrained(lora_adapter_path)\n",
    "tokenizer.save_pretrained(lora_adapter_path)\n",
    "logger.info(f\"LoRA adapter and tokenizer saved to {lora_adapter_path}\")\n",
    "\n",
    "if CONFIG[\"mount_drive\"] and os.path.exists(CONFIG[\"gdrive_output_dir\"]):\n",
    "    gdrive_lora_path = os.path.join(CONFIG[\"gdrive_output_dir\"], CONFIG[\"lora_adapter_name\"])\n",
    "    os.system(f\"cp -r '{lora_adapter_path}' '{CONFIG['gdrive_output_dir']}/'\")\n",
    "    logger.info(f\"LoRA adapter copied to {gdrive_lora_path}\")\n",
    "\n",
    "if wandb.run:\n",
    "    wandb.finish()\n",
    "\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3683f7e",
   "metadata": {},
   "source": [
    "## 14. Test the Model\n",
    "\n",
    "**What it does**: Tests the model on a sample input and prints the generated summary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0fe855",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_document = \"\"\"\n",
    "CYBERSECURITY AND PRIVACY PROTECTION ACT OF 2025\n",
    "\n",
    "SECTION 1. SHORT TITLE AND PURPOSE\n",
    "\n",
    "    (a) This Act may be cited as the 'Cybersecurity and Privacy Protection Act of 2025'.\n",
    "    (b) The purpose of this Act is to enhance cybersecurity measures and protect individual privacy in the digital age.\n",
    "\n",
    "SECTION 2. DEFINITIONS\n",
    "\n",
    "In this Act:\n",
    "(1) 'Personal Data' means any information relating to an identified or identifiable natural person.\n",
    "(2) 'Data Controller' means any entity that determines the purposes and means of processing personal data.\n",
    "(3) 'Critical Infrastructure' means systems and assets vital to national security.\n",
    "\n",
    "SECTION 3. CYBERSECURITY REQUIREMENTS\n",
    "\n",
    "    (a) MANDATORY SECURITY MEASURES.—\n",
    "        (1) All Data Controllers shall implement:\n",
    "            (A) End-to-end encryption for data transmission\n",
    "            (B) Multi-factor authentication for system access\n",
    "            (C) Regular security audits and vulnerability assessments\n",
    "\n",
    "    (b) INCIDENT REPORTING.—\n",
    "        (1) Data Controllers shall report any security breach within 48 hours.\n",
    "        (2) Penalties for non-compliance shall be up to $500,000 per incident.\n",
    "\n",
    "SECTION 4. PRIVACY PROTECTIONS\n",
    "\n",
    "    (a) CONSENT REQUIREMENTS.—\n",
    "        (1) Explicit consent required for data collection\n",
    "        (2) Right to access and delete personal data\n",
    "        (3) Annual privacy impact assessments\n",
    "\n",
    "    (b) CHILDREN'S PRIVACY.—\n",
    "        (1) Enhanced protections for users under 13\n",
    "        (2) Parental consent requirements\n",
    "\n",
    "SECTION 5. ENFORCEMENT\n",
    "\n",
    "    (a) The Federal Trade Commission shall enforce this Act.\n",
    "    (b) State Attorneys General may bring civil actions.\n",
    "\n",
    "SECTION 6. AUTHORIZATION OF APPROPRIATIONS\n",
    "\n",
    "    There is authorized to be appropriated $275,000,000 for fiscal year 2026 to carry out this Act.\n",
    "\"\"\"\n",
    "\n",
    "logger.info(\"Loading trained model for testing...\")\n",
    "try:\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        CONFIG[\"model_name\"],\n",
    "        quantization_config=CONFIG[\"quantization_config\"],\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.load_adapter(lora_adapter_path, CONFIG[\"lora_adapter_name\"])\n",
    "    model.set_adapter(CONFIG[\"lora_adapter_name\"])\n",
    "    model.eval()\n",
    "    logger.info(\"Model loaded for testing.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load model for testing: {e}\")\n",
    "    raise\n",
    "\n",
    "inputs = tokenizer(\n",
    "    f\"{CONFIG['prompt_prefix']}{clean_text(test_document)}\",\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=CONFIG[\"max_input_tokens\"]\n",
    ").to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=CONFIG[\"max_target_tokens\"],\n",
    "        num_beams=CONFIG[\"gen_num_beams\"],\n",
    "        length_penalty=CONFIG[\"gen_length_penalty\"],\n",
    "        early_stopping=CONFIG[\"gen_early_stopping\"]\n",
    "    )\n",
    "summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Summary:\")\n",
    "print(summary)\n",
    "clear_memory()\n",
    "logger.info(\"Test completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
