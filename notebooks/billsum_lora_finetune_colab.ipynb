{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b37ddabf",
   "metadata": {},
   "source": [
    "# Fine-tuning Flan-T5-base for Legal Document Summarization - google colab version\n",
    "\n",
    "This notebook is a modified version of the original notebook by Gourab S. (@heygourab)\n",
    "\n",
    "Author: Gourab S.(@heygourab),\n",
    "\n",
    "<a href=\"https://github.com/heygourab\"><img src=\"https://github.com/fluidicon.png\" width=\"20\" height=\"20\" alt=\"github\" /> @heygourab</a>\n",
    "\n",
    "This notebook demonstrates fine-tuning the Flan-T5-base model on the BillSum dataset using LoRA (Low-Rank Adaptation). We'll use the Hugging Face ecosystem (`transformers`, `datasets`, `peft`) for efficient fine-tuning.\n",
    "\n",
    "## Setup Overview\n",
    "\n",
    "- Base Model: google/flan-t5-base\n",
    "- Dataset: BillSum (~2000 samples)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "This notebook assumes a Colab environment with a GPU available. If you're running this locally, make sure to install the required packages and set up your GPU environment accordingly.\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/heygourab/pdf_summarization_model_fine_tuning/blob/main/notebooks/billsum_lora_finetune_colab.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e4d2ae",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, let's install the required dependencies and set up GPU monitoring.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2475d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install -q transformers datasets accelerate evaluate peft bitsandbytes bert_score rouge_score nltk wandb omegaconf torch \n",
    "%pip install -U bitsandbytes -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4a9ea0",
   "metadata": {},
   "source": [
    "## 2. Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a8c4ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import nltk\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import sys\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType\n",
    ")\n",
    "import wandb\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba69578d",
   "metadata": {},
   "source": [
    "## 3. Logger setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59c4567e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:52:07 â€” train_logger â€” INFO â€” Logger initialized: train_logger\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:52:07 â€” train_logger â€” INFO â€” Log file created at: /Users/gourabsarkar/Developer/college_project/pdf_summarization_model_fine_tuning/notebooks/logs/training_20250519_135207.log\n",
      "2025-05-19 13:52:07 â€” train_logger â€” INFO â€” Python version: 3.10.17 (main, Apr  8 2025, 12:10:59) [Clang 16.0.0 (clang-1600.0.26.6)]\n"
     ]
    }
   ],
   "source": [
    "def setup_logger(name=\"train_logger\", level=logging.INFO, log_file=None):\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level)\n",
    "    logger.propagate = False  # Avoid duplicate logs\n",
    "\n",
    "    # Clear existing handlers\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "\n",
    "    # Formatter for log messages\n",
    "    formatter = logging.Formatter(\n",
    "        fmt='%(asctime)s â€” %(name)s â€” %(levelname)s â€” %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S'\n",
    "    )\n",
    "\n",
    "    # Console handler setup\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setFormatter(formatter)\n",
    "    logger.addHandler(console_handler)\n",
    "\n",
    "    # File handler setup\n",
    "    if log_file is None:\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        log_dir = os.path.join(os.getcwd(), 'logs')  # Safe fallback to current dir\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        log_file = os.path.join(log_dir, f'training_{timestamp}.log')\n",
    "    else:\n",
    "        log_dir = os.path.dirname(log_file)\n",
    "        if log_dir:\n",
    "            os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    file_handler = logging.FileHandler(log_file)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "    # Log header\n",
    "    logger.info(f\"Logger initialized: {name}\")\n",
    "    logger.info(f\"Log file created at: {os.path.abspath(log_file)}\")\n",
    "    logger.info(f\"Python version: {sys.version}\")\n",
    "\n",
    "    return logger\n",
    "\n",
    "# Use it\n",
    "logger = setup_logger(\"train_logger\", logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712d79dc",
   "metadata": {},
   "source": [
    "## 4. Loading the required NLTK libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68fbb34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:52:31 â€” train_logger â€” INFO â€” Successfully downloaded NLTK resource: punkt\n",
      "2025-05-19 13:52:33 â€” train_logger â€” INFO â€” Successfully downloaded NLTK resource: punkt_tab\n"
     ]
    }
   ],
   "source": [
    "# Download required NLTK data\n",
    "for resource in ['punkt', 'punkt_tab']:\n",
    "    try:\n",
    "        nltk.download(resource, quiet=True)\n",
    "        logger.info(f\"Successfully downloaded NLTK resource: {resource}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error downloading {resource}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d615513",
   "metadata": {},
   "source": [
    "## 5. Memory Usage Monitoring\n",
    "\n",
    "The `print_memory_usage()` function monitors system resource utilization during model training:\n",
    "\n",
    "- Tracks RAM usage by getting the Resident Set Size (RSS) of current process in GB\n",
    "- For GPU-enabled systems:\n",
    "  - Reports allocated GPU memory\n",
    "  - Shows total available GPU memory\n",
    "  - Calculates percentage of GPU memory utilization\n",
    "  - Resets peak memory tracking statistics\n",
    "\n",
    "This helps identify potential memory bottlenecks and optimize resource usage during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "393f44a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:52:43 â€” train_logger â€” INFO â€” RAM usage: 0.04 GB\n",
      "2025-05-19 13:52:43 â€” train_logger â€” INFO â€” Total system RAM: 8.59 GB\n"
     ]
    }
   ],
   "source": [
    "def print_memory_usage():\n",
    "    process = psutil.Process(os.getpid()) # Get the current process\n",
    "\n",
    "    ram_gb = process.memory_info().rss / 1e9 # Convert bytes to GB\n",
    "    total_gb = psutil.virtual_memory().total / 1e9 # Total system RAM in GB\n",
    "\n",
    "    logger.info(f\"RAM usage: {ram_gb:.2f} GB\") # Current process RAM usage\n",
    "    logger.info(f\"Total system RAM: {total_gb:.2f} GB\") # Total system RAM\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "        gpu_mem = torch.cuda.memory_allocated() / 1e9\n",
    "        gpu_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        peak_gpu_mem = torch.cuda.max_memory_allocated() / 1e9\n",
    "\n",
    "        logger.info(f\"GPU memory usage: {gpu_mem:.2f}/{gpu_total:.2f} GB ({gpu_mem/gpu_total*100:.1f}%)\")\n",
    "        logger.info(f\"Peak GPU memory: {peak_gpu_mem:.2f} GB\")\n",
    "\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9070a4ca",
   "metadata": {},
   "source": [
    "## 6. Configuration Parameters\n",
    "\n",
    "all hyperparameters and configuration settings for the model, dataset, LoRA, and training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580b1886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gourabsarkar/Developer/college_project/pdf_summarization_model_fine_tuning/.venv/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'BitsAndBytesConfig' from 'bitsandbytes' (/Users/gourabsarkar/Developer/college_project/pdf_summarization_model_fine_tuning/.venv/lib/python3.10/site-packages/bitsandbytes/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BitsAndBytesConfig\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TaskType\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'BitsAndBytesConfig' from 'bitsandbytes' (/Users/gourabsarkar/Developer/college_project/pdf_summarization_model_fine_tuning/.venv/lib/python3.10/site-packages/bitsandbytes/__init__.py)"
     ]
    }
   ],
   "source": [
    "from bitsandbytes import BitsAndBytesConfig\n",
    "from transformers import TaskType\n",
    "from datetime import datetime\n",
    "\n",
    "CONFIG = {\n",
    "    # Model & Quantization\n",
    "    \"model_name\": \"google/flan-t5-base\",\n",
    "    \"model_type\": \"encoder-decoder\",\n",
    "    \"quantization_config\": BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    ),\n",
    "\n",
    "    # Dataset\n",
    "    \"dataset_name\": \"billsum\",\n",
    "    \"text_col\": \"text\",\n",
    "    \"summary_col\": \"summary\",\n",
    "    \"max_input_tokens\": 512,\n",
    "    \"max_target_tokens\": 256,\n",
    "    \"sample_size\": 2000,\n",
    "    \"filter_by_length\": True,\n",
    "    \"split_train_frac\": 0.8,\n",
    "\n",
    "    # Prompt\n",
    "    \"prompt_prefix\": \"Summarize: \",\n",
    "\n",
    "    # LoRA Adapter\n",
    "    \"lora_r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_target_modules\": [\"q\", \"v\", \"k\"],\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"lora_bias\": \"none\",\n",
    "    \"lora_task_type\": TaskType.SEQ_2_SEQ_LM,\n",
    "\n",
    "    # Training (Turbo Mode)\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": True,\n",
    "    \"num_train_epochs\": 3,                 # Reduced for speed\n",
    "    \"per_device_train_batch_size\": 16,     # Larger batch\n",
    "    \"per_device_eval_batch_size\": 16,\n",
    "    \"gradient_accumulation_steps\": 2,      # Fewer accum steps\n",
    "    \"learning_rate\": 2e-4,                 # Higher LR for faster convergence\n",
    "    \"weight_decay\": 0.05,\n",
    "    \"warmup_steps\": 200,\n",
    "    \"fp16\": False,\n",
    "    \"bf16\": True,                          # If Colab Pro/A100 supports BF16\n",
    "    \"torch_compile\": True,                 # PyTorch 2.0+ compilation\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"optim\": \"adamw_torch\",                # Stable optimizer\n",
    "\n",
    "    # Logging & Checkpointing\n",
    "    \"logging_steps\": 100,                  # Log less often\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"eval_steps\": 500,                     # Evaluate less often\n",
    "    \"save_strategy\": \"steps\",\n",
    "    \"save_steps\": 1000,                    # Save less often\n",
    "    \"save_total_limit\": 3,\n",
    "    \"load_best_model_at_end\": True,\n",
    "    \"metric_for_best_model\": \"rougeL\",\n",
    "    \"greater_is_better\": True,\n",
    "    \"report_to\": \"wandb\",\n",
    "    \"overwrite_output_dir\": True,\n",
    "\n",
    "    # Generation (Inference)\n",
    "    \"gen_num_beams\": 5,\n",
    "    \"gen_length_penalty\": 0.8,\n",
    "    \"gen_early_stopping\": True,\n",
    "\n",
    "    # Random seed\n",
    "    \"seed\": 42,\n",
    "\n",
    "    # Google Drive (Colab)\n",
    "    \"mount_drive\": True,\n",
    "    \"drive_path\": \"MyDrive/ML_models/pdf_summarization\",\n",
    "    \"training_report_filename\": \"training_report.json\",\n",
    "    \"lora_adapter_name\": \"lora_billsum_legal\",\n",
    "}\n",
    "\n",
    "# Timestamped output directories\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "CONFIG[\"output_dir\"] = f\"{CONFIG['lora_adapter_name']}_{timestamp}\"\n",
    "if CONFIG[\"mount_drive\"]:\n",
    "    CONFIG[\"gdrive_output_dir\"] = (\n",
    "        f\"/content/drive/{CONFIG['drive_path']}/{CONFIG['output_dir']}\"\n",
    "    )\n",
    "\n",
    "# Sanity print\n",
    "logger.info(\"Turbo CONFIG:\")\n",
    "for k, v in CONFIG.items():\n",
    "    logger.info(f\"  {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a0224a",
   "metadata": {},
   "source": [
    "## 6. Login to Hugging Face Hub and Weights & Biases\n",
    "\n",
    "You'll need to log in to Hugging Face to download models/datasets and to Weights & Biases for experiment tracking.\n",
    "You can get your Hugging Face token from [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
    "and your W&B API key from [https://wandb.ai/authorize](https://wandb.ai/authorize).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49076cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder, notebook_login\n",
    "\n",
    "try:\n",
    "    if HfFolder.get_token() is None:\n",
    "        logger.info(\"Hugging Face token not found. Please log in.\")\n",
    "        notebook_login()\n",
    "    else:\n",
    "        logger.info(\"Already logged in to Hugging Face Hub.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"An error occurred during Hugging Face login check: {e}\")\n",
    "    logger.info(\"Attempting login...\")\n",
    "    notebook_login()\n",
    "\n",
    "# Login to Weights & Biases\n",
    "try:\n",
    "    wandb.login()\n",
    "    wandb.init(project=\"flan-t5-billsum-lora\", name=CONFIG[\"output_dir\"], config=CONFIG)\n",
    "    logger.info(\"Successfully logged in to W&B and initialized experiment.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not login to W&B: {e}. Ensure you have run `wandb login` or set WANDB_API_KEY.\")\n",
    "    CONFIG[\"report_to\"] = \"tensorboard\" # Fallback to tensorboard\n",
    "    logger.info(\"Falling back to TensorBoard for logging.\")\n",
    "    # No explicit init for tensorboard here, Trainer handles it via TrainingArguments\n",
    "# Note: Use environment variables or notebook secrets to store your tokens securely"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e96255",
   "metadata": {},
   "source": [
    "## 7. Mount Google Drive (Optional)\n",
    "\n",
    "If you want to save your model checkpoints and outputs to Google Drive, mount it here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa3daab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_colab():\n",
    "    \"\"\"Check if the current environment is Google Colab.\"\"\"\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "        \n",
    "if is_colab() and CONFIG[\"mount_drive\"]:\n",
    "    from google.colab import drive\n",
    "    try:\n",
    "        drive.mount('/content/drive')\n",
    "        logger.info(\"Google Drive mounted successfully.\")\n",
    "        # Create the output directory on Drive if it doesn't exist\n",
    "        if not os.path.exists(CONFIG[\"gdrive_output_dir\"]):\n",
    "            os.makedirs(CONFIG[\"gdrive_output_dir\"], exist_ok=True)\n",
    "            logger.info(f\"Created Google Drive output directory: {CONFIG['gdrive_output_dir']}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to mount Google Drive: {e}\")\n",
    "        logger.info(\"Proceeding without Google Drive. Outputs will be saved to Colab ephemeral storage.\")\n",
    "        CONFIG[\"mount_drive\"] = False # Disable drive features if mount fails\n",
    "else:\n",
    "    logger.info(\"Not running in Colab or Google Drive is disabled in the configuration.\")\n",
    "    CONFIG[\"mount_drive\"] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559ed30a",
   "metadata": {},
   "source": [
    "## 8. Load Model, Tokenizer and Configure LoRA\n",
    "\n",
    "Loads the Flan-T5-base model and tokenizer from Hugging Face, configures the model for LoRA training and returns the model and tokenizer objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe80f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install latest bitsandbytes and required dependencies\n",
    "%pip install -U bitsandbytes --no-cache-dir -q\n",
    "%pip install accelerate --upgrade -q\n",
    "%pip install transformers --upgrade -q\n",
    "\n",
    "# Import required libraries\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, BitsAndBytesConfig\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "# Function to verify CUDA and bitsandbytes setup\n",
    "def verify_installation():\n",
    "    logger.info(f\"PyTorch version: {torch.__version__}\")\n",
    "    logger.info(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    logger.info(f\"bitsandbytes version: {bnb.__version__}\")\n",
    "    \n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"CUDA is not available. 4-bit quantization requires a CUDA-enabled GPU.\")\n",
    "    \n",
    "    # Test BitsAndBytes CUDA kernels\n",
    "    try:\n",
    "        _ = bnb.matmul(torch.zeros(2, 2).cuda(), torch.zeros(2, 2).cuda())\n",
    "        logger.info(\"BitsAndBytes CUDA kernels working correctly\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"BitsAndBytes CUDA test failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Verify installation\n",
    "verify_installation()\n",
    "\n",
    "# Load tokenizer\n",
    "logger.info(f\"Loading tokenizer for model: {CONFIG['model_name']}\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        CONFIG[\"model_name\"],\n",
    "        use_fast=True,\n",
    "        padding_side=\"right\",\n",
    "        model_max_length=CONFIG[\"max_input_tokens\"]\n",
    "    )\n",
    "    logger.info(\"Tokenizer loaded successfully\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load tokenizer: {e}\")\n",
    "    raise\n",
    "\n",
    "\n",
    "# Configure quantization\n",
    "logger.info(\"Configuring 4-bit quantization...\")\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # Changed from bfloat16 for better compatibility\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "# Load model with proper error handling\n",
    "logger.info(f\"Loading base model: {CONFIG['model_name']} with 4-bit quantization...\")\n",
    "try:\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        CONFIG[\"model_name\"],\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,  # Match compute dtype\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    logger.info(\"Model loaded successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load model: {e}\")\n",
    "    logger.info(\"Attempting to load without quantization as fallback...\")\n",
    "    try:\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            CONFIG[\"model_name\"],\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "        logger.warning(\"Model loaded without quantization\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load model even without quantization: {e}\")\n",
    "        raise\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "logger.info(\"Preparing model for k-bit training...\")\n",
    "try:\n",
    "    model = prepare_model_for_kbit_training(\n",
    "        model,\n",
    "        use_gradient_checkpointing=CONFIG[\"gradient_checkpointing\"]\n",
    "    )\n",
    "    logger.info(\"Model prepared for k-bit training\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to prepare model for kbit training: {e}\")\n",
    "    raise\n",
    "\n",
    "# LoRA configuration and application\n",
    "lora_config = LoraConfig(\n",
    "    r=CONFIG[\"lora_r\"],\n",
    "    lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "    target_modules=CONFIG[\"lora_target_modules\"],\n",
    "    lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "    bias=CONFIG[\"lora_bias\"],\n",
    "    task_type=CONFIG[\"lora_task_type\"],\n",
    ")\n",
    "\n",
    "logger.info('Applying LoRA to the model')\n",
    "try:\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    logger.info(\"LoRA configured and applied to the model\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to apply LoRA to the model: {e}\")\n",
    "    raise\n",
    "\n",
    "# Print model statistics\n",
    "model.print_trainable_parameters()\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9673f4b4",
   "metadata": {},
   "source": [
    "## 9. Load and Preprocess Dataset\n",
    "\n",
    "Load the BillSum dataset, preprocess it for Flan-T5, and split into training and evaluation sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd47db1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    text = text.strip()\n",
    "    text = \" \".join(text.split())\n",
    "    # Only strip metadata if required\n",
    "    text = re.sub(r'\\s*\\([^\\)]{0,40}\\)\\s*', ' ', text)  # remove very short inlines\n",
    "    text = re.sub(r'\\s*\\[[^\\]]{0,40}\\]\\s*', ' ', text)\n",
    "    return text\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    try:\n",
    "        input_texts = examples.get(CONFIG[\"text_col\"], [])\n",
    "        summary_texts = examples.get(CONFIG[\"summary_col\"], [])\n",
    "\n",
    "        if not input_texts:\n",
    "            raise ValueError(\"Empty input text\")\n",
    "\n",
    "        cleaned_inputs = [clean_text(doc) for doc in input_texts]\n",
    "\n",
    "        prompts = [f\"{CONFIG[\"prompt_prefix\"]} {doc}\" for doc in cleaned_inputs]\n",
    "\n",
    "        print(f\"Raw: {input_texts[0][:150]}...\")\n",
    "        print(f\"Cleaned: {cleaned_inputs[0][:150]}...\")\n",
    "\n",
    "        model_inputs = tokenizer(\n",
    "            prompts,\n",
    "            max_length=CONFIG[\"max_input_tokens\"] - 32,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        # Use plain summaries â€” no extra formatting\n",
    "        summaries = [s if s else \"No summary provided.\" for s in summary_texts]\n",
    "\n",
    "        labels = tokenizer(\n",
    "            summaries,\n",
    "            max_length=CONFIG[\"max_target_tokens\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Preprocessing failed: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b54f4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.warning(\"Attempting to update some libraries.\")\n",
    "%pip install datasets --upgrade -q\n",
    "%pip install fsspec --upgrade -q\n",
    "%pip install pyarrow --upgrade -q\n",
    "logger.warning(\"Library update attempts finished. If issues persist, ensure runtime was restarted after updates.\")\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "logger.info(f\"Starting dataset loading and processing for: {CONFIG['dataset_name']}\")\n",
    "\n",
    "dataset = None\n",
    "load_dataset_kwargs = {\n",
    "    \"path\": CONFIG[\"dataset_name\"],\n",
    "    \"split\": f\"train[:{CONFIG['sample_size']}]\",\n",
    "}\n",
    "\n",
    "try:\n",
    "    logger.info(f\"Loading dataset with streaming=False and split sample size: {CONFIG['sample_size']}\")\n",
    "    dataset = load_dataset(\n",
    "        **load_dataset_kwargs,\n",
    "    )\n",
    "    logger.info(\"Dataset loaded successfully with streaming=False\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Attempt 1 failed: {str(e)}\")\n",
    "    logger.info(\"Attempting alternative loading method using pandas...\")\n",
    "    try:\n",
    "        # Define splits and paths\n",
    "        splits = {\n",
    "            'train': 'data/train-00000-of-00001.parquet',\n",
    "            'test': 'data/test-00000-of-00001.parquet',\n",
    "            'ca_test': 'data/ca_test-00000-of-00001.parquet'\n",
    "        }\n",
    "        \n",
    "        # Load training data using pandas\n",
    "        df = pd.read_parquet(f\"hf://datasets/FiscalNote/billsum/{splits['train']}\")\n",
    "        \n",
    "        # Convert to Dataset format\n",
    "        from datasets import Dataset\n",
    "        dataset = Dataset.from_pandas(df)\n",
    "        \n",
    "        # Sample if needed\n",
    "        if CONFIG['sample_size'] and CONFIG['sample_size'] < len(dataset):\n",
    "            dataset = dataset.shuffle(seed=CONFIG['seed']).select(range(CONFIG['sample_size']))\n",
    "            \n",
    "        logger.info(\"Dataset loaded successfully using pandas alternative method\")\n",
    "        \n",
    "    except Exception as e2:\n",
    "        logger.error(f\"Alternative loading method also failed: {str(e2)}\")\n",
    "        raise RuntimeError(f\"Both loading methods failed. Original error: {str(e)}, Alternative error: {str(e2)}\")\n",
    "        \n",
    "if dataset is None:\n",
    "    logger.critical(\"FATAL: Dataset could not be loaded by any implemented method.\")\n",
    "    raise RuntimeError(\"Dataset loading failed. Please check the dataset name and parameters.\")\n",
    "\n",
    "\n",
    "logger.info(f\"Dataset loaded. Original columns: {dataset.column_names}\")\n",
    "logger.info(f\"Number of samples in loaded dataset: {len(dataset)}\")\n",
    "\n",
    "if 'text' in dataset.column_names:\n",
    "    dataset = dataset.rename_column('text', 'article')\n",
    "    logger.info(\"Renamed dataset column: 'text' -> 'article'\")\n",
    "    CONFIG['text_col'] = 'article'\n",
    "    logger.info(f\"Updated CONFIG['text_col'] to '{CONFIG['text_col']}'\")\n",
    "else:\n",
    "    logger.warning(f\"'text' column not found in dataset columns: {dataset.column_names}. Skipping rename. Current text column in CONFIG: {CONFIG['text_col']}\")\n",
    "\n",
    "logger.info(\"Creating dataset splits...\")\n",
    "try:\n",
    "    total_size = len(dataset)\n",
    "    logger.info(f\"Total dataset size for splitting: {total_size}\")\n",
    "\n",
    "    train_size = int(total_size * CONFIG[\"split_train_frac\"])\n",
    "    eval_size = total_size - train_size\n",
    "\n",
    "    if train_size <= 0 or eval_size <= 0:\n",
    "        logger.error(f\"Calculated train_size ({train_size}) or eval_size ({eval_size}) is non-positive. Aborting split.\")\n",
    "        raise ValueError(\"Train or evaluation set size is not positive. Check dataset size and split_train_frac.\")\n",
    "\n",
    "    dataset_shuffled = dataset.shuffle(seed=CONFIG[\"seed\"]) # Shuffle before selecting\n",
    "    train_dataset = dataset_shuffled.select(range(train_size))\n",
    "    eval_dataset = dataset_shuffled.select(range(train_size, total_size))\n",
    "\n",
    "    logger.info(f\"Created splits - Train: {len(train_dataset)}, Eval: {len(eval_dataset)}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error creating dataset splits: {e}\")\n",
    "    raise\n",
    "\n",
    "logger.info(\"Processing datasets (tokenization, etc.)...\")\n",
    "try:\n",
    "    tokenized_datasets = {\n",
    "        'train': train_dataset.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            remove_columns=train_dataset.column_names\n",
    "        ),\n",
    "        'eval': eval_dataset.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            remove_columns=eval_dataset.column_names\n",
    "        )\n",
    "    }\n",
    "    logger.info(\"Dataset processing complete.\")\n",
    "    logger.info(f\"Final tokenized dataset sizes - Training samples: {len(tokenized_datasets['train'])}, Evaluation samples: {len(tokenized_datasets['eval'])}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error processing datasets: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56a9f58",
   "metadata": {},
   "source": [
    "## 10. Define Training Arguments\n",
    "\n",
    "Configure the training process using Seq2SeqTrainingArguments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dd1a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=CONFIG[\"output_dir\"],\n",
    "    num_train_epochs=CONFIG[\"num_train_epochs\"],\n",
    "    per_device_train_batch_size=CONFIG[\"per_device_train_batch_size\"],\n",
    "    per_device_eval_batch_size=CONFIG[\"per_device_eval_batch_size\"],\n",
    "    gradient_accumulation_steps=4,  \n",
    "    learning_rate=CONFIG[\"learning_rate\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"],\n",
    "    warmup_steps=CONFIG[\"warmup_steps\"],\n",
    "    \n",
    "    # Optimization\n",
    "    fp16=CONFIG[\"fp16\"],\n",
    "    fp16_full_eval=True,\n",
    "    bf16=False,\n",
    "    optim=\"adafactor\",\n",
    "    \n",
    "    # Logging & Evaluation\n",
    "    logging_dir=f\"{CONFIG['output_dir']}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,  # More frequent logging\n",
    "    eval_strategy=CONFIG[\"evaluation_strategy\"],\n",
    "    eval_steps=100,\n",
    "    \n",
    "    # Saving\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,  # Match with eval_steps\n",
    "    save_total_limit=3,  # Keep three checkpoints\n",
    "    \n",
    "    # Model Loading\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rougeL\",\n",
    "    greater_is_better=True,\n",
    "    \n",
    "    # Generation\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=CONFIG[\"max_target_tokens\"],\n",
    "    generation_num_beams=CONFIG[\"gen_num_beams\"],\n",
    "    \n",
    "    # Other\n",
    "    report_to=CONFIG[\"report_to\"],\n",
    "    seed=CONFIG[\"seed\"],\n",
    "    gradient_checkpointing=CONFIG[\"gradient_checkpointing\"],\n",
    "    overwrite_output_dir=CONFIG[\"overwrite_output_dir\"],\n",
    ")\n",
    "\n",
    "logger.info(f\"Training arguments: {training_args}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78898be",
   "metadata": {},
   "source": [
    "## 11. Define Metrics Computation\n",
    "\n",
    "Function to compute ROUGE and BLEU scores for evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4970687",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "import numpy as np\n",
    "import nltk\n",
    "from typing import Dict, List\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def get_metrics():\n",
    "    \"\"\"Load and cache evaluation metrics.\"\"\"\n",
    "    return {\n",
    "        \"rouge\": evaluate.load(\"rouge\"),\n",
    "        \"bleu\": evaluate.load(\"bleu\"),\n",
    "        \"bertscore\": evaluate.load(\"bertscore\")\n",
    "    }\n",
    "\n",
    "def process_texts(texts: List[str]) -> List[str]:\n",
    "    \"\"\"Clean and process texts for evaluation.\"\"\"\n",
    "    return [\"\\n\".join(nltk.sent_tokenize(text.strip())) for text in texts]\n",
    "\n",
    "def compute_metrics(eval_preds, batch_size: int = 32) -> Dict[str, float]:\n",
    "    \"\"\"Compute evaluation metrics with improved error handling and statistics.\"\"\"\n",
    "    try:\n",
    "        metrics = get_metrics()\n",
    "        preds, labels = eval_preds\n",
    "        if isinstance(preds, tuple):\n",
    "            preds = preds[0]\n",
    "\n",
    "        # Decode predictions and labels\n",
    "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        # Process texts\n",
    "        decoded_preds = process_texts(decoded_preds)\n",
    "        decoded_labels = process_texts(decoded_labels)\n",
    "\n",
    "        # Compute metrics\n",
    "        rouge_results = metrics[\"rouge\"].compute(\n",
    "            predictions=decoded_preds, \n",
    "            references=decoded_labels\n",
    "        )\n",
    "        \n",
    "        decoded_labels_bleu = [[label] for label in decoded_labels]\n",
    "        bleu_results = metrics[\"bleu\"].compute(\n",
    "            predictions=decoded_preds, \n",
    "            references=decoded_labels_bleu\n",
    "        )\n",
    "\n",
    "        # Compute additional statistics\n",
    "        pred_lengths = [len(p.split()) for p in decoded_preds]\n",
    "        ref_lengths = [len(r.split()) for r in decoded_labels]\n",
    "\n",
    "        results = {\n",
    "            \"rouge1\": rouge_results[\"rouge1\"],\n",
    "            \"rouge2\": rouge_results[\"rouge2\"],\n",
    "            \"rougeL\": rouge_results[\"rougeL\"],\n",
    "            \"rougeLsum\": rouge_results[\"rougeLsum\"],\n",
    "            \"bleu\": bleu_results[\"bleu\"],\n",
    "            \"avg_pred_length\": np.mean(pred_lengths),\n",
    "            \"avg_ref_length\": np.mean(ref_lengths),\n",
    "            \"compression_ratio\": np.mean([p/r for p, r in zip(pred_lengths, ref_lengths)])\n",
    "        }\n",
    "\n",
    "        # Add generation length metric\n",
    "        prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "        results[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "        return {k: round(v, 4) for k, v in results.items()}\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error computing metrics: {e}\")\n",
    "        return {\n",
    "            \"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0,\n",
    "            \"rougeLsum\": 0.0, \"bleu\": 0.0, \"gen_len\": 0,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "logger.info(\"Enhanced metrics computation function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b54a510",
   "metadata": {},
   "source": [
    "## 11. Initialize Trainer\n",
    "\n",
    "Set up the `Seq2SeqTrainer` with the model, arguments, datasets, tokenizer, and metrics function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601cfed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback,TrainerCallback\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Clear unused memory before training\"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print_memory_usage()\n",
    "\n",
    "class MemoryTrackingCallback(TrainerCallback):\n",
    "    \"\"\"Callback to track memory usage during training\"\"\"\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % 100 == 0:  # Monitor every 100 steps\n",
    "            print_memory_usage()\n",
    "\n",
    "def validate_training_args(args, model):\n",
    "    \"\"\"Validate training arguments for potential issues\"\"\"\n",
    "    if args.per_device_train_batch_size * args.gradient_accumulation_steps > 64:\n",
    "        logger.warning(\"Total batch size might be too large for available memory\")\n",
    "    \n",
    "    if args.fp16 and not torch.cuda.is_available():\n",
    "        raise ValueError(\"FP16 requires CUDA\")\n",
    "\n",
    "# Clear memory before initialization\n",
    "clear_memory()\n",
    "\n",
    "# Initialize data collator with error handling\n",
    "try:\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer,\n",
    "        model=model,\n",
    "        label_pad_token_id=tokenizer.pad_token_id,\n",
    "        pad_to_multiple_of=8 if CONFIG[\"fp16\"] else None\n",
    "    )\n",
    "    logger.info(\"Data collator initialized successfully\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to initialize data collator: {e}\")\n",
    "    raise\n",
    "\n",
    "# Validate training arguments\n",
    "validate_training_args(training_args, model)\n",
    "\n",
    "# Initialize trainer with enhanced monitoring\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"eval\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(early_stopping_patience=3),\n",
    "        MemoryTrackingCallback()\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger.info(\"Trainer initialized with enhanced monitoring\")\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d90adf",
   "metadata": {},
   "source": [
    "## 12. Train the Model\n",
    "\n",
    "Start the fine-tuning process. This will take some time depending on the dataset size and hardware. ðŸ¥³\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fc3bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_checkpoint(checkpoint_dir):\n",
    "    \"\"\"Find most recent checkpoint in the directory\"\"\"\n",
    "    checkpoints = [d for d in os.listdir(checkpoint_dir) \n",
    "                  if d.startswith(\"checkpoint-\")]\n",
    "    if not checkpoints:\n",
    "        return None\n",
    "    return os.path.join(checkpoint_dir, \n",
    "                       sorted(checkpoints, key=lambda x: int(x.split(\"-\")[1]))[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce148d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Starting training...\")\n",
    "try:\n",
    "    # Setup checkpoint directory\n",
    "    checkpoint_dir = os.path.join(CONFIG[\"output_dir\"], \"checkpoints\")\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # Check for existing checkpoint\n",
    "    resume_checkpoint = get_latest_checkpoint(checkpoint_dir)\n",
    "    if resume_checkpoint:\n",
    "        logger.info(f\"Resuming from checkpoint: {resume_checkpoint}\")\n",
    "    \n",
    "    # Print training info\n",
    "    logger.info(\"Training Configuration:\")\n",
    "    logger.info(f\"- Number of training examples: {len(trainer.train_dataset)}\")\n",
    "    logger.info(f\"- Number of validation examples: {len(trainer.eval_dataset)}\")\n",
    "    logger.info(f\"- Training Epochs: {CONFIG['num_train_epochs']}\")\n",
    "    logger.info(f\"- Batch size: {CONFIG['per_device_train_batch_size']}\")\n",
    "    \n",
    "    # Execute training\n",
    "    train_result = trainer.train(resume_from_checkpoint=resume_checkpoint)\n",
    "    \n",
    "    # Log final metrics\n",
    "    metrics = train_result.metrics\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "    \n",
    "    logger.info(\"Training completed successfully!\")\n",
    "    logger.info(f\"Final Training Loss: {metrics.get('train_loss', 'N/A')}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Training failed: {e}\")\n",
    "    if wandb.run:\n",
    "        wandb.log({\"training_error\": str(e)})\n",
    "        wandb.run.finish(exit_code=1)\n",
    "    raise e\n",
    "\n",
    "finally:\n",
    "    print_memory_usage()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0458a99f",
   "metadata": {},
   "source": [
    "## 13. Evaluate the Model\n",
    "\n",
    "Evaluate the fine-tuned model on the evaluation set to get final performance metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e88d6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.log(\"Evaluating model...\")\n",
    "eval_metrics = trainer.evaluate()\n",
    "\n",
    "logger.info(\"Evaluation metrics:\")\n",
    "for key, value in eval_metrics.items():\n",
    "    logger.info(f\"{key}: {value}\")\n",
    "\n",
    "# Log evaluation metrics\n",
    "trainer.log_metrics(\"eval\", eval_metrics)\n",
    "trainer.save_metrics(\"eval\", eval_metrics) # Saves to all_results.json\n",
    "\n",
    "# Prepare the training_report.json\n",
    "training_report = {\n",
    "    \"model_name\": CONFIG[\"model_name\"],\n",
    "    \"dataset_name\": CONFIG[\"dataset_name\"],\n",
    "    \"lora_adapter_name\": CONFIG[\"lora_adapter_name\"],\n",
    "    \"output_directory\": CONFIG[\"output_dir\"],\n",
    "    \"training_arguments\": {k: str(v) if isinstance(v, (torch.device, BitsAndBytesConfig)) else v for k, v in training_args.to_dict().items()}, # Convert non-serializable items\n",
    "    \"train_metrics\": trainer.state.log_history[:-1], # All logged steps except final eval\n",
    "    \"eval_metrics\": eval_metrics,\n",
    "    \"final_training_loss\": trainer.state.log_history[-2].get('loss') if len(trainer.state.log_history) > 1 and 'loss' in trainer.state.log_history[-2] else trainer.state.log_history[-1].get('train_loss', 'N/A')\n",
    "}\n",
    "\n",
    "\n",
    "# Add ROUGE and BLEU from eval_metrics to the top level for easier access\n",
    "for metric_key in [\"eval_rouge1\", \"eval_rouge2\", \"eval_rougeL\", \"eval_bleu\"]:\n",
    "    if metric_key in eval_metrics:\n",
    "        training_report[metric_key.replace(\"eval_\", \"\")] = eval_metrics[metric_key]\n",
    "\n",
    "\n",
    "# Save training_report.json locally\n",
    "report_path = os.path.join(CONFIG[\"output_dir\"], CONFIG[\"training_report_filename\"])\n",
    "with open(report_path, \"w\") as f:\n",
    "    json.dump(training_report, f, indent=4)\n",
    "logger.info(f\"Training report saved to {report_path}\")\n",
    "\n",
    "if wandb.run:\n",
    "    wandb.log(eval_metrics) # Log final eval metrics\n",
    "    wandb.save(report_path) # Save report to W&B artifacts\n",
    "    logger.info(\"Evaluation metrics and report logged to W&B.\")\n",
    "\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305442e8",
   "metadata": {},
   "source": [
    "## 14. Save Model and LoRA Adapter\n",
    "\n",
    "Save the fine-tuned LoRA adapter and the full model if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7083fe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the LoRA adapter\n",
    "lora_adapter_path = os.path.join(CONFIG[\"output_dir\"], CONFIG[\"lora_adapter_name\"])\n",
    "model.save_pretrained(lora_adapter_path) # Saves only the LoRA adapter\n",
    "tokenizer.save_pretrained(lora_adapter_path) # Save tokenizer with adapter\n",
    "logger.info(f\"LoRA adapter and tokenizer saved to {lora_adapter_path}\")\n",
    "\n",
    "# To save the full model (optional, requires more space)\n",
    "# merged_model_path = os.path.join(CONFIG[\"output_dir\"], \"merged_model_flan_t5_base_billsum\")\n",
    "# try:\n",
    "#     # Merge LoRA weights with the base model\n",
    "#     merged_model = model.merge_and_unload()\n",
    "#     merged_model.save_pretrained(merged_model_path)\n",
    "#     tokenizer.save_pretrained(merged_model_path)\n",
    "#     logger.info(f\"Full merged model saved to {merged_model_path}\")\n",
    "# except Exception as e:\n",
    "#     logger.error(f\"Could not merge and save full model: {e}. This might happen if the base model is not fully on CPU or due to memory constraints.\")\n",
    "#     logger.info(\"Only the LoRA adapter was saved.\")\n",
    "\n",
    "\n",
    "# If Google Drive is mounted, copy outputs there\n",
    "if CONFIG[\"mount_drive\"] and os.path.exists(CONFIG[\"gdrive_output_dir\"]):\n",
    "    logger.info(f\"Copying outputs to Google Drive: {CONFIG['gdrive_output_dir']}\")\n",
    "    # Copy LoRA adapter\n",
    "    gdrive_lora_path = os.path.join(CONFIG[\"gdrive_output_dir\"], CONFIG[\"lora_adapter_name\"])\n",
    "    if os.path.exists(gdrive_lora_path):\n",
    "        logger.info(f\"Removing existing LoRA adapter from GDrive: {gdrive_lora_path}\")\n",
    "        os.system(f\"rm -rf '{gdrive_lora_path}'\") # Use os.system for `rm -rf`\n",
    "    os.system(f\"cp -r '{lora_adapter_path}' '{CONFIG['gdrive_output_dir']}/'\")\n",
    "    logger.info(f\"LoRA adapter copied to {gdrive_lora_path}\")\n",
    "\n",
    "    # Copy training report\n",
    "    gdrive_report_path = os.path.join(CONFIG[\"gdrive_output_dir\"], CONFIG[\"training_report_filename\"])\n",
    "    os.system(f\"cp '{report_path}' '{gdrive_report_path}'\")\n",
    "    logger.info(f\"Training report copied to {gdrive_report_path}\")\n",
    "\n",
    "    # Copy all_results.json (contains eval metrics)\n",
    "    all_results_path = os.path.join(CONFIG[\"output_dir\"], \"all_results.json\")\n",
    "    if os.path.exists(all_results_path):\n",
    "        gdrive_all_results_path = os.path.join(CONFIG[\"gdrive_output_dir\"], \"all_results.json\")\n",
    "        os.system(f\"cp '{all_results_path}' '{gdrive_all_results_path}'\")\n",
    "        logger.info(f\"all_results.json copied to {gdrive_all_results_path}\")\n",
    "\n",
    "    # If merged model was saved and exists, copy it too\n",
    "    # if 'merged_model' in locals() and os.path.exists(merged_model_path):\n",
    "    #     gdrive_merged_model_path = os.path.join(CONFIG[\"gdrive_output_dir\"], \"merged_model_flan_t5_base_billsum\")\n",
    "    #     if os.path.exists(gdrive_merged_model_path):\n",
    "    #         logger.info(f\"Removing existing merged model from GDrive: {gdrive_merged_model_path}\")\n",
    "    #         os.system(f\"rm -rf '{gdrive_merged_model_path}'\")\n",
    "    #     os.system(f\"cp -r '{merged_model_path}' '{CONFIG['gdrive_output_dir']}/'\")\n",
    "    #     logger.info(f\"Full merged model copied to {gdrive_merged_model_path}\")\n",
    "else:\n",
    "    logger.warning(\"Google Drive not mounted or GDrive output path does not exist. Outputs saved locally.\")\n",
    "\n",
    "if wandb.run:\n",
    "    # Log LoRA adapter as artifact if desired\n",
    "    # lora_artifact = wandb.Artifact(CONFIG[\"lora_adapter_name\"], type=\"model\")\n",
    "    # lora_artifact.add_dir(lora_adapter_path)\n",
    "    # wandb.log_artifact(lora_artifact)\n",
    "    # logger.info(f\"LoRA adapter logged as W&B artifact: {CONFIG['lora_adapter_name']}\")\n",
    "    wandb.finish()\n",
    "\n",
    "logger.info(\"Script finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5720cd",
   "metadata": {},
   "source": [
    "## Test the model with a sample input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed36bdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test document - Cybersecurity and Privacy Protection Act\n",
    "test_document = \"\"\"\n",
    "CYBERSECURITY AND PRIVACY PROTECTION ACT OF 2025\n",
    "\n",
    "SECTION 1. SHORT TITLE AND PURPOSE\n",
    "\n",
    "    (a) This Act may be cited as the 'Cybersecurity and Privacy Protection Act of 2025'.\n",
    "    (b) The purpose of this Act is to enhance cybersecurity measures and protect individual privacy in the digital age.\n",
    "\n",
    "SECTION 2. DEFINITIONS\n",
    "\n",
    "In this Act:\n",
    "    (1) 'Personal Data' means any information relating to an identified or identifiable natural person.\n",
    "    (2) 'Data Controller' means any entity that determines the purposes and means of processing personal data.\n",
    "    (3) 'Critical Infrastructure' means systems and assets vital to national security.\n",
    "\n",
    "SECTION 3. CYBERSECURITY REQUIREMENTS\n",
    "\n",
    "    (a) MANDATORY SECURITY MEASURES.â€”\n",
    "        (1) All Data Controllers shall implement:\n",
    "            (A) End-to-end encryption for data transmission\n",
    "            (B) Multi-factor authentication for system access\n",
    "            (C) Regular security audits and vulnerability assessments\n",
    "\n",
    "    (b) INCIDENT REPORTING.â€”\n",
    "        (1) Data Controllers shall report any security breach within 48 hours.\n",
    "        (2) Penalties for non-compliance shall be up to $500,000 per incident.\n",
    "\n",
    "SECTION 4. PRIVACY PROTECTIONS\n",
    "\n",
    "    (a) CONSENT REQUIREMENTS.â€”\n",
    "        (1) Explicit consent required for data collection\n",
    "        (2) Right to access and delete personal data\n",
    "        (3) Annual privacy impact assessments\n",
    "\n",
    "    (b) CHILDREN'S PRIVACY.â€”\n",
    "        (1) Enhanced protections for users under 13\n",
    "        (2) Parental consent requirements\n",
    "\n",
    "SECTION 5. ENFORCEMENT\n",
    "\n",
    "    (a) The Federal Trade Commission shall enforce this Act.\n",
    "    (b) State Attorneys General may bring civil actions.\n",
    "\n",
    "SECTION 6. AUTHORIZATION OF APPROPRIATIONS\n",
    "\n",
    "    There is authorized to be appropriated $275,000,000 for fiscal year 2026 to carry out this Act.\n",
    "\"\"\"\n",
    "\n",
    "# Test the preprocessing function\n",
    "\n",
    "## adding trained lora adapter to the model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    lora_adapter_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.load_adapter(lora_adapter_path, CONFIG[\"lora_adapter_name\"])\n",
    "model.set_adapter(CONFIG[\"lora_adapter_name\"])\n",
    "model.eval()\n",
    "model.to(\"cuda:0\")\n",
    "# Tokenize the test document\n",
    "inputs = tokenizer(test_document, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda:0\")\n",
    "# Generate summary\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs)\n",
    "summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated Summary:\")\n",
    "print(summary)\n",
    "print_memory_usage()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
