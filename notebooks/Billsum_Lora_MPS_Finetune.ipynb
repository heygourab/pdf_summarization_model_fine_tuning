{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b37ddabf",
   "metadata": {},
   "source": [
    "# Fine-tuning Flan-T5-base for Legal Document Summarization\n",
    "\n",
    "This notebook is a modified version of the original notebook by Gourab S. (@heygourab).\n",
    "Author: Gourab S. (@heygourab)\n",
    "This notebook demonstrates fine-tuning the Flan-T5-base model on the BillSum dataset using LoRA (Low-Rank Adaptation). We'll use the Hugging Face ecosystem (`transformers`, `datasets`, `peft`) for efficient fine-tuning.\n",
    "\n",
    "## Setup Overview\n",
    "\n",
    "- Base Model: google/flan-t5-base\n",
    "- Dataset: BillSum (~2000 samples)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "This notebook assumes a Colab environment with a GPU available. If you're running this locally, make sure to install the required packages and set up your GPU environment accordingly.\n",
    "[Open in Colab](https://colab.research.google.com/github/heygourab/pdf_summarization_model_fine_tuning/blob/main/notebooks/billsum_lora_finetune_colab.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e4d2ae",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, let's install the required dependencies and set up GPU monitoring.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b6b71a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# M1/M2 Mac Setup for PyTorch with MPS (Metal GPU)\n",
    "%pip install -q torch torchvision torchaudio\n",
    "%pip install -q transformers datasets accelerate evaluate peft nltk wandb omegaconf fsspec pyarrow\n",
    "# Note: bitsandbytes is not supported on M1/M2 Mac with MPS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37f3264a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS (Metal Performance Shaders) is available! Using MPS for acceleration.\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# 🔥 Device setup for M1/M2 (Metal Performance Shaders)\n",
    "import torch\n",
    "\n",
    "def get_device():\n",
    "    if torch.backends.mps.is_available():\n",
    "        print(\"MPS (Metal Performance Shaders) is available! Using MPS for acceleration.\")\n",
    "        return torch.device('mps')\n",
    "    elif torch.cuda.is_available():\n",
    "        print(\"CUDA is available! Using CUDA for acceleration.\")\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        print(\"No GPU acceleration available. Using CPU.\")\n",
    "        return torch.device('cpu')\n",
    "\n",
    "device = get_device()\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4a9ea0",
   "metadata": {},
   "source": [
    "## 2. Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a8c4ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import nltk\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import sys\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType\n",
    ")\n",
    "import wandb\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba69578d",
   "metadata": {},
   "source": [
    "## 3. Logger setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59c4567e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-21 15:53:05 — train_logger — INFO — Logger initialized: train_logger\n",
      "2025-05-21 15:53:05 — train_logger — INFO — Log file created at: /Users/gourabsarkar/Developer/college_project/pdf_summarization_model_fine_tuning/notebooks/logs/training_20250521_155305.log\n",
      "2025-05-21 15:53:05 — train_logger — INFO — Python version: 3.10.17 (main, Apr  8 2025, 12:10:59) [Clang 16.0.0 (clang-1600.0.26.6)]\n"
     ]
    }
   ],
   "source": [
    "def setup_logger(name=\"train_logger\", level=logging.INFO, log_file=None):\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level)\n",
    "    logger.propagate = False  # Avoid duplicate logs\n",
    "\n",
    "    # Clear existing handlers\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "\n",
    "    # Formatter for log messages\n",
    "    formatter = logging.Formatter(\n",
    "        fmt='%(asctime)s — %(name)s — %(levelname)s — %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S'\n",
    "    )\n",
    "\n",
    "    # Console handler setup\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setFormatter(formatter)\n",
    "    logger.addHandler(console_handler)\n",
    "\n",
    "    # File handler setup\n",
    "    if log_file is None:\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        log_dir = os.path.join(os.getcwd(), 'logs')  # Safe fallback to current dir\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        log_file = os.path.join(log_dir, f'training_{timestamp}.log')\n",
    "    else:\n",
    "        log_dir = os.path.dirname(log_file)\n",
    "        if log_dir:\n",
    "            os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    file_handler = logging.FileHandler(log_file)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "    # Log header\n",
    "    logger.info(f\"Logger initialized: {name}\")\n",
    "    logger.info(f\"Log file created at: {os.path.abspath(log_file)}\")\n",
    "    logger.info(f\"Python version: {sys.version}\")\n",
    "\n",
    "    return logger\n",
    "\n",
    "# Use it\n",
    "logger = setup_logger(\"train_logger\", logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712d79dc",
   "metadata": {},
   "source": [
    "## 4. Loading the required NLTK libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68fbb34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-21 15:53:06 — train_logger — INFO — Successfully downloaded NLTK resource: punkt\n",
      "2025-05-21 15:53:06 — train_logger — INFO — Successfully downloaded NLTK resource: punkt_tab\n"
     ]
    }
   ],
   "source": [
    "# Download required NLTK data\n",
    "for resource in ['punkt', 'punkt_tab']:\n",
    "    try:\n",
    "        nltk.download(resource, quiet=True)\n",
    "        logger.info(f\"Successfully downloaded NLTK resource: {resource}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error downloading {resource}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d615513",
   "metadata": {},
   "source": [
    "## 5. Memory Usage Monitoring\n",
    "\n",
    "The `print_memory_usage()` function monitors system resource utilization during model training:\n",
    "\n",
    "- Tracks RAM usage by getting the Resident Set Size (RSS) of current process in GB\n",
    "- For GPU-enabled systems:\n",
    "  - Reports allocated GPU memory\n",
    "  - Shows total available GPU memory\n",
    "  - Calculates percentage of GPU memory utilization\n",
    "  - Resets peak memory tracking statistics\n",
    "\n",
    "This helps identify potential memory bottlenecks and optimize resource usage during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "393f44a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-21 15:53:06 — train_logger — INFO — RAM usage: 0.46 GB\n",
      "2025-05-21 15:53:06 — train_logger — INFO — Total system RAM: 8.59 GB\n",
      "2025-05-21 15:53:06 — train_logger — INFO — Using MPS (Metal Performance Shaders) - Memory stats not available\n"
     ]
    }
   ],
   "source": [
    "def print_memory_usage():\n",
    "    process = psutil.Process(os.getpid()) # Get the current process\n",
    "\n",
    "    ram_gb = process.memory_info().rss / 1e9 # Convert bytes to GB\n",
    "    total_gb = psutil.virtual_memory().total / 1e9 # Total system RAM in GB\n",
    "\n",
    "    logger.info(f\"RAM usage: {ram_gb:.2f} GB\") # Current process RAM usage\n",
    "    logger.info(f\"Total system RAM: {total_gb:.2f} GB\") # Total system RAM\n",
    "\n",
    "    # Check for CUDA device\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "        gpu_mem = torch.cuda.memory_allocated() / 1e9\n",
    "        gpu_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        peak_gpu_mem = torch.cuda.max_memory_allocated() / 1e9\n",
    "\n",
    "        logger.info(f\"GPU memory usage: {gpu_mem:.2f}/{gpu_total:.2f} GB ({gpu_mem/gpu_total*100:.1f}%)\")\n",
    "        logger.info(f\"Peak GPU memory: {peak_gpu_mem:.2f} GB\")\n",
    "\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    # Check for MPS device\n",
    "    elif torch.backends.mps.is_available():\n",
    "        # MPS doesn't have built-in memory tracking like CUDA\n",
    "        # We can only log that we're using MPS\n",
    "        logger.info(\"Using MPS (Metal Performance Shaders) - Memory stats not available\")\n",
    "\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9070a4ca",
   "metadata": {},
   "source": [
    "## 6. Configuration Parameters\n",
    "\n",
    "all hyperparameters and configuration settings for the model, dataset, LoRA, and training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580b1886",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import TaskType \n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "CONFIG = {\n",
    "    # ====== MODEL & ARCHITECTURE ======\n",
    "    \"model_name\": \"google/flan-t5-base\",  # Lightweight encoder-decoder model\n",
    "    \"model_type\": \"encoder-decoder\",       # Needed for proper Trainer setup\n",
    "\n",
    "    # ====== DATASET CONFIG ======\n",
    "    \"dataset_name\": \"billsum\",\n",
    "    \"text_col\": \"text\",\n",
    "    \"summary_col\": \"summary\",\n",
    "    \"max_input_tokens\": 512,\n",
    "    \"max_target_tokens\": 384,\n",
    "\n",
    "    \"sample_size\": 1000,  # Increased sample size to stabilize training\n",
    "    \"filter_by_length\": True,\n",
    "    \"split_train_frac\": 0.9,  # Give model more data to learn from\n",
    "\n",
    "    # ====== PROMPT INJECTION ======\n",
    "    \"prompt_prefix\": \"summarize: \",\n",
    "\n",
    "    # ====== LoRA CONFIG ======\n",
    "    \"lora_r\": 8,  # Reduced rank for MPS stability\n",
    "    \"lora_alpha\": 16,  # Lower alpha to reduce overfitting\n",
    "    \"lora_target_modules\": [\"q\", \"k\", \"v\"],  # Cover more layers (T5 specifics)\n",
    "    \"lora_dropout\": 0.1,  # Slightly increased to help generalization\n",
    "    \"lora_bias\": \"none\",\n",
    "    \"lora_task_type\": TaskType.SEQ_2_SEQ_LM,\n",
    "\n",
    "    # ====== TRAINING CONFIG ======\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": True,\n",
    "    \"num_train_epochs\": 3,  # Increased for better convergence\n",
    "    \"per_device_train_batch_size\": 1,  # Lower batch size for stability on MPS\n",
    "    \"per_device_eval_batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 4,  # Reduced to ease memory load (effective batch size = 4)\n",
    "\n",
    "    \"learning_rate\": 5e-5,  # Lowered for stable convergence\n",
    "    \"weight_decay\": 0.01,   # Lowered to avoid underfitting\n",
    "    \"warmup_steps\": 20,     # Faster warmup for small dataset\n",
    "\n",
    "    \"fp16\": False,  # MPS doesn’t support it\n",
    "    \"bf16\": False,\n",
    "    \"torch_compile\": False,  # Not stable on MPS\n",
    "    \"gradient_checkpointing\": True,  # Saves memory\n",
    "\n",
    "    \"optim\": \"adamw_torch\",  # MPS-safe optimizer\n",
    "\n",
    "    # ====== EVALUATION & LOGGING ======\n",
    "    \"logging_steps\": 10,\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"eval_steps\": 100,  # Slightly slower eval to reduce jitter\n",
    "    \"save_strategy\": \"steps\",\n",
    "    \"save_steps\": 100,\n",
    "    \"save_total_limit\": 3,\n",
    "\n",
    "    \"load_best_model_at_end\": True,\n",
    "    \"metric_for_best_model\": \"rougeL\",  # Logical for summarization\n",
    "    \"greater_is_better\": True,\n",
    "    \"report_to\": \"wandb\",  # Replace with \"none\" if not using WandB\n",
    "    \"overwrite_output_dir\": True,\n",
    "\n",
    "    # ====== GENERATION CONFIG ======\n",
    "    \"gen_num_beams\": 4,         # Slightly cheaper beam search\n",
    "    \"gen_length_penalty\": 0.8,\n",
    "    \"gen_early_stopping\": True,\n",
    "\n",
    "    # ====== MISC ======\n",
    "    \"seed\": 42,\n",
    "\n",
    "    # ====== GOOGLE DRIVE SUPPORT (COLAB) ======\n",
    "    \"mount_drive\": False,\n",
    "    \"drive_path\": \"MyDrive/ML_models/pdf_summarization\",\n",
    "    \"training_report_filename\": \"training_report.json\",\n",
    "    \"lora_adapter_name\": \"lora_billsum_legal\",\n",
    "}\n",
    "# ====== OUTPUT DIR HANDLER ======\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "CONFIG[\"output_dir\"] = f\"{CONFIG['lora_adapter_name']}_{timestamp}\"\n",
    "\n",
    "if CONFIG[\"mount_drive\"]:\n",
    "    CONFIG[\"gdrive_output_dir\"] = f\"/content/drive/{CONFIG['drive_path']}/{CONFIG['output_dir']}\"\n",
    "\n",
    "# ====== SANITY LOG ======\n",
    "logger.info(\"🔧 MPS CONFIG DUMP:\")\n",
    "for k, v in CONFIG.items():\n",
    "    logger.info(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a0224a",
   "metadata": {},
   "source": [
    "## 6. Login to Hugging Face Hub and Weights & Biases\n",
    "\n",
    "You'll need to log in to Hugging Face to download models/datasets and to Weights & Biases for experiment tracking.\n",
    "You can get your Hugging Face token from [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
    "and your W&B API key from [https://wandb.ai/authorize](https://wandb.ai/authorize).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49076cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgoheygourab\u001b[0m (\u001b[33mgoheygourab-self\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.4s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/gourabsarkar/Developer/college_project/pdf_summarization_model_fine_tuning/notebooks/wandb/run-20250521_155307-ybvrumm8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/goheygourab-self/flan-t5-billsum-lora/runs/ybvrumm8' target=\"_blank\">run_20250521_155307</a></strong> to <a href='https://wandb.ai/goheygourab-self/flan-t5-billsum-lora' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/goheygourab-self/flan-t5-billsum-lora' target=\"_blank\">https://wandb.ai/goheygourab-self/flan-t5-billsum-lora</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/goheygourab-self/flan-t5-billsum-lora/runs/ybvrumm8' target=\"_blank\">https://wandb.ai/goheygourab-self/flan-t5-billsum-lora/runs/ybvrumm8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import HfFolder, notebook_login\n",
    "\n",
    "try:\n",
    "    if HfFolder.get_token() is None:\n",
    "        logger.info(\"Hugging Face token not found. Please log in.\")\n",
    "        notebook_login()\n",
    "    else:\n",
    "        logger.info(\"Already logged in to Hugging Face Hub.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"An error occurred during Hugging Face login check: {e}\")\n",
    "    logger.info(\"Attempting login...\")\n",
    "    notebook_login()\n",
    "\n",
    "# Login to Weights & Biases\n",
    "try:\n",
    "    wandb.login()\n",
    "    # Use a different run name than output_dir\n",
    "    run_name = f\"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    wandb.init(\n",
    "        project=\"flan-t5-billsum-lora\", \n",
    "        name=run_name,  # Use different run name\n",
    "        config=CONFIG\n",
    "    )\n",
    "    logger.info(\"Successfully logged in to W&B and initialized experiment.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not login to W&B: {e}. Ensure you have run `wandb login` or set WANDB_API_KEY.\")\n",
    "    CONFIG[\"report_to\"] = \"tensorboard\" # Fallback to tensorboard\n",
    "    logger.info(\"Falling back to TensorBoard for logging.\")\n",
    "    # No explicit init for tensorboard here, Trainer handles it via TrainingArguments\n",
    "# Note: Use environment variables or notebook secrets to store your tokens securely\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e96255",
   "metadata": {},
   "source": [
    "## 7. Mount Google Drive (Optional)\n",
    "\n",
    "If you want to save your model checkpoints and outputs to Google Drive, mount it here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faa3daab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_colab():\n",
    "    \"\"\"Check if the current environment is Google Colab.\"\"\"\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "        \n",
    "if is_colab() and CONFIG[\"mount_drive\"]:\n",
    "    from google.colab import drive\n",
    "    try:\n",
    "        drive.mount('/content/drive')\n",
    "        logger.info(\"Google Drive mounted successfully.\")\n",
    "        # Create the output directory on Drive if it doesn't exist\n",
    "        if not os.path.exists(CONFIG[\"gdrive_output_dir\"]):\n",
    "            os.makedirs(CONFIG[\"gdrive_output_dir\"], exist_ok=True)\n",
    "            logger.info(f\"Created Google Drive output directory: {CONFIG['gdrive_output_dir']}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to mount Google Drive: {e}\")\n",
    "        logger.info(\"Proceeding without Google Drive. Outputs will be saved to Colab ephemeral storage.\")\n",
    "        CONFIG[\"mount_drive\"] = False # Disable drive features if mount fails\n",
    "else:\n",
    "    logger.info(\"Not running in Colab or Google Drive is disabled in the configuration.\")\n",
    "    CONFIG[\"mount_drive\"] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559ed30a",
   "metadata": {},
   "source": [
    "## 8. Load Model, Tokenizer and Configure LoRA\n",
    "\n",
    "Loads the Flan-T5-base model and tokenizer from Hugging Face, configures the model for LoRA training and returns the model and tokenizer objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfe80f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gourabsarkar/Developer/college_project/pdf_summarization_model_fine_tuning/.venv/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,327,104 || all params: 248,904,960 || trainable%: 0.5332\n"
     ]
    }
   ],
   "source": [
    "# Install latest bitsandbytes and required dependencies\n",
    "%pip install -U bitsandbytes --no-cache-dir -q\n",
    "%pip install accelerate --upgrade -q\n",
    "%pip install transformers --upgrade -q\n",
    "\n",
    "# Don't install bitsandbytes on Mac M1 as it's not compatible with MPS\n",
    "\n",
    "# Import required libraries\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, BitsAndBytesConfig\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "# Print versions and available devices \n",
    "def check_environment():\n",
    "    logger.info(f\"PyTorch version: {torch.__version__}\")\n",
    "    logger.info(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
    "    logger.info(f\"MPS built: {torch.backends.mps.is_built()}\")\n",
    "    logger.info(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "check_environment()\n",
    "\n",
    "# Function to verify CUDA and bitsandbytes setup\n",
    "def verify_installation():\n",
    "    logger.info(f\"PyTorch version: {torch.__version__}\")\n",
    "    logger.info(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    logger.info(f\"bitsandbytes version: {bnb.__version__}\")\n",
    "    \n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"CUDA is not available. 4-bit quantization requires a CUDA-enabled GPU.\")\n",
    "    \n",
    "    # Test BitsAndBytes CUDA kernels\n",
    "    try:\n",
    "        _ = bnb.matmul(torch.zeros(2, 2).cuda(), torch.zeros(2, 2).cuda())\n",
    "        logger.info(\"BitsAndBytes CUDA kernels working correctly\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"BitsAndBytes CUDA test failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Verify installation\n",
    "# verify_installation()\n",
    "\n",
    "# Load tokenizer\n",
    "logger.info(f\"Loading tokenizer for model: {CONFIG['model_name']}\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        CONFIG[\"model_name\"],\n",
    "        use_fast=True,\n",
    "        padding_side=\"right\",\n",
    "        model_max_length=CONFIG[\"max_input_tokens\"]\n",
    "    )\n",
    "    logger.info(\"Tokenizer loaded successfully\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load tokenizer: {e}\")\n",
    "    raise\n",
    "\n",
    "# Load model for MPS\n",
    "logger.info(f\"Loading base model: {CONFIG['model_name']} for MPS...\")\n",
    "try:\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        CONFIG[\"model_name\"],\n",
    "        device_map=None,  # Don't use auto device mapping on MPS\n",
    "        torch_dtype=torch.float32,  # Use FP32 on MPS for better compatibility\n",
    "        use_cache=False  # Disable KV cache to avoid past_key_values warning\n",
    "    )\n",
    "    # Move model to MPS device after loading\n",
    "    if torch.backends.mps.is_available():\n",
    "        model = model.to(device)\n",
    "    logger.info(f\"Model loaded successfully and moved to {device}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load model: {e}\")\n",
    "    raise\n",
    "\n",
    "# Prepare model for training with proper error handling\n",
    "logger.info(\"Configuring model for LoRA training...\")\n",
    "try:\n",
    "    # Configure LoRA for the model\n",
    "    lora_config = LoraConfig(\n",
    "        r=CONFIG[\"lora_r\"],\n",
    "        lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "        target_modules=CONFIG[\"lora_target_modules\"],\n",
    "        lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "        bias=CONFIG[\"lora_bias\"],\n",
    "        task_type=CONFIG[\"lora_task_type\"],\n",
    "    )\n",
    "    \n",
    "    # Enable gradients before applying LoRA\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    # Explicitly verify trainable parameters\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    logger.info(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")\n",
    "    \n",
    "    logger.info(\"LoRA adapter applied successfully\")\n",
    "    model.print_trainable_parameters()\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to apply LoRA adapter: {e}\")\n",
    "    raise\n",
    "\n",
    "# Show memory usage\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b15b44bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSeq2SeqLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): T5ForConditionalGeneration(\n",
       "      (shared): Embedding(32128, 768)\n",
       "      (encoder): T5Stack(\n",
       "        (embed_tokens): Embedding(32128, 768)\n",
       "        (block): ModuleList(\n",
       "          (0): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (k): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (v): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (relative_attention_bias): Embedding(32, 12)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1-11): 11 x T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (k): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (v): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): T5LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (decoder): T5Stack(\n",
       "        (embed_tokens): Embedding(32128, 768)\n",
       "        (block): ModuleList(\n",
       "          (0): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (k): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (v): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (relative_attention_bias): Embedding(32, 12)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (k): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (v): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1-11): 11 x T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (k): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (v): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (k): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (v): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): T5LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9673f4b4",
   "metadata": {},
   "source": [
    "## 9. Load and Preprocess Dataset\n",
    "\n",
    "Load the BillSum dataset, preprocess it for Flan-T5, and split into training and evaluation sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd47db1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    text = text.strip()\n",
    "    text = \" \".join(text.split())\n",
    "    # Only strip metadata if required\n",
    "    text = re.sub(r'\\s*\\([^\\)]{0,40}\\)\\s*', ' ', text)  # remove very short inlines\n",
    "    text = re.sub(r'\\s*\\[[^\\]]{0,40}\\]\\s*', ' ', text)\n",
    "    return text\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    try:\n",
    "        input_texts = examples.get(CONFIG[\"text_col\"], [])\n",
    "        summary_texts = examples.get(CONFIG[\"summary_col\"], [])\n",
    "\n",
    "        if not input_texts:\n",
    "            raise ValueError(\"Empty input text\")\n",
    "\n",
    "        cleaned_inputs = [clean_text(doc) for doc in input_texts]\n",
    "\n",
    "        prompts = [f'{CONFIG[\"prompt_prefix\"]}{doc}' for doc in cleaned_inputs]\n",
    "\n",
    "        print(f\"Raw: {input_texts[0][:150]}...\")\n",
    "        print(f\"Cleaned: {cleaned_inputs[0][:150]}...\")\n",
    "\n",
    "        model_inputs = tokenizer(\n",
    "            prompts,\n",
    "            max_length=CONFIG[\"max_input_tokens\"] - 32,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        # Use plain summaries — no extra formatting\n",
    "        summaries = [s if s else \"No summary provided.\" for s in summary_texts]\n",
    "\n",
    "        labels = tokenizer(\n",
    "            summaries,\n",
    "            max_length=CONFIG[\"max_target_tokens\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Preprocessing failed: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b54f4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to update some libraries.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Library update attempts finished. If issues persist, ensure runtime was restarted after updates.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc0de26f74d945ada2fd96433cc58f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw: SECTION 1. SHORT TITLE.\n",
      "\n",
      "    This Act may be cited as the ``Border Health Security Act of \n",
      "2006''.\n",
      "\n",
      "SEC. 2. DEFINITIONS.\n",
      "\n",
      "    In this Act:\n",
      "           ...\n",
      "Cleaned: SECTION 1. SHORT TITLE. This Act may be cited as the ``Border Health Security Act of 2006''. SEC. 2. DEFINITIONS. In this Act: Border area.--The term ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c3202d5a6984f3b808e6121c0ddfbaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw: SECTION 1. SHORT TITLE.\n",
      "\n",
      "    This Act may be cited as the ``Financial Oversight Commission Act \n",
      "of 2009''.\n",
      "\n",
      "SEC. 2. ESTABLISHMENT OF COMMISSION.\n",
      "\n",
      "    ...\n",
      "Cleaned: SECTION 1. SHORT TITLE. This Act may be cited as the ``Financial Oversight Commission Act of 2009''. SEC. 2. ESTABLISHMENT OF COMMISSION. There is est...\n"
     ]
    }
   ],
   "source": [
    "logger.warning(\"Attempting to update some libraries.\")\n",
    "%pip install datasets --upgrade -q\n",
    "%pip install fsspec --upgrade -q\n",
    "%pip install pyarrow --upgrade -q\n",
    "logger.warning(\"Library update attempts finished. If issues persist, ensure runtime was restarted after updates.\")\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "logger.info(f\"Starting dataset loading and processing for: {CONFIG['dataset_name']}\")\n",
    "\n",
    "dataset = None\n",
    "load_dataset_kwargs = {\n",
    "    \"path\": CONFIG[\"dataset_name\"],\n",
    "    \"split\": f\"train[:{CONFIG['sample_size']}]\",\n",
    "}\n",
    "\n",
    "try:\n",
    "    logger.info(f\"Loading dataset with streaming=False and split sample size: {CONFIG['sample_size']}\")\n",
    "    dataset = load_dataset(\n",
    "        **load_dataset_kwargs,\n",
    "    )\n",
    "    logger.info(\"Dataset loaded successfully with streaming=False\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Attempt 1 failed: {str(e)}\")\n",
    "    logger.info(\"Attempting alternative loading method using pandas...\")\n",
    "    try:\n",
    "        # Define splits and paths\n",
    "        splits = {\n",
    "            'train': 'data/train-00000-of-00001.parquet',\n",
    "            'test': 'data/test-00000-of-00001.parquet',\n",
    "            'ca_test': 'data/ca_test-00000-of-00001.parquet'\n",
    "        }\n",
    "        \n",
    "        # Load training data using pandas\n",
    "        df = pd.read_parquet(f\"hf://datasets/FiscalNote/billsum/{splits['train']}\")\n",
    "        \n",
    "        # Convert to Dataset format\n",
    "        from datasets import Dataset\n",
    "        dataset = Dataset.from_pandas(df)\n",
    "        \n",
    "        # Sample if needed\n",
    "        if CONFIG['sample_size'] and CONFIG['sample_size'] < len(dataset):\n",
    "            dataset = dataset.shuffle(seed=CONFIG['seed']).select(range(CONFIG['sample_size']))\n",
    "            \n",
    "        logger.info(\"Dataset loaded successfully using pandas alternative method\")\n",
    "        \n",
    "    except Exception as e2:\n",
    "        logger.error(f\"Alternative loading method also failed: {str(e2)}\")\n",
    "        raise RuntimeError(f\"Both loading methods failed. Original error: {str(e)}, Alternative error: {str(e2)}\")\n",
    "        \n",
    "if dataset is None:\n",
    "    logger.critical(\"FATAL: Dataset could not be loaded by any implemented method.\")\n",
    "    raise RuntimeError(\"Dataset loading failed. Please check the dataset name and parameters.\")\n",
    "\n",
    "\n",
    "logger.info(f\"Dataset loaded. Original columns: {dataset.column_names}\")\n",
    "logger.info(f\"Number of samples in loaded dataset: {len(dataset)}\")\n",
    "\n",
    "if 'text' in dataset.column_names:\n",
    "    dataset = dataset.rename_column('text', 'article')\n",
    "    logger.info(\"Renamed dataset column: 'text' -> 'article'\")\n",
    "    CONFIG['text_col'] = 'article'\n",
    "    logger.info(f\"Updated CONFIG['text_col'] to '{CONFIG['text_col']}'\")\n",
    "else:\n",
    "    logger.warning(f\"'text' column not found in dataset columns: {dataset.column_names}. Skipping rename. Current text column in CONFIG: {CONFIG['text_col']}\")\n",
    "\n",
    "logger.info(\"Creating dataset splits...\")\n",
    "try:\n",
    "    total_size = len(dataset)\n",
    "    logger.info(f\"Total dataset size for splitting: {total_size}\")\n",
    "\n",
    "    train_size = int(total_size * CONFIG[\"split_train_frac\"])\n",
    "    eval_size = total_size - train_size\n",
    "\n",
    "    if train_size <= 0 or eval_size <= 0:\n",
    "        logger.error(f\"Calculated train_size ({train_size}) or eval_size ({eval_size}) is non-positive. Aborting split.\")\n",
    "        raise ValueError(\"Train or evaluation set size is not positive. Check dataset size and split_train_frac.\")\n",
    "\n",
    "    dataset_shuffled = dataset.shuffle(seed=CONFIG[\"seed\"]) # Shuffle before selecting\n",
    "    train_dataset = dataset_shuffled.select(range(train_size))\n",
    "    eval_dataset = dataset_shuffled.select(range(train_size, total_size))\n",
    "\n",
    "    logger.info(f\"Created splits - Train: {len(train_dataset)}, Eval: {len(eval_dataset)}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error creating dataset splits: {e}\")\n",
    "    raise\n",
    "\n",
    "logger.info(\"Processing datasets (tokenization, etc.)...\")\n",
    "try:\n",
    "    tokenized_datasets = {\n",
    "        'train': train_dataset.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            remove_columns=train_dataset.column_names\n",
    "        ),\n",
    "        'eval': eval_dataset.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            remove_columns=eval_dataset.column_names\n",
    "        )\n",
    "    }\n",
    "    logger.info(\"Dataset processing complete.\")\n",
    "    logger.info(f\"Final tokenized dataset sizes - Training samples: {len(tokenized_datasets['train'])}, Evaluation samples: {len(tokenized_datasets['eval'])}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error processing datasets: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56a9f58",
   "metadata": {},
   "source": [
    "## 10. Define Training Arguments\n",
    "\n",
    "Configure the training process using Seq2SeqTrainingArguments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85dd1a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=CONFIG[\"output_dir\"],\n",
    "    num_train_epochs=CONFIG[\"num_train_epochs\"],\n",
    "    per_device_train_batch_size=CONFIG[\"per_device_train_batch_size\"],\n",
    "    per_device_eval_batch_size=CONFIG[\"per_device_eval_batch_size\"],\n",
    "    gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n",
    "    learning_rate=CONFIG[\"learning_rate\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"],\n",
    "    warmup_steps=CONFIG[\"warmup_steps\"],\n",
    "    \n",
    "    # Optimization - modified for MPS\n",
    "    fp16=False,  # Disable FP16 for MPS\n",
    "    fp16_full_eval=False,\n",
    "    bf16=False,\n",
    "    optim=\"adamw_torch\",  # Use adamw_torch optimizer\n",
    "    \n",
    "    # Disable features that might cause issues on MPS\n",
    "    gradient_checkpointing=False,  # Disable gradient checkpointing on MPS\n",
    "    group_by_length=False,  # Disable length batching\n",
    "    dataloader_pin_memory=False,  # Disable pin_memory on MPS\n",
    "    \n",
    "    # Logging & Evaluation\n",
    "    logging_dir=f\"{CONFIG['output_dir']}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=CONFIG[\"logging_steps\"],\n",
    "    eval_strategy=CONFIG[\"evaluation_strategy\"],\n",
    "    eval_steps=CONFIG[\"eval_steps\"],\n",
    "    \n",
    "    # Saving\n",
    "    save_strategy=CONFIG[\"save_strategy\"],\n",
    "    save_steps=CONFIG[\"save_steps\"],\n",
    "    save_total_limit=CONFIG[\"save_total_limit\"],\n",
    "    \n",
    "    # Model Loading\n",
    "    load_best_model_at_end=CONFIG[\"load_best_model_at_end\"],\n",
    "    metric_for_best_model=CONFIG[\"metric_for_best_model\"],\n",
    "    greater_is_better=CONFIG[\"greater_is_better\"],\n",
    "    \n",
    "    # Generation\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=CONFIG[\"max_target_tokens\"],\n",
    "    generation_num_beams=CONFIG[\"gen_num_beams\"],\n",
    "    \n",
    "    # Other\n",
    "    report_to=CONFIG[\"report_to\"],\n",
    "    seed=CONFIG[\"seed\"],\n",
    "    remove_unused_columns=False,  # Keep all columns\n",
    "    overwrite_output_dir=CONFIG[\"overwrite_output_dir\"],\n",
    "    use_legacy_prediction_loop=True,  # Use legacy prediction loop to avoid past_key_values warning\n",
    ")\n",
    "\n",
    "logger.info(f\"Training arguments configured for MPS\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78898be",
   "metadata": {},
   "source": [
    "## 11. Define Metrics Computation\n",
    "\n",
    "Function to compute ROUGE and BLEU scores for evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4970687",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "import numpy as np\n",
    "import nltk\n",
    "from typing import Dict, List\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def get_metrics():\n",
    "    \"\"\"Load and cache evaluation metrics.\"\"\"\n",
    "    return {\n",
    "        \"rouge\": evaluate.load(\"rouge\"),\n",
    "        \"bleu\": evaluate.load(\"bleu\"),\n",
    "        \"bertscore\": evaluate.load(\"bertscore\")\n",
    "    }\n",
    "\n",
    "def process_texts(texts: List[str]) -> List[str]:\n",
    "    \"\"\"Clean and process texts for evaluation.\"\"\"\n",
    "    return [\"\\n\".join(nltk.sent_tokenize(text.strip())) for text in texts]\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    try:\n",
    "        predictions, labels = eval_pred\n",
    "        # Replace -100 (padded labels) with pad_token_id\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        \n",
    "        # Decode predictions and labels\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        \n",
    "        # Clean empty or invalid outputs\n",
    "        decoded_preds = [pred.strip() if pred.strip() else \"No summary generated\" for pred in decoded_preds]\n",
    "        decoded_labels = [label.strip() if label.strip() else \"No summary provided\" for label in decoded_labels]\n",
    "        \n",
    "        # Compute ROUGE scores\n",
    "        rouge = evaluate.load(\"rouge\")\n",
    "        rouge_results = rouge.compute(\n",
    "            predictions=decoded_preds,\n",
    "            references=decoded_labels,\n",
    "            use_stemmer=True\n",
    "        )\n",
    "        \n",
    "        # Compute BLEU score\n",
    "        bleu = evaluate.load(\"bleu\")\n",
    "        bleu_results = bleu.compute(\n",
    "            predictions=decoded_preds,\n",
    "            references=[[label] for label in decoded_labels]\n",
    "        )\n",
    "        \n",
    "        # Compute average generated length\n",
    "        gen_len = np.mean([len(tokenizer(pred, add_special_tokens=False)[\"input_ids\"]) for pred in decoded_preds])\n",
    "        \n",
    "        # Combine metrics\n",
    "        metrics = {\n",
    "            \"rouge1\": rouge_results[\"rouge1\"],\n",
    "            \"rouge2\": rouge_results[\"rouge2\"],\n",
    "            \"rougeL\": rouge_results[\"rougeL\"],\n",
    "            \"bleu\": bleu_results[\"bleu\"],\n",
    "            \"gen_len\": gen_len\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error computing metrics: {str(e)}\")\n",
    "        return {\n",
    "            \"rouge1\": 0.0,\n",
    "            \"rouge2\": 0.0,\n",
    "            \"rougeL\": 0.0,\n",
    "            \"bleu\": 0.0,\n",
    "            \"gen_len\": 0.0,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "logger.info(\"Enhanced metrics computation function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b54a510",
   "metadata": {},
   "source": [
    "## 11. Initialize Trainer\n",
    "\n",
    "Set up the `Seq2SeqTrainer` with the model, arguments, datasets, tokenizer, and metrics function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "601cfed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bh/db9plr7n08g67qlmw3z86x340000gn/T/ipykernel_96356/1412348843.py:52: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "from transformers import EarlyStoppingCallback,TrainerCallback\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Clear unused memory before training\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    print_memory_usage()\n",
    "\n",
    "class MemoryTrackingCallback(TrainerCallback):\n",
    "    \"\"\"Callback to track memory usage during training\"\"\"\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % 100 == 0:  # Monitor every 100 steps\n",
    "            print_memory_usage()\n",
    "\n",
    "def validate_training_args(args, model):\n",
    "    \"\"\"Validate training arguments for potential issues\"\"\"\n",
    "    if args.per_device_train_batch_size * args.gradient_accumulation_steps > 32:\n",
    "        logger.warning(\"Total batch size might be too large for available memory\")\n",
    "    \n",
    "    if args.fp16 and not torch.cuda.is_available() and not torch.backends.mps.is_available():\n",
    "        raise ValueError(\"FP16 requires CUDA or MPS\")\n",
    "    \n",
    "    if args.fp16 and torch.backends.mps.is_available():\n",
    "        logger.warning(\"FP16 is not fully supported on MPS. Turning it off.\")\n",
    "        args.fp16 = False\n",
    "        args.fp16_full_eval = False\n",
    "\n",
    "# Clear memory before initialization\n",
    "clear_memory()\n",
    "\n",
    "# Initialize data collator with error handling\n",
    "try:\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer,\n",
    "        model=model,\n",
    "        label_pad_token_id=tokenizer.pad_token_id,\n",
    "        pad_to_multiple_of=None  # Changed for MPS compatibility\n",
    "    )\n",
    "    logger.info(\"Data collator initialized successfully\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to initialize data collator: {e}\")\n",
    "    raise\n",
    "\n",
    "# Validate training arguments\n",
    "validate_training_args(training_args, model)\n",
    "\n",
    "# Initialize trainer with enhanced monitoring\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"eval\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(early_stopping_patience=3),\n",
    "        MemoryTrackingCallback()\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger.info(\"Trainer initialized with enhanced monitoring\")\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d90adf",
   "metadata": {},
   "source": [
    "## 12. Train the Model\n",
    "\n",
    "Start the fine-tuning process. This will take some time depending on the dataset size and hardware. 🥳\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7fc3bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_checkpoint(checkpoint_dir):\n",
    "    \"\"\"Find most recent checkpoint in the directory\"\"\"\n",
    "    checkpoints = [d for d in os.listdir(checkpoint_dir) \n",
    "                  if d.startswith(\"checkpoint-\")]\n",
    "    if not checkpoints:\n",
    "        return None\n",
    "    return os.path.join(checkpoint_dir, \n",
    "                       sorted(checkpoints, key=lambda x: int(x.split(\"-\")[1]))[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce148d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='56' max='1125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  56/1125 41:10 < 13:35:13, 0.02 it/s, Epoch 0.24/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Gen Len</th>\n",
       "      <th>Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>23.334300</td>\n",
       "      <td>25.460405</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>out of range integral type conversion attempted</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gourabsarkar/Developer/college_project/pdf_summarization_model_fine_tuning/.venv/lib/python3.10/site-packages/transformers/trainer_pt_utils.py:477: FutureWarning: DistributedTensorGatherer is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Error computing metrics: out of range integral type conversion attempted\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- Batch size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mper_device_train_batch_size\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Execute training\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_checkpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Log final metrics\u001b[39;00m\n\u001b[1;32m     23\u001b[0m metrics \u001b[38;5;241m=\u001b[39m train_result\u001b[38;5;241m.\u001b[39mmetrics\n",
      "File \u001b[0;32m~/Developer/college_project/pdf_summarization_model_fine_tuning/.venv/lib/python3.10/site-packages/transformers/trainer.py:2240\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2241\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/college_project/pdf_summarization_model_fine_tuning/.venv/lib/python3.10/site-packages/transformers/trainer.py:2555\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2548\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2549\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2550\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2551\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2552\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2553\u001b[0m )\n\u001b[1;32m   2554\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2555\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2558\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2559\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2560\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2561\u001b[0m ):\n\u001b[1;32m   2562\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2563\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/Developer/college_project/pdf_summarization_model_fine_tuning/.venv/lib/python3.10/site-packages/transformers/trainer.py:3745\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3742\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3744\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3745\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3747\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3749\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3750\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3751\u001b[0m ):\n",
      "File \u001b[0;32m~/Developer/college_project/pdf_summarization_model_fine_tuning/.venv/lib/python3.10/site-packages/transformers/trainer.py:3810\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3808\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3809\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3810\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3811\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3812\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Developer/college_project/pdf_summarization_model_fine_tuning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/college_project/pdf_summarization_model_fine_tuning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Developer/college_project/pdf_summarization_model_fine_tuning/.venv/lib/python3.10/site-packages/peft/peft_model.py:2031\u001b[0m, in \u001b[0;36mPeftModelForSeq2SeqLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, decoder_input_ids, decoder_attention_mask, decoder_inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   2029\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   2030\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 2031\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2032\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2033\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2034\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2035\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2036\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2037\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2038\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2039\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2040\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2041\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2042\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2043\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2045\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   2046\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoder_attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2047\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/Developer/college_project/pdf_summarization_model_fine_tuning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/college_project/pdf_summarization_model_fine_tuning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Developer/college_project/pdf_summarization_model_fine_tuning/.venv/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:193\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/college_project/pdf_summarization_model_fine_tuning/.venv/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1774\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1771\u001b[0m \u001b[38;5;66;03m# Encode if needed (training, first prediction pass)\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;66;03m# Convert encoder inputs in embeddings if needed\u001b[39;00m\n\u001b[0;32m-> 1774\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1775\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1776\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1777\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1778\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1779\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1780\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, BaseModelOutput):\n\u001b[1;32m   1784\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[1;32m   1785\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   1786\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1787\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1788\u001b[0m     )\n",
      "File \u001b[0;32m~/Developer/college_project/pdf_summarization_model_fine_tuning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/college_project/pdf_summarization_model_fine_tuning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Developer/college_project/pdf_summarization_model_fine_tuning/.venv/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1124\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1107\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1108\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39mforward,\n\u001b[1;32m   1109\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1121\u001b[0m         cache_position,\n\u001b[1;32m   1122\u001b[0m     )\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1124\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Developer/college_project/pdf_summarization_model_fine_tuning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/college_project/pdf_summarization_model_fine_tuning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Developer/college_project/pdf_summarization_model_fine_tuning/.venv/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:729\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    726\u001b[0m     attention_outputs \u001b[38;5;241m=\u001b[39m attention_outputs \u001b[38;5;241m+\u001b[39m cross_attention_outputs[\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m    728\u001b[0m \u001b[38;5;66;03m# Apply Feed Forward layer\u001b[39;00m\n\u001b[0;32m--> 729\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;66;03m# clamp inf values to enable fp16 training\u001b[39;00m\n\u001b[1;32m    732\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hidden_states\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n",
      "File \u001b[0;32m~/Developer/college_project/pdf_summarization_model_fine_tuning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/college_project/pdf_summarization_model_fine_tuning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Developer/college_project/pdf_summarization_model_fine_tuning/.venv/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:343\u001b[0m, in \u001b[0;36mT5LayerFF.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[1;32m    342\u001b[0m     forwarded_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 343\u001b[0m     forwarded_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDenseReluDense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforwarded_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    344\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(forwarded_states)\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/Developer/college_project/pdf_summarization_model_fine_tuning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/college_project/pdf_summarization_model_fine_tuning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Developer/college_project/pdf_summarization_model_fine_tuning/.venv/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:314\u001b[0m, in \u001b[0;36mT5DenseGatedActDense.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    312\u001b[0m hidden_linear \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwi_1(hidden_states)\n\u001b[1;32m    313\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_gelu \u001b[38;5;241m*\u001b[39m hidden_linear\n\u001b[0;32m--> 314\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# To make 8bit quantization work for google/flan-t5-xxl, self.wo is kept in float32.\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;66;03m# See https://github.com/huggingface/transformers/issues/20287\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;66;03m# we also make sure the weights are not in `int8` in case users will force `_keep_in_fp32_modules` to be `None``\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwo\u001b[38;5;241m.\u001b[39mweight, torch\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m hidden_states\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwo\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwo\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mint8\n\u001b[1;32m    323\u001b[0m ):\n",
      "File \u001b[0;32m~/Developer/college_project/pdf_summarization_model_fine_tuning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/college_project/pdf_summarization_model_fine_tuning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Developer/college_project/pdf_summarization_model_fine_tuning/.venv/lib/python3.10/site-packages/torch/nn/modules/dropout.py:70\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/college_project/pdf_summarization_model_fine_tuning/.venv/lib/python3.10/site-packages/torch/nn/functional.py:1425\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1423\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1424\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m-> 1425\u001b[0m     _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1426\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logger.info(\"Starting training...\")\n",
    "try:\n",
    "    # Setup checkpoint directory\n",
    "    checkpoint_dir = os.path.join(CONFIG[\"output_dir\"], \"checkpoints\")\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # Check for existing checkpoint\n",
    "    resume_checkpoint = get_latest_checkpoint(checkpoint_dir)\n",
    "    if resume_checkpoint:\n",
    "        logger.info(f\"Resuming from checkpoint: {resume_checkpoint}\")\n",
    "    \n",
    "    # Print training info\n",
    "    logger.info(\"Training Configuration:\")\n",
    "    logger.info(f\"- Number of training examples: {len(trainer.train_dataset)}\")\n",
    "    logger.info(f\"- Number of validation examples: {len(trainer.eval_dataset)}\")\n",
    "    logger.info(f\"- Training Epochs: {CONFIG['num_train_epochs']}\")\n",
    "    logger.info(f\"- Batch size: {CONFIG['per_device_train_batch_size']}\")\n",
    "    \n",
    "    # Execute training\n",
    "    train_result = trainer.train(resume_from_checkpoint=resume_checkpoint)\n",
    "    \n",
    "    # Log final metrics\n",
    "    metrics = train_result.metrics\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "    \n",
    "    logger.info(\"Training completed successfully!\")\n",
    "    logger.info(f\"Final Training Loss: {metrics.get('train_loss', 'N/A')}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Training failed: {e}\")\n",
    "    if wandb.run:\n",
    "        wandb.log({\"training_error\": str(e)})\n",
    "        wandb.run.finish(exit_code=1)\n",
    "    raise e\n",
    "\n",
    "finally:\n",
    "    print_memory_usage()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0458a99f",
   "metadata": {},
   "source": [
    "## 13. Evaluate the Model\n",
    "\n",
    "Evaluate the fine-tuned model on the evaluation set to get final performance metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e88d6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gourabsarkar/Developer/college_project/pdf_summarization_model_fine_tuning/.venv/lib/python3.10/site-packages/transformers/trainer_pt_utils.py:477: FutureWarning: DistributedTensorGatherer is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 22:42]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error computing metrics: out of range integral type conversion attempted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =                                          1.7733\n",
      "  eval_bleu               =                                             0.0\n",
      "  eval_error              = out of range integral type conversion attempted\n",
      "  eval_gen_len            =                                               0\n",
      "  eval_loss               =                                          3.2697\n",
      "  eval_rouge1             =                                             0.0\n",
      "  eval_rouge2             =                                             0.0\n",
      "  eval_rougeL             =                                             0.0\n",
      "  eval_rougeLsum          =                                             0.0\n",
      "  eval_runtime            =                                      0:22:53.13\n",
      "  eval_samples_per_second =                                           0.073\n",
      "  eval_steps_per_second   =                                           0.073\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Evaluating model...\")\n",
    "eval_metrics = trainer.evaluate()\n",
    "\n",
    "logger.info(\"Evaluation metrics:\")\n",
    "for key, value in eval_metrics.items():\n",
    "    logger.info(f\"{key}: {value}\")\n",
    "\n",
    "# Log evaluation metrics\n",
    "trainer.log_metrics(\"eval\", eval_metrics)\n",
    "trainer.save_metrics(\"eval\", eval_metrics) # Saves to all_results.json\n",
    "\n",
    "# Prepare the training_report.json\n",
    "training_report = {\n",
    "    \"model_name\": CONFIG[\"model_name\"],\n",
    "    \"dataset_name\": CONFIG[\"dataset_name\"],\n",
    "    \"lora_adapter_name\": CONFIG[\"lora_adapter_name\"],\n",
    "    \"output_directory\": CONFIG[\"output_dir\"],\n",
    "    \"training_arguments\": {k: str(v) if isinstance(v, (torch.device, BitsAndBytesConfig)) else v for k, v in training_args.to_dict().items()}, # Convert non-serializable items\n",
    "    \"train_metrics\": trainer.state.log_history[:-1], # All logged steps except final eval\n",
    "    \"eval_metrics\": eval_metrics,\n",
    "    \"final_training_loss\": trainer.state.log_history[-2].get('loss') if len(trainer.state.log_history) > 1 and 'loss' in trainer.state.log_history[-2] else trainer.state.log_history[-1].get('train_loss', 'N/A')\n",
    "}\n",
    "\n",
    "\n",
    "# Add ROUGE and BLEU from eval_metrics to the top level for easier access\n",
    "for metric_key in [\"eval_rouge1\", \"eval_rouge2\", \"eval_rougeL\", \"eval_bleu\"]:\n",
    "    if metric_key in eval_metrics:\n",
    "        training_report[metric_key.replace(\"eval_\", \"\")] = eval_metrics[metric_key]\n",
    "\n",
    "\n",
    "# Save training_report.json locally\n",
    "report_path = os.path.join(CONFIG[\"output_dir\"], CONFIG[\"training_report_filename\"])\n",
    "with open(report_path, \"w\") as f:\n",
    "    json.dump(training_report, f, indent=4)\n",
    "logger.info(f\"Training report saved to {report_path}\")\n",
    "\n",
    "if wandb.run:\n",
    "    wandb.log(eval_metrics) # Log final eval metrics\n",
    "    wandb.save(report_path) # Save report to W&B artifacts\n",
    "    logger.info(\"Evaluation metrics and report logged to W&B.\")\n",
    "\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305442e8",
   "metadata": {},
   "source": [
    "## 14. Save Model and LoRA Adapter\n",
    "\n",
    "Save the fine-tuned LoRA adapter and the full model if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7083fe5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Google Drive not mounted or GDrive output path does not exist. Outputs saved locally.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁</td></tr><tr><td>eval/bleu</td><td>▁▁▁▁▁</td></tr><tr><td>eval/gen_len</td><td>▁▁▁▁▁</td></tr><tr><td>eval/loss</td><td>█▅▃▁▁</td></tr><tr><td>eval/rouge1</td><td>▁▁▁▁▁</td></tr><tr><td>eval/rouge2</td><td>▁▁▁▁▁</td></tr><tr><td>eval/rougeL</td><td>▁▁▁▁▁</td></tr><tr><td>eval/rougeLsum</td><td>▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁▆█▄▃</td></tr><tr><td>eval/samples_per_second</td><td>█▃▁▅▅</td></tr><tr><td>eval/steps_per_second</td><td>█▃▁▅▅</td></tr><tr><td>eval_bleu</td><td>▁</td></tr><tr><td>eval_gen_len</td><td>▁</td></tr><tr><td>eval_loss</td><td>▁</td></tr><tr><td>eval_rouge1</td><td>▁</td></tr><tr><td>eval_rouge2</td><td>▁</td></tr><tr><td>eval_rougeL</td><td>▁</td></tr><tr><td>eval_rougeLsum</td><td>▁</td></tr><tr><td>eval_runtime</td><td>▁</td></tr><tr><td>eval_samples_per_second</td><td>▁</td></tr><tr><td>eval_steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▆▆▆▇▇▇█████</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▆▆▆▇▇▇██████</td></tr><tr><td>train/grad_norm</td><td>▃▁█▆█▃▃▂▅▂▂▄▃▄▄▁▁▃▄▂</td></tr><tr><td>train/learning_rate</td><td>▁███▇▇▆▆▆▅▅▄▄▃▃▃▂▂▁▁</td></tr><tr><td>train/loss</td><td>▆▆█▆▆▅▄▄▄▄▂▃▃▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1.77333</td></tr><tr><td>eval/bleu</td><td>0</td></tr><tr><td>eval/error</td><td>out of range integra...</td></tr><tr><td>eval/gen_len</td><td>0</td></tr><tr><td>eval/loss</td><td>3.26971</td></tr><tr><td>eval/rouge1</td><td>0</td></tr><tr><td>eval/rouge2</td><td>0</td></tr><tr><td>eval/rougeL</td><td>0</td></tr><tr><td>eval/rougeLsum</td><td>0</td></tr><tr><td>eval/runtime</td><td>1373.1336</td></tr><tr><td>eval/samples_per_second</td><td>0.073</td></tr><tr><td>eval/steps_per_second</td><td>0.073</td></tr><tr><td>eval_bleu</td><td>0</td></tr><tr><td>eval_error</td><td>out of range integra...</td></tr><tr><td>eval_gen_len</td><td>0</td></tr><tr><td>eval_loss</td><td>3.26971</td></tr><tr><td>eval_rouge1</td><td>0</td></tr><tr><td>eval_rouge2</td><td>0</td></tr><tr><td>eval_rougeL</td><td>0</td></tr><tr><td>eval_rougeLsum</td><td>0</td></tr><tr><td>eval_runtime</td><td>1373.1336</td></tr><tr><td>eval_samples_per_second</td><td>0.073</td></tr><tr><td>eval_steps_per_second</td><td>0.073</td></tr><tr><td>total_flos</td><td>1030668195594240.0</td></tr><tr><td>train/epoch</td><td>1.77333</td></tr><tr><td>train/global_step</td><td>200</td></tr><tr><td>train/grad_norm</td><td>1.45518</td></tr><tr><td>train/learning_rate</td><td>4e-05</td></tr><tr><td>train/loss</td><td>4.1551</td></tr><tr><td>train_loss</td><td>9.26897</td></tr><tr><td>train_runtime</td><td>7021.373</td></tr><tr><td>train_samples_per_second</td><td>0.385</td></tr><tr><td>train_steps_per_second</td><td>0.048</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">run_20250520_144932</strong> at: <a href='https://wandb.ai/goheygourab-self/flan-t5-billsum-lora/runs/zn1cihl4' target=\"_blank\">https://wandb.ai/goheygourab-self/flan-t5-billsum-lora/runs/zn1cihl4</a><br> View project at: <a href='https://wandb.ai/goheygourab-self/flan-t5-billsum-lora' target=\"_blank\">https://wandb.ai/goheygourab-self/flan-t5-billsum-lora</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250520_144932-zn1cihl4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save the LoRA adapter\n",
    "lora_adapter_path = os.path.join(CONFIG[\"output_dir\"], CONFIG[\"lora_adapter_name\"])\n",
    "model.save_pretrained(lora_adapter_path) # Saves only the LoRA adapter\n",
    "tokenizer.save_pretrained(lora_adapter_path) # Save tokenizer with adapter\n",
    "logger.info(f\"LoRA adapter and tokenizer saved to {lora_adapter_path}\")\n",
    "\n",
    "# To save the full model (optional, requires more space)\n",
    "# merged_model_path = os.path.join(CONFIG[\"output_dir\"], \"merged_model_flan_t5_base_billsum\")\n",
    "# try:\n",
    "#     # Merge LoRA weights with the base model\n",
    "#     merged_model = model.merge_and_unload()\n",
    "#     merged_model.save_pretrained(merged_model_path)\n",
    "#     tokenizer.save_pretrained(merged_model_path)\n",
    "#     logger.info(f\"Full merged model saved to {merged_model_path}\")\n",
    "# except Exception as e:\n",
    "#     logger.error(f\"Could not merge and save full model: {e}. This might happen if the base model is not fully on CPU or due to memory constraints.\")\n",
    "#     logger.info(\"Only the LoRA adapter was saved.\")\n",
    "\n",
    "\n",
    "# If Google Drive is mounted, copy outputs there\n",
    "if CONFIG[\"mount_drive\"] and os.path.exists(CONFIG[\"gdrive_output_dir\"]):\n",
    "    logger.info(f\"Copying outputs to Google Drive: {CONFIG['gdrive_output_dir']}\")\n",
    "    # Copy LoRA adapter\n",
    "    gdrive_lora_path = os.path.join(CONFIG[\"gdrive_output_dir\"], CONFIG[\"lora_adapter_name\"])\n",
    "    if os.path.exists(gdrive_lora_path):\n",
    "        logger.info(f\"Removing existing LoRA adapter from GDrive: {gdrive_lora_path}\")\n",
    "        os.system(f\"rm -rf '{gdrive_lora_path}'\") # Use os.system for `rm -rf`\n",
    "    os.system(f\"cp -r '{lora_adapter_path}' '{CONFIG['gdrive_output_dir']}/'\")\n",
    "    logger.info(f\"LoRA adapter copied to {gdrive_lora_path}\")\n",
    "\n",
    "    # Copy training report\n",
    "    gdrive_report_path = os.path.join(CONFIG[\"gdrive_output_dir\"], CONFIG[\"training_report_filename\"])\n",
    "    os.system(f\"cp '{report_path}' '{gdrive_report_path}'\")\n",
    "    logger.info(f\"Training report copied to {gdrive_report_path}\")\n",
    "\n",
    "    # Copy all_results.json (contains eval metrics)\n",
    "    all_results_path = os.path.join(CONFIG[\"output_dir\"], \"all_results.json\")\n",
    "    if os.path.exists(all_results_path):\n",
    "        gdrive_all_results_path = os.path.join(CONFIG[\"gdrive_output_dir\"], \"all_results.json\")\n",
    "        os.system(f\"cp '{all_results_path}' '{gdrive_all_results_path}'\")\n",
    "        logger.info(f\"all_results.json copied to {gdrive_all_results_path}\")\n",
    "\n",
    "    # If merged model was saved and exists, copy it too\n",
    "    # if 'merged_model' in locals() and os.path.exists(merged_model_path):\n",
    "    #     gdrive_merged_model_path = os.path.join(CONFIG[\"gdrive_output_dir\"], \"merged_model_flan_t5_base_billsum\")\n",
    "    #     if os.path.exists(gdrive_merged_model_path):\n",
    "    #         logger.info(f\"Removing existing merged model from GDrive: {gdrive_merged_model_path}\")\n",
    "    #         os.system(f\"rm -rf '{gdrive_merged_model_path}'\")\n",
    "    #     os.system(f\"cp -r '{merged_model_path}' '{CONFIG['gdrive_output_dir']}/'\")\n",
    "    #     logger.info(f\"Full merged model copied to {gdrive_merged_model_path}\")\n",
    "else:\n",
    "    logger.warning(\"Google Drive not mounted or GDrive output path does not exist. Outputs saved locally.\")\n",
    "\n",
    "if wandb.run:\n",
    "    # Log LoRA adapter as artifact if desired\n",
    "    # lora_artifact = wandb.Artifact(CONFIG[\"lora_adapter_name\"], type=\"model\")\n",
    "    # lora_artifact.add_dir(lora_adapter_path)\n",
    "    # wandb.log_artifact(lora_artifact)\n",
    "    # logger.info(f\"LoRA adapter logged as W&B artifact: {CONFIG['lora_adapter_name']}\")\n",
    "    wandb.finish()\n",
    "\n",
    "logger.info(\"Script finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5720cd",
   "metadata": {},
   "source": [
    "## Test the model with a sample input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed36bdde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS (Metal Performance Shaders) is available! Using MPS for acceleration.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gourabsarkar/Developer/college_project/pdf_summarization_model_fine_tuning/.venv/lib/python3.10/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/Users/gourabsarkar/Developer/college_project/pdf_summarization_model_fine_tuning/.venv/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Summary:\n",
      "Write a bill.\n"
     ]
    }
   ],
   "source": [
    "# Test document - Cybersecurity and Privacy Protection Act\n",
    "test_document = \"\"\"\n",
    "Summarize the following legal bill: \n",
    "CYBERSECURITY AND PRIVACY PROTECTION ACT OF 2025\n",
    "\n",
    "SECTION 1. SHORT TITLE AND PURPOSE\n",
    "\n",
    "    (a) This Act may be cited as the 'Cybersecurity and Privacy Protection Act of 2025'.\n",
    "    (b) The purpose of this Act is to enhance cybersecurity measures and protect individual privacy in the digital age.\n",
    "\n",
    "SECTION 2. DEFINITIONS\n",
    "\n",
    "In this Act:\n",
    "    (1) 'Personal Data' means any information relating to an identified or identifiable natural person.\n",
    "    (2) 'Data Controller' means any entity that determines the purposes and means of processing personal data.\n",
    "    (3) 'Critical Infrastructure' means systems and assets vital to national security.\n",
    "\n",
    "SECTION 3. CYBERSECURITY REQUIREMENTS\n",
    "\n",
    "    (a) MANDATORY SECURITY MEASURES.—\n",
    "        (1) All Data Controllers shall implement:\n",
    "            (A) End-to-end encryption for data transmission\n",
    "            (B) Multi-factor authentication for system access\n",
    "            (C) Regular security audits and vulnerability assessments\n",
    "\n",
    "    (b) INCIDENT REPORTING.—\n",
    "        (1) Data Controllers shall report any security breach within 48 hours.\n",
    "        (2) Penalties for non-compliance shall be up to $500,000 per incident.\n",
    "\n",
    "SECTION 4. PRIVACY PROTECTIONS\n",
    "\n",
    "    (a) CONSENT REQUIREMENTS.—\n",
    "        (1) Explicit consent required for data collection\n",
    "        (2) Right to access and delete personal data\n",
    "        (3) Annual privacy impact assessments\n",
    "\n",
    "    (b) CHILDREN'S PRIVACY.—\n",
    "        (1) Enhanced protections for users under 13\n",
    "        (2) Parental consent requirements\n",
    "\n",
    "SECTION 5. ENFORCEMENT\n",
    "\n",
    "    (a) The Federal Trade Commission shall enforce this Act.\n",
    "    (b) State Attorneys General may bring civil actions.\n",
    "\n",
    "SECTION 6. AUTHORIZATION OF APPROPRIATIONS\n",
    "\n",
    "    There is authorized to be appropriated $275,000,000 for fiscal year 2026 to carry out this Act.\n",
    "\"\"\"\n",
    "\n",
    "# Test the model with the adapter\n",
    "try:\n",
    "    # Load the trained model with LoRA adapter\n",
    "    logger.info(f\"Loading model with LoRA adapter from {lora_adapter_path}...\")\n",
    "    \n",
    "    # Get the device\n",
    "    current_device = get_device()\n",
    "    \n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        lora_adapter_path,\n",
    "        torch_dtype=torch.float32  # Use float32 for MPS compatibility\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.load_adapter(lora_adapter_path, CONFIG[\"lora_adapter_name\"])\n",
    "    model.set_adapter(CONFIG[\"lora_adapter_name\"])\n",
    "    model.eval()\n",
    "    model.to(current_device)  # Move the model to the appropriate device\n",
    "    \n",
    "    # Process the test document\n",
    "    logger.info(\"Tokenizing test document...\")\n",
    "    prompt = f\"{CONFIG['prompt_prefix']}{test_document}\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(current_device)\n",
    "    \n",
    "    # Generate summary\n",
    "    logger.info(\"Generating summary...\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs, \n",
    "            max_length=512,\n",
    "            num_beams=CONFIG[\"gen_num_beams\"],\n",
    "            length_penalty=CONFIG[\"gen_length_penalty\"],\n",
    "            early_stopping=CONFIG[\"gen_early_stopping\"]\n",
    "        )\n",
    "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(\"\\nGenerated Summary:\")\n",
    "    print(summary)\n",
    "    \n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during inference: {e}\")\n",
    "    print(f\"An error occurred during inference: {str(e)}\")\n",
    "\n",
    "print_memory_usage()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
