{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c206dd2a",
      "metadata": {
        "id": "c206dd2a"
      },
      "source": [
        "# Fine-Tune Flan-T5-base on BillSum\n",
        "\n",
        "Author: Gourab S.(heygourab),\n",
        "github: https://github.com/heygourab\n",
        "\n",
        "This notebook fine-tunes Flan-T5-base with LoRA on the BillSum dataset (~800 samples) for legal document summarization. Outputs a LoRA adapter (`lora_billsum`) and a `training_report.json` via RogerReportCallback.\n",
        "\n",
        "The model is trained for 3 epochs with a batch size of 16 and a learning rate of 2e-4.\n",
        "The model is saved in the `lora_billsum` directory.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bc5d042",
      "metadata": {
        "id": "2bc5d042"
      },
      "source": [
        "## For macOS M1/Apple Silicon\n",
        "\n",
        "Drop this cell into your Jupyter Notebook (assuming venv is active and you're not using Colab):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "931c111e",
      "metadata": {
        "id": "931c111e",
        "outputId": "d528136b-2aa2-420e-9244-5758823edf55"
      },
      "outputs": [],
      "source": [
        "%pip install -r ../requirements/mac.txt -q"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dda7c950",
      "metadata": {
        "id": "dda7c950"
      },
      "source": [
        "## Import necessary libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f0ec670",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "9f0ec670",
        "outputId": "e2a7f4b4-e5d5-4657-ae50-ab0ceeddcd63"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import logging\n",
        "import sys\n",
        "import tqdm\n",
        "import traceback\n",
        "from datetime import datetime\n",
        "from typing import Dict\n",
        "import torch\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments,\n",
        "    DataCollatorForSeq2Seq, TrainerCallback,  logging\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
        "from accelerate import Accelerator\n",
        "import evaluate\n",
        "from omegaconf import OmegaConf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import psutil\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd79bd19",
      "metadata": {
        "id": "cd79bd19"
      },
      "outputs": [],
      "source": [
        "# Set the logging level for the transformers library\n",
        "logging.set_verbosity_info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "358b8833",
      "metadata": {
        "id": "358b8833"
      },
      "source": [
        "### Setup logger\n",
        "\n",
        "Sets up logging to track training progress and debugging information. The logger is configured to:\n",
        "\n",
        "- Display timestamp, log level, and message\n",
        "- Output logs to standard output (stdout)\n",
        "- Use INFO level logging\n",
        "- Create a logger instance named after the current module\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "646c33ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "646c33ba",
        "outputId": "6d8d2753-88f9-478d-a47e-b964b21234cf"
      },
      "outputs": [],
      "source": [
        "import sys \n",
        "import os\n",
        "import logging\n",
        "from datetime import datetime\n",
        "def setup_logger(name=\"train_logger\", level=logging.INFO, log_file=None):\n",
        "    \"\"\"\n",
        "    Set up a logger with both console and file output.\n",
        "\n",
        "    Args:\n",
        "        name (str): Logger name\n",
        "        level (int): Logging level (e.g., logging.INFO)\n",
        "        log_file (str or None): Custom log file path. If None, auto-generates one.\n",
        "\n",
        "    Returns:\n",
        "        logging.Logger: Configured logger\n",
        "    \"\"\"\n",
        "    logger = logging.getLogger(name)\n",
        "    logger.setLevel(level)\n",
        "    logger.propagate = False  # Avoid duplicate logs\n",
        "\n",
        "    # üßº Remove existing handlers if already attached\n",
        "    if logger.hasHandlers():\n",
        "        logger.handlers.clear()\n",
        "\n",
        "    # üì¶ Formatter\n",
        "    formatter = logging.Formatter(\n",
        "        fmt='%(asctime)s ‚Äî %(name)s ‚Äî %(levelname)s ‚Äî %(message)s',\n",
        "        datefmt='%Y-%m-%d %H:%M:%S'\n",
        "    )\n",
        "\n",
        "    # üñ•Ô∏è Console handler\n",
        "    console_handler = logging.StreamHandler(sys.stdout)\n",
        "    console_handler.setFormatter(formatter)\n",
        "    logger.addHandler(console_handler)\n",
        "\n",
        "    # üìÅ File handler setup\n",
        "    if log_file is None:\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        log_dir = os.path.join(os.getcwd(), 'logs')  # üëà Safe fallback to current dir\n",
        "        os.makedirs(log_dir, exist_ok=True)\n",
        "        log_file = os.path.join(log_dir, f'training_{timestamp}.log')\n",
        "    else:\n",
        "        log_dir = os.path.dirname(log_file)\n",
        "        if log_dir:\n",
        "            os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "    file_handler = logging.FileHandler(log_file)\n",
        "    file_handler.setFormatter(formatter)\n",
        "    logger.addHandler(file_handler)\n",
        "\n",
        "    # Log header\n",
        "    logger.info(f\"Logger initialized: {name}\")\n",
        "    logger.info(f\"Log file created at: {os.path.abspath(log_file)}\")\n",
        "    logger.info(f\"Python version: {sys.version}\")\n",
        "\n",
        "    return logger\n",
        "\n",
        "# Use it\n",
        "logger = setup_logger(\"train_logger\", logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af93ecb3",
      "metadata": {
        "id": "af93ecb3"
      },
      "source": [
        "## Memory Usage Monitoring\n",
        "\n",
        "The `print_memory_usage()` function monitors system resource utilization during model training:\n",
        "\n",
        "- Tracks RAM usage by getting the Resident Set Size (RSS) of current process in GB\n",
        "- For GPU-enabled systems:\n",
        "  - Reports allocated GPU memory\n",
        "  - Shows total available GPU memory\n",
        "  - Calculates percentage of GPU memory utilization\n",
        "  - Resets peak memory tracking statistics\n",
        "\n",
        "This helps identify potential memory bottlenecks and optimize resource usage during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff63c13c",
      "metadata": {
        "id": "ff63c13c"
      },
      "outputs": [],
      "source": [
        "def print_memory_usage():\n",
        "    process = psutil.Process(os.getpid())\n",
        "\n",
        "    ram_gb = process.memory_info().rss / 1e9\n",
        "    total_gb = psutil.virtual_memory().total / 1e9\n",
        "\n",
        "    logger.info(f\"RAM usage: {ram_gb:.2f} GB\")\n",
        "    logger.info(f\"Total system RAM: {total_gb:.2f} GB\")\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "        gpu_mem = torch.cuda.memory_allocated() / 1e9\n",
        "        gpu_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "        peak_gpu_mem = torch.cuda.max_memory_allocated() / 1e9\n",
        "\n",
        "        logger.info(f\"GPU memory usage: {gpu_mem:.2f}/{gpu_total:.2f} GB ({gpu_mem/gpu_total*100:.1f}%)\")\n",
        "        logger.info(f\"Peak GPU memory: {peak_gpu_mem:.2f} GB\")\n",
        "\n",
        "        torch.cuda.reset_peak_memory_stats()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0e3025a",
      "metadata": {
        "id": "c0e3025a",
        "outputId": "9b631e00-02a0-4a97-8c38-1eee0e267196"
      },
      "outputs": [],
      "source": [
        "# Testing memory usage\n",
        "print_memory_usage()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df71ee77",
      "metadata": {
        "id": "df71ee77"
      },
      "source": [
        "## MetricsTrackingCallback\n",
        "\n",
        "The `MetricsTrackingCallback` is a custom callback for tracking and logging training metrics during the training process. It is designed to work with the Hugging Face Trainer API and provides functionality to log various metrics at specified intervals.\n",
        "\n",
        "### This callback is particularly useful because it:\n",
        "\n",
        "- Provides real-time monitoring of training progress\n",
        "- Creates visualizations to help understand model performance\n",
        "- Saves metrics for later analysis\n",
        "- Helps identify potential issues during training (like overfitting or unstable training)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de4aa1a7",
      "metadata": {
        "id": "de4aa1a7"
      },
      "outputs": [],
      "source": [
        "class MetricsTrackingCallback(TrainerCallback):\n",
        "    \"\"\"\n",
        "    A callback to track and plot training metrics during training.\n",
        "    \"\"\"\n",
        "    def __init__(self, output_dir: str, plot_every_n: int = 5):\n",
        "        self.output_dir = output_dir\n",
        "        self.plot_every_n = plot_every_n\n",
        "        self.training_loss = []  # (step, loss)\n",
        "        self.eval_metrics_by_key = defaultdict(list)  # key -> [(step, value)]\n",
        "        self.eval_count = 0\n",
        "        logger.info(\"Initialized MetricsTrackingCallback\")\n",
        "\n",
        "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
        "        \"\"\"Called after evaluation\"\"\"\n",
        "        if not metrics:\n",
        "            logger.debug(\"No metrics provided in on_evaluate\")\n",
        "            return\n",
        "        step = state.global_step\n",
        "        for key, value in metrics.items():\n",
        "            if isinstance(value, (int, float)):\n",
        "                self.eval_metrics_by_key[key].append((step, value))\n",
        "                logger.info(f\"Tracked metric: {key}={value} at step {step}\")\n",
        "        self.eval_count += 1\n",
        "        if self.eval_count % self.plot_every_n == 0:\n",
        "            self._generate_plots()\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        \"\"\"Track training loss only\"\"\"\n",
        "        if not logs:\n",
        "            logger.debug(\"No logs provided\")\n",
        "            return\n",
        "        step = state.global_step\n",
        "        if 'loss' in logs:\n",
        "            self.training_loss.append((step, logs['loss']))\n",
        "            logger.info(f\"Logged loss: {logs['loss']} at step {step}\")\n",
        "\n",
        "    def on_train_end(self, args, state, control, **kwargs):\n",
        "        \"\"\"Generate final plots and save metrics\"\"\"\n",
        "        try:\n",
        "            self._generate_plots()\n",
        "            self._save_metrics_data()\n",
        "            logger.info(f\"Training completed. Metrics saved to {self.output_dir}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to save metrics on train end: {e}\")\n",
        "\n",
        "    def _generate_plots(self):\n",
        "        \"\"\"Generate plots for loss and selected metrics\"\"\"\n",
        "        plot_dir = os.path.join(self.output_dir, 'plots')\n",
        "        os.makedirs(plot_dir, exist_ok=True)\n",
        "        plt.figure(figsize=(12, 8))\n",
        "\n",
        "        # Plot training loss\n",
        "        if self.training_loss:\n",
        "            steps, losses = zip(*self.training_loss)\n",
        "            plt.subplot(2, 1, 1)\n",
        "            plt.plot(steps, losses, label='Training Loss', color='blue')\n",
        "            plt.xlabel('Steps')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.title('Training Loss')\n",
        "            plt.legend()\n",
        "\n",
        "        # Plot ROUGE-L F1 (and optionally other metrics)\n",
        "        if self.eval_metrics_by_key.get('rougeL_f1'):\n",
        "            steps, metrics = zip(*self.eval_metrics_by_key['rougeL_f1'])\n",
        "            plt.subplot(2, 1, 2)\n",
        "            plt.plot(steps, metrics, label='ROUGE-L F1', color='red')\n",
        "            plt.xlabel('Steps')\n",
        "            plt.ylabel('ROUGE-L F1')\n",
        "            plt.title('Evaluation Metric')\n",
        "            plt.legend()\n",
        "\n",
        "        try:\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(plot_dir, 'training_metrics.png'))\n",
        "            plt.close()\n",
        "            logger.info(f\"Saved plot to {plot_dir}/training_metrics.png\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to save plot: {e}\")\n",
        "\n",
        "    def _save_metrics_data(self):\n",
        "        \"\"\"Save metrics to CSVs\"\"\"\n",
        "        try:\n",
        "            if self.training_loss:\n",
        "                pd.DataFrame(self.training_loss, columns=['step', 'loss'])\\\n",
        "                  .to_csv(os.path.join(self.output_dir, 'training_loss.csv'), index=False)\n",
        "                logger.info(f\"Saved training_loss.csv\")\n",
        "            for key, metrics in self.eval_metrics_by_key.items():\n",
        "                pd.DataFrame(metrics, columns=['step', key])\\\n",
        "                  .to_csv(os.path.join(self.output_dir, f'eval_{key}.csv'), index=False)\n",
        "                logger.info(f\"Saved eval_{key}.csv\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to save metrics CSVs: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2caf1ad9",
      "metadata": {
        "id": "2caf1ad9"
      },
      "source": [
        "## RogerReportCallback class:\n",
        "\n",
        "1. Purpose:\n",
        "\n",
        "- Creates a detailed JSON report of the training process\n",
        "- Captures timing information\n",
        "- Records model and dataset details\n",
        "- Stores training metrics and system information\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b57ab54",
      "metadata": {
        "id": "1b57ab54"
      },
      "outputs": [],
      "source": [
        "# Roger report callback\n",
        "class RogerReportCallback(TrainerCallback):\n",
        "    \"\"\"\n",
        "    A callback to generate a training report at the end of training.\n",
        "    \"\"\"\n",
        "    def __init__(self, output_dir: str, config):\n",
        "        self.output_dir = output_dir # Directory to save report\n",
        "        self.config = config # Configuration object\n",
        "        self.start_time = datetime.now() # Start time of training\n",
        "        logger.info(f\"Training started at {self.start_time.isoformat()}\")\n",
        "\n",
        "    def on_train_end(self, args, state, control, **kwargs):\n",
        "        end_time = datetime.now() # End time of training\n",
        "\n",
        "        # Calculate duration in minutes\n",
        "        duration = (end_time - self.start_time).total_seconds() / 60\n",
        "\n",
        "        # Safe extraction with fallback\n",
        "        def safe_get(obj, attr, default='N/A'):\n",
        "            return getattr(obj, attr, default)\n",
        "\n",
        "        # Get training state and arguments\n",
        "        report = {\n",
        "            'training_summary': {\n",
        "                'model': self.config.model.name,\n",
        "                'dataset': self.config.dataset.name,\n",
        "                'start_time': self.start_time.isoformat(),\n",
        "                'end_time': end_time.isoformat(),\n",
        "                'duration_minutes': duration,\n",
        "                'epochs': safe_get(self.config.training, 'epochs'),\n",
        "                'train_examples': safe_get(state, 'num_train_examples'),\n",
        "                'eval_examples': safe_get(state, 'num_eval_examples'),\n",
        "                'best_metric': safe_get(state, 'best_metric'),\n",
        "                'best_model_checkpoint': safe_get(state, 'best_model_checkpoint'),\n",
        "            },\n",
        "            'state': {\n",
        "                k: v for k, v in state.__dict__.items()\n",
        "                if isinstance(v, (int, float, str, bool))\n",
        "            },\n",
        "            'training_args': {\n",
        "                k: v for k, v in args.__dict__.items()\n",
        "                if isinstance(v, (int, float, str, bool))\n",
        "            },\n",
        "            'config': OmegaConf.to_container(self.config),\n",
        "            'system_info': {\n",
        "                'torch_version': torch.__version__,\n",
        "                'cuda_available': torch.cuda.is_available(),\n",
        "                'cuda_device_count': torch.cuda.device_count() if torch.cuda.is_available() else 0,\n",
        "                'cuda_device_name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU',\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # create output directory if it doesn't exist\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "        # Save the report as a JSON file\n",
        "        report_path = os.path.join(self.output_dir, 'training_report.json')\n",
        "        with open(report_path, 'w') as f:\n",
        "            json.dump(report, f, indent=2)\n",
        "        logger.info(f\"Training report saved to {report_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e1e36dc",
      "metadata": {
        "id": "3e1e36dc"
      },
      "source": [
        "## load_and_preprocess_data function:\n",
        "\n",
        "- Loads the BillSum dataset from Hugging Face\n",
        "- Preprocesses the data by tokenizing the input and output sequences\n",
        "- Splits the dataset into training and validation sets\n",
        "- Returns the preprocessed datasets for training and validation\n",
        "\n",
        "### This function is crucial for the fine-tuning process because it:\n",
        "\n",
        "1. Ensures data quality by removing invalid examples\n",
        "2. Prevents memory issues by filtering too-long sequences\n",
        "3. Provides consistent train/eval splits\n",
        "4. Saves processing time through caching\n",
        "5. Maintains reproducibility through fixed seeds\n",
        "6. Gives visibility into the data preparation process\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed257295",
      "metadata": {
        "id": "ed257295"
      },
      "outputs": [],
      "source": [
        "def filter_non_empty(example: dict) -> bool:\n",
        "    \"\"\"\n",
        "    Safely filter out dataset examples where 'text' or 'summary' is missing, empty, or too short.\n",
        "\n",
        "    Args:\n",
        "        example (dict): A dictionary with at least 'text' and 'summary' keys.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if valid, False otherwise.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        text = example.get('text', '')\n",
        "        summary = example.get('summary', '')\n",
        "\n",
        "        if not isinstance(text, str) or not isinstance(summary, str):\n",
        "            logger.debug(f\"Invalid types - text: {type(text)}, summary: {type(summary)}\")\n",
        "            return False\n",
        "\n",
        "        text = text.strip()\n",
        "        summary = summary.strip()\n",
        "\n",
        "        text_valid = bool(text) and len(text.split()) >= 3\n",
        "        summary_valid = bool(summary) and len(summary.split()) >= 1\n",
        "\n",
        "        if not (text_valid and summary_valid):\n",
        "            logger.debug(\n",
        "                f\"Filtered example - text_len={len(text)} chars ({len(text.split())} words), \"\n",
        "                f\"summary_len={len(summary)} chars ({len(summary.split())} words)\"\n",
        "            )\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Exception during filtering: {e}\")\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "552f3ff3",
      "metadata": {
        "id": "552f3ff3"
      },
      "outputs": [],
      "source": [
        "def load_and_prepare_dataset(cfg, tokenizer=None):\n",
        "    \"\"\"\n",
        "    Load and preprocess the dataset based on the provided config.\n",
        "\n",
        "    Args:\n",
        "        cfg (DictConfig): Hydra/OmegaConf config object.\n",
        "        tokenizer (PreTrainedTokenizer): Optional tokenizer for token-based filtering.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dict with 'train' and 'eval' splits.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logger.info(f\"Loading dataset: {cfg.dataset.name}\")\n",
        "\n",
        "        # Load a sample or full dataset\n",
        "        if cfg.dataset.sample_size:\n",
        "            ds = load_dataset(cfg.dataset.name, split=f\"train[:{cfg.dataset.sample_size}]\", verification_mode=\"no_checks\")\n",
        "        else:\n",
        "            ds = load_dataset(cfg.dataset.name, split=\"train\", verification_mode=\"no_checks\")\n",
        "\n",
        "        logger.info(f\"Dataset loaded with {len(ds)} examples\")\n",
        "\n",
        "        # Validate required columns\n",
        "        for col in [cfg.dataset.text_col, cfg.dataset.summary_col]:\n",
        "            if col not in ds.column_names:\n",
        "                raise ValueError(f\"Column '{col}' not found in dataset\")\n",
        "\n",
        "        # Rename for consistency\n",
        "        ds = ds.rename_columns({\n",
        "            cfg.dataset.text_col: \"article\",\n",
        "            cfg.dataset.summary_col: \"highlights\"\n",
        "        })\n",
        "        logger.info(f\"Dataset columns after renaming: {ds.column_names}\")\n",
        "\n",
        "        # Filter out empty or bad examples\n",
        "        ds = ds.filter(filter_non_empty, num_proc=cfg.preprocessing.num_proc)\n",
        "        logger.info(f\"After non-empty filtering: {len(ds)} examples remain\")\n",
        "\n",
        "        # Optionally filter based on token length\n",
        "        if tokenizer and cfg.dataset.filter_by_length:\n",
        "            def within_token_limit(example):\n",
        "                return (\n",
        "                    len(tokenizer.encode(example[\"article\"])) < cfg.dataset.max_input_tokens and\n",
        "                    len(tokenizer.encode(example[\"highlights\"])) < cfg.dataset.max_target_tokens\n",
        "                )\n",
        "            ds = ds.filter(within_token_limit, num_proc=cfg.preprocessing.num_proc)\n",
        "            logger.info(f\"After token-length filtering: {len(ds)} examples remain\")\n",
        "\n",
        "        # Shuffle and split\n",
        "        ds = ds.shuffle(seed=cfg.seed)\n",
        "        train_size = int(cfg.split.train_frac * len(ds))\n",
        "\n",
        "        logger.info(f\"Train size: {train_size}, Eval size: {len(ds) - train_size}\")\n",
        "\n",
        "        return {\n",
        "            \"train\": ds.select(range(train_size)),\n",
        "            \"eval\": ds.select(range(train_size, len(ds)))\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Dataset loading/preprocessing failed: {e}\")\n",
        "        raise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89572de2",
      "metadata": {
        "id": "89572de2"
      },
      "source": [
        "## setup_model_and_tokenizer function:\n",
        "\n",
        "- Loads the Flan-T5-base model and tokenizer from Hugging Face\n",
        "- Configures the model for LoRA training\n",
        "- Returns the model and tokenizer objects\n",
        "\n",
        "### This function is important because it:\n",
        "\n",
        "1. Minimizes memory usage through LoRA\n",
        "2. Provides extensive configuration options\n",
        "3. Handles common edge cases\n",
        "4. Gives clear visibility into the model setup process\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd5c32f6",
      "metadata": {
        "id": "fd5c32f6"
      },
      "outputs": [],
      "source": [
        "def setup_model_and_tokenizer(cfg):\n",
        "    logger.info(f\"Loading tokenizer from {cfg.model.name}\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(cfg.model.name)\n",
        "    tokenizer.padding_side = cfg.tokenizer.get(\"padding_side\", \"right\")\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        logger.warning(\"Tokenizer has no pad token. Setting pad_token = eos_token.\")\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    logger.info(f\"Loading base model from {cfg.model.name}\")\n",
        "    low_cpu_mem = getattr(cfg.model.loading_args, \"low_cpu_mem_usage\", True)\n",
        "    device_map = getattr(cfg.model.loading_args, \"device_map\", \"auto\")\n",
        "\n",
        "    # model loading\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "        cfg.model.name,\n",
        "        torch_dtype=torch.float16 if cfg.training.fp16 else None,\n",
        "        low_cpu_mem_usage=low_cpu_mem,\n",
        "        device_map=device_map\n",
        "    )\n",
        "\n",
        "    logger.info(f\"Model class: {model.__class__.__name__}\")\n",
        "\n",
        "    # setup LoRA\n",
        "    lora_cfg = LoraConfig(\n",
        "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "        r=cfg.lora.r,\n",
        "        lora_alpha=cfg.lora.alpha,\n",
        "        target_modules=cfg.lora.target_modules,\n",
        "        lora_dropout=cfg.lora.dropout,\n",
        "        bias=cfg.lora.bias\n",
        "    )\n",
        "\n",
        "    logger.info(f\"LoRA config: {lora_cfg}\")\n",
        "    logger.info(f\"LoRA targeting modules: {lora_cfg.target_modules}\")\n",
        "\n",
        "    # Apply LoRA\n",
        "    logger.info(\"Applying LoRA to the model...\")\n",
        "    peft_model = get_peft_model(model, lora_cfg)\n",
        "    logger.info(\"LoRA applied successfully.\")\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\n",
        "\n",
        "    logger.info(f\"Total parameters: {total_params:,}\")\n",
        "    logger.info(f\"Trainable parameters: {trainable_params:,} ({trainable_params / total_params * 100:.2f}%)\")\n",
        "\n",
        "    return tokenizer, peft_model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "569c8c79",
      "metadata": {
        "id": "569c8c79"
      },
      "source": [
        "## Preprocessing Function\n",
        "\n",
        "The `preprocess_datasets` function handles:\n",
        "\n",
        "- Tokenization of input texts and summaries\n",
        "- Proper padding and truncation\n",
        "- Handling of special tokens\n",
        "- Batch processing for efficiency\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45dedb81",
      "metadata": {
        "id": "45dedb81"
      },
      "outputs": [],
      "source": [
        "def chunk_with_stride(text, tokenizer, max_length=512, stride=128):\n",
        "    tokenized = tokenizer(\n",
        "        text,\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        stride=stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_length=True,\n",
        "    )\n",
        "    input_ids = tokenized[\"input_ids\"]\n",
        "    lengths   = tokenized[\"length\"]\n",
        "    chunks = []\n",
        "    for ids, length in zip(input_ids, lengths):\n",
        "        chunks.append(tokenizer.decode(ids[:length], skip_special_tokens=True))\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53bde6be",
      "metadata": {
        "id": "53bde6be"
      },
      "outputs": [],
      "source": [
        "def preprocess_datasets(tokenizer, datasets: Dict[str, Dataset], cfg) -> Dict[str, Dataset]:\n",
        "    \"\"\"\n",
        "    Preprocess datasets by tokenizing inputs and targets for seq2seq training.\n",
        "\n",
        "    Args:\n",
        "        tokenizer (PreTrainedTokenizer): Tokenizer for encoding text (e.g., Flan-T5-base).\n",
        "        datasets (dict): Dictionary of train/eval datasets with 'article' and 'highlights' columns.\n",
        "        cfg: Config object with dataset.max_input_tokens, max_target_tokens, and prompt.prefix.\n",
        "\n",
        "    Returns:\n",
        "        dict: Processed datasets with tokenized inputs and labels.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If required columns are missing.\n",
        "        Exception: If tokenization or mapping fails.\n",
        "    \"\"\"\n",
        "\n",
        "    # here we assume that the datasets are already loaded and passed as a dictionary\n",
        "    # with keys 'train', 'validation', etc.\n",
        "\n",
        "    # setup logging\n",
        "    logger.info(\"Preprocessing datasets...\")\n",
        "\n",
        "    def preprocess_function(examples):\n",
        "        try:\n",
        "            # Validate inputs\n",
        "            inputs = []\n",
        "            for doc in examples['article']:\n",
        "                if not isinstance(doc, str):\n",
        "                    logger.warning(f\"Non-string article: {type(doc)}\")\n",
        "                    doc = \"\"\n",
        "\n",
        "                # Chunk the document if it's too long\n",
        "                chunks = chunk_with_stride(doc, tokenizer=tokenizer, max_length=cfg.dataset.max_input_tokens)\n",
        "\n",
        "                # Add prefix to the first chunk\n",
        "                # prompt prefix -- Summarize this legal document:\\n\n",
        "                inputs.append(cfg.prompt.prefix + (chunks[0] if chunks else doc[:cfg.dataset.max_input_tokens]))\n",
        "\n",
        "            # Tokenize inputs\n",
        "            model_inputs = tokenizer(\n",
        "                inputs,\n",
        "                max_length=cfg.dataset.max_input_tokens,\n",
        "                padding=False,\n",
        "                truncation=True\n",
        "            )\n",
        "\n",
        "            # Tokenize targets\n",
        "            highlights = [h if isinstance(h, str) else \"\" for h in examples[\"highlights\"]]\n",
        "            if not all(isinstance(h, str) for h in highlights):\n",
        "                logger.warning(\"Non-string highlights detected\")\n",
        "            labels = tokenizer(\n",
        "                highlights,\n",
        "                max_length=cfg.dataset.max_target_tokens,\n",
        "                padding=False,\n",
        "                truncation=True\n",
        "            )\n",
        "\n",
        "            model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "            return model_inputs\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Preprocessing failed for batch: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    processed_datasets = {}\n",
        "\n",
        "    for split, dataset in datasets.items():\n",
        "        logger.info(f\"Processing {split} split...\")\n",
        "        try:\n",
        "            # Validate columns\n",
        "            if not all(col in dataset.column_names for col in ['article', 'highlights']):\n",
        "                raise ValueError(f\"Missing required columns in {split} split: {dataset.column_names}\")\n",
        "\n",
        "            processed = dataset.map(\n",
        "                preprocess_function,\n",
        "                batched=True,\n",
        "                batch_size=1000,\n",
        "                remove_columns=dataset.column_names,\n",
        "                desc=f\"Preprocessing {split} dataset\",\n",
        "                num_proc=max(1, cfg.preprocessing.num_proc // 2),\n",
        "                load_from_cache_file=True\n",
        "            )\n",
        "            processed_datasets[split] = processed\n",
        "            logger.info(f\"Processed {len(processed)} examples in {split} split\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing {split} split: {str(e)}\")\n",
        "            logger.error(f\"Dataset columns: {dataset.column_names}\")\n",
        "            raise\n",
        "\n",
        "    return processed_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a1fb1b3",
      "metadata": {
        "id": "0a1fb1b3"
      },
      "source": [
        "## Setup Training\n",
        "\n",
        "- Defines training arguments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "305055fc",
      "metadata": {
        "id": "305055fc"
      },
      "outputs": [],
      "source": [
        "def setup_training(tokenizer, cfg):\n",
        "    logger.info(\"Setting up training configuration...\")\n",
        "\n",
        "    # Set compute dtype\n",
        "    compute_dtype = torch.float16 if cfg.training.fp16 else torch.float32\n",
        "    logger.info(f\"Compute dtype: {compute_dtype}\")\n",
        "\n",
        "    logger.info(f\"Loading model from {cfg.model.name}\")\n",
        "    # Load base model with proper memory settings\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "        cfg.model.name,\n",
        "        torch_dtype=compute_dtype,\n",
        "        low_cpu_mem_usage=cfg.model.loading_args.low_cpu_mem_usage,\n",
        "        device_map=cfg.model.loading_args.device_map\n",
        "    )\n",
        "    logger.info(f\"Model loaded successfully.\")\n",
        "    logger.info(f\"Model class: {model.__class__.__name__}\")\n",
        "    logger.info(f\"Model config: {model.config}\")\n",
        "    logger.info(f\"Model parameters: {model.num_parameters():,}\")\n",
        "\n",
        "    # Prepare model for LoRA training\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "    # LoRA Configuration\n",
        "    peft_config = LoraConfig(\n",
        "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "        inference_mode=False,\n",
        "        r=cfg.lora.r,\n",
        "        lora_alpha=cfg.lora.alpha,\n",
        "        lora_dropout=cfg.lora.dropout,\n",
        "        target_modules=cfg.lora.target_modules,\n",
        "        bias=cfg.lora.bias\n",
        "    )\n",
        "    model = get_peft_model(model, peft_config)\n",
        "\n",
        "    # Training arguments\n",
        "    training_args = Seq2SeqTrainingArguments(\n",
        "        output_dir=cfg.output_dir,\n",
        "        evaluation_strategy=cfg.training.evaluation_strategy,\n",
        "        learning_rate=cfg.training.lr,\n",
        "        per_device_train_batch_size=cfg.training.batch_size,\n",
        "        per_device_eval_batch_size=cfg.training.eval_batch_size,\n",
        "        gradient_accumulation_steps=cfg.training.grad_accum_steps,\n",
        "        num_train_epochs=cfg.training.epochs,\n",
        "        weight_decay=cfg.training.weight_decay,\n",
        "        logging_steps=cfg.training.logging_steps,\n",
        "        save_steps=cfg.training.save_steps,\n",
        "        eval_steps=cfg.training.eval_steps,\n",
        "        save_strategy=cfg.training.save_strategy,\n",
        "        metric_for_best_model=cfg.training.metric_for_best,\n",
        "        greater_is_better=cfg.training.greater_is_better,\n",
        "        load_best_model_at_end=cfg.training.load_best_model_at_end,\n",
        "        save_total_limit=2,\n",
        "        generation_max_length=cfg.dataset.max_target_tokens,\n",
        "        generation_num_beams=cfg.generation.num_beams,\n",
        "        fp16=cfg.training.fp16,\n",
        "        optim=\"paged_adamw_32bit\",\n",
        "        gradient_checkpointing=cfg.training.gradient_checkpointing\n",
        "    )\n",
        "\n",
        "    # Data collator\n",
        "    data_collator = DataCollatorForSeq2Seq(\n",
        "        tokenizer,\n",
        "        model=model,\n",
        "        label_pad_token_id=-100,\n",
        "        pad_to_multiple_of=8 if cfg.training.fp16 else None\n",
        "    )\n",
        "\n",
        "    return model, training_args, data_collator"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcd34de3",
      "metadata": {
        "id": "dcd34de3"
      },
      "source": [
        "# Train function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b845bc4",
      "metadata": {
        "id": "1b845bc4"
      },
      "outputs": [],
      "source": [
        "def train(cfg):\n",
        "    try:\n",
        "        # Set up logging\n",
        "        logger.info(\"Starting training process...\")\n",
        "\n",
        "        # Load tokenizer\n",
        "        logger.info(\"Loading tokenizer...\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(cfg.model.name)\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token or tokenizer.unk_token\n",
        "\n",
        "\n",
        "        # Load and prepare datasets\n",
        "        logger.info(\"Loading and preparing datasets...\")\n",
        "        datasets = load_and_prepare_dataset(cfg, tokenizer)\n",
        "        print_memory_usage()  # Print memory usage after loading datasets\n",
        "        logger.info(f\"Loaded {len(datasets['train'])} training examples and {len(datasets['eval'])} evaluation examples\")\n",
        "\n",
        "\n",
        "        processed_datasets = preprocess_datasets(tokenizer, datasets, cfg)\n",
        "\n",
        "        # Set up model and training configuration\n",
        "        logger.info(\"Setting up model and training configuration...\")\n",
        "        model, training_args, data_collator = setup_training(tokenizer, cfg)\n",
        "\n",
        "        # Setup callbacks\n",
        "        callbacks = [\n",
        "            MetricsTrackingCallback(cfg.output_dir),\n",
        "            RogerReportCallback(cfg.output_dir, cfg)\n",
        "        ]\n",
        "\n",
        "        # Set up trainer\n",
        "        trainer = Seq2SeqTrainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=processed_datasets[\"train\"],\n",
        "            eval_dataset=processed_datasets[\"eval\"],  # Changed from \"validation\" to \"eval\"\n",
        "            data_collator=data_collator,\n",
        "            tokenizer=tokenizer,\n",
        "            callbacks=callbacks\n",
        "        )\n",
        "\n",
        "        logger.info(\"Starting training...\")\n",
        "        trainer.train()\n",
        "\n",
        "        # Save the final model\n",
        "        logger.info(\"Saving final model...\")\n",
        "        final_model_dir = os.path.join(cfg.output_dir, \"final_model\")\n",
        "        trainer.save_model(final_model_dir)\n",
        "\n",
        "        logger.info(f\"Training completed successfully! Model saved to: {final_model_dir}\")\n",
        "        return cfg.output_dir\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during training: {str(e)}\")\n",
        "        raise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac5187be",
      "metadata": {
        "id": "ac5187be"
      },
      "source": [
        "## Main Execution\n",
        "\n",
        "Load the configuration and start the training process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc147d10",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "ed3f446626e5414ab88692db5c02535d"
          ]
        },
        "id": "dc147d10",
        "outputId": "27693b2f-628e-475f-dad3-a9cdf2763011"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    config_path = \"../configs/billsum.yaml\"\n",
        "\n",
        "    try:\n",
        "        # 1. Load configuration\n",
        "        cfg = OmegaConf.load(config_path)\n",
        "        logger.info(f\"‚úÖ Loaded configuration from {config_path}\")\n",
        "        logger.debug(f\"üîß Full config:\\n{OmegaConf.to_yaml(cfg)}\")\n",
        "\n",
        "        # 2. Log core info\n",
        "        logger.info(f\"üì¶ Model: {cfg.model.name}\")\n",
        "        logger.info(f\"üìä Dataset: {cfg.dataset.name}\")\n",
        "        logger.info(f\"üìÅ Output dir: {cfg.output_dir}\")\n",
        "        logger.info(f\"üìà Epochs: {cfg.training.epochs}\")\n",
        "\n",
        "        # 3. Create output directory early\n",
        "        os.makedirs(cfg.output_dir, exist_ok=True)\n",
        "\n",
        "        # 4. Launch training\n",
        "        output_dir = train(cfg)\n",
        "        logger.info(f\"üèÅ Training completed successfully! Artifacts at: {output_dir}\")\n",
        "\n",
        "        # 5. Final memory log\n",
        "        print_memory_usage()\n",
        "        sys.exit(0)\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Training failed: {str(e)}\")\n",
        "        traceback_str = traceback.format_exc()\n",
        "        logger.error(f\"üìâ Full traceback:\\n{traceback_str}\")\n",
        "        sys.exit(1)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
