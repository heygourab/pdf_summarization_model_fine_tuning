{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c206dd2a",
   "metadata": {},
   "source": [
    "# Fine-Tune Flan-T5-base on BillSum\n",
    "Author: Gourab S.(heygourab),\n",
    "github: https://github.com/heygourab\n",
    "\n",
    "This notebook fine-tunes Flan-T5-base with LoRA on the BillSum dataset (~800 samples) for legal document summarization. Outputs a LoRA adapter (`lora_billsum`) and a `training_report.json` via RogerReportCallback.\n",
    "\n",
    "The model is trained for 3 epochs with a batch size of 16 and a learning rate of 2e-4.\n",
    "The model is saved in the `lora_billsum` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc5d042",
   "metadata": {},
   "source": [
    "## For macOS M1/Apple Silicon\n",
    "Drop this cell into your Jupyter Notebook (assuming venv is active and you're not using Colab):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "931c111e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ../requirements/mac.txt -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda7c950",
   "metadata": {},
   "source": [
    "## Import necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f0ec670",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from typing import Dict\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq, TrainerCallback\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from accelerate import Accelerator\n",
    "import evaluate\n",
    "from omegaconf import OmegaConf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358b8833",
   "metadata": {},
   "source": [
    "### Setup logger\n",
    "Sets up logging to track training progress and debugging information. The logger is configured to:\n",
    "- Display timestamp, log level, and message\n",
    "- Output logs to standard output (stdout)\n",
    "- Use INFO level logging\n",
    "- Create a logger instance named after the current module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "646c33ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging setup\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af93ecb3",
   "metadata": {},
   "source": [
    "## Memory Usage Monitoring\n",
    "The `print_memory_usage()` function monitors system resource utilization during model training:\n",
    "\n",
    "- Tracks RAM usage by getting the Resident Set Size (RSS) of current process in GB\n",
    "- For GPU-enabled systems:\n",
    "    - Reports allocated GPU memory\n",
    "    - Shows total available GPU memory\n",
    "    - Calculates percentage of GPU memory utilization\n",
    "    - Resets peak memory tracking statistics\n",
    "\n",
    "This helps identify potential memory bottlenecks and optimize resource usage during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff63c13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory usage\n",
    "def print_memory_usage():\n",
    "    process = psutil.Process(os.getpid()) # Get current process\n",
    "    # Get memory usage in GB\n",
    "    ram_gb = process.memory_info().rss / 1e9\n",
    "    # Get total memory in GB\n",
    "    total_gb = psutil.virtual_memory().total / 1e9\n",
    "    # Print memory usage\n",
    "    logger.info(f\"RAM usage: {ram_gb:.2f} GB\")\n",
    "    logger.info(f\"Total memory: {total_gb:.2f} GB\")\n",
    "\n",
    "    # if running on GPU, print GPU memory usage\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_mem = torch.cuda.memory_allocated() / 1e9\n",
    "        gpu_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        logger.info(f\"GPU memory: {gpu_mem:.2f}/{gpu_total:.2f} GB ({gpu_mem/gpu_total*100:.1f}%)\")\n",
    "        torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0e3025a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-15 18:00:42,740 - INFO - RAM usage: 0.03 GB\n",
      "2025-05-15 18:00:42,743 - INFO - Total memory: 8.59 GB\n"
     ]
    }
   ],
   "source": [
    "# Testing memory usage\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df71ee77",
   "metadata": {},
   "source": [
    "## MetricsTrackingCallback\n",
    "The `MetricsTrackingCallback` is a custom callback for tracking and logging training metrics during the training process. It is designed to work with the Hugging Face Trainer API and provides functionality to log various metrics at specified intervals.\n",
    "\n",
    "### This callback is particularly useful because it:\n",
    "\n",
    "- Provides real-time monitoring of training progress\n",
    "- Creates visualizations to help understand model performance\n",
    "- Saves metrics for later analysis\n",
    "- Helps identify potential issues during training (like overfitting or unstable training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de4aa1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MetricsTrackingCallback \n",
    "class MetricsTrackingCallback(TrainerCallback):\n",
    "\n",
    "    \"\"\"\n",
    "    A callback to track and plot training metrics during training.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dir: str, plot_every_n: int = 5):\n",
    "        self.output_dir = output_dir # Directory to save plots and metrics\n",
    "        self.plot_every_n = plot_every_n # Frequency of plotting\n",
    "        self.training_loss = [] # List to store training loss\n",
    "        self.eval_metrics = [] # List to store evaluation metrics\n",
    "        self.step_numbers = [] # List to store step numbers\n",
    "\n",
    "    # function for logging metrics\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if not logs: # Check if logs are empty\n",
    "            return   # No logs to process\n",
    "        step = state.global_step \n",
    "        if 'loss' in logs:\n",
    "            # Append loss to training loss list\n",
    "            self.training_loss.append((step, logs['loss'])) \n",
    "        if 'eval_rougeL_f1' in logs: # Check if eval_rougeL_f1 is in logs\n",
    "            # Append eval_rougeL_f1 to eval metrics list\n",
    "            self.eval_metrics.append((step, logs['eval_rougeL_f1']))\n",
    "            # Append step number to step numbers list\n",
    "            self.step_numbers.append(step) \n",
    "\n",
    "        if self.eval_metrics and len(self.eval_metrics) % self.plot_every_n == 0:\n",
    "            self._generate_plots() # Generate plots every n steps\n",
    "\n",
    "    # function for on_train_end\n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        self._generate_plots() # Generate plots at the end of training\n",
    "        self._save_metrics_data() # Save metrics data to CSV\n",
    "        logger.info(f\"Training completed. Metrics saved to {self.output_dir}\")\n",
    "\n",
    "    # function for generating plots\n",
    "    def _generate_plots(self):\n",
    "        plot_dir = os.path.join(self.output_dir, 'plots') # Directory to save plots\n",
    "        os.makedirs(plot_dir, exist_ok=True) # Create directory if it doesn't exist\n",
    "\n",
    "        # Plot training loss and evaluation metrics\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        if self.training_loss:\n",
    "            steps, losses = zip(*self.training_loss)\n",
    "            plt.subplot(2, 1, 1)\n",
    "            plt.plot(steps, losses, label='Training Loss', color='blue')\n",
    "            plt.xlabel('Steps')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title('Training Loss')\n",
    "            plt.legend()\n",
    "\n",
    "        # Plot evaluation metrics\n",
    "        if self.eval_metrics:\n",
    "            steps, metrics = zip(*self.eval_metrics)\n",
    "            plt.subplot(2, 1, 2)\n",
    "            plt.plot(steps, metrics, label='ROUGE-L F1', color='red')\n",
    "            plt.xlabel('Steps')\n",
    "            plt.ylabel('ROUGE-L F1')\n",
    "            plt.title('Evaluation Metric')\n",
    "            plt.legend()\n",
    "\n",
    "        # Adjust layout and save plot\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(plot_dir, 'training_metrics.png'))\n",
    "        plt.close()\n",
    "\n",
    "    # function for saving metrics data to CSV\n",
    "    def _save_metrics_data(self):\n",
    "        if self.training_loss:\n",
    "            pd.DataFrame(self.training_loss, columns=['step', 'loss'])\\\n",
    "              .to_csv(os.path.join(self.output_dir, 'training_loss.csv'), index=False)\n",
    "        if self.eval_metrics:\n",
    "            pd.DataFrame(self.eval_metrics, columns=['step', 'rougeL_f1'])\\\n",
    "              .to_csv(os.path.join(self.output_dir, 'eval_metrics.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caf1ad9",
   "metadata": {},
   "source": [
    "## RogerReportCallback class:\n",
    "1. Purpose:\n",
    "- Creates a detailed JSON report of the training process\n",
    "- Captures timing information\n",
    "- Records model and dataset details\n",
    "- Stores training metrics and system information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b57ab54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roger report callback\n",
    "class RogerReportCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    A callback to generate a training report at the end of training.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dir: str, config):\n",
    "        self.output_dir = output_dir # Directory to save report\n",
    "        self.config = config # Configuration object\n",
    "        self.start_time = datetime.now() # Start time of training\n",
    "        logger.info(f\"Training started at {self.start_time.isoformat()}\")\n",
    "        # Print system information\n",
    "\n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        end_time = datetime.now() # End time of training\n",
    "\n",
    "        # Calculate duration in minutes\n",
    "        duration = (end_time - self.start_time).total_seconds() / 60 \n",
    "\n",
    "        # Safe extraction with fallback\n",
    "        def safe_get(obj, attr, default='N/A'):\n",
    "            return getattr(obj, attr, default)\n",
    "\n",
    "        # Get training state and arguments\n",
    "        report = {\n",
    "            'training_summary': {\n",
    "                'model': self.config.model.name,\n",
    "                'dataset': self.config.dataset.name,\n",
    "                'start_time': self.start_time.isoformat(),\n",
    "                'end_time': end_time.isoformat(),\n",
    "                'duration_minutes': duration,\n",
    "                'epochs': safe_get(self.config.training, 'epochs'),\n",
    "                'train_examples': safe_get(state, 'num_train_examples'),\n",
    "                'eval_examples': safe_get(state, 'num_eval_examples'),\n",
    "                'best_metric': safe_get(state, 'best_metric'),\n",
    "                'best_model_checkpoint': safe_get(state, 'best_model_checkpoint'),\n",
    "            },\n",
    "            'state': {\n",
    "                k: v for k, v in state.__dict__.items()\n",
    "                if isinstance(v, (int, float, str, bool))\n",
    "            },\n",
    "            'training_args': {\n",
    "                k: v for k, v in args.__dict__.items()\n",
    "                if isinstance(v, (int, float, str, bool))\n",
    "            },\n",
    "            'config': OmegaConf.to_container(self.config),\n",
    "            'system_info': {\n",
    "                'torch_version': torch.__version__,\n",
    "                'cuda_available': torch.cuda.is_available(),\n",
    "                'cuda_device_count': torch.cuda.device_count() if torch.cuda.is_available() else 0,\n",
    "                'cuda_device_name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU',\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # create output directory if it doesn't exist\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        # Save the report as a JSON file\n",
    "        report_path = os.path.join(self.output_dir, 'training_report.json')\n",
    "        with open(report_path, 'w') as f:\n",
    "            json.dump(report, f, indent=2)\n",
    "        logger.info(f\"Training report saved to {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1e36dc",
   "metadata": {},
   "source": [
    "## load_and_preprocess_data function:\n",
    "\n",
    "- Loads the BillSum dataset from Hugging Face\n",
    "- Preprocesses the data by tokenizing the input and output sequences\n",
    "- Splits the dataset into training and validation sets\n",
    "- Returns the preprocessed datasets for training and validation\n",
    "\n",
    "\n",
    "### This function is crucial for the fine-tuning process because it:\n",
    "\n",
    "1. Ensures data quality by removing invalid examples\n",
    "2. Prevents memory issues by filtering too-long sequences\n",
    "3. Provides consistent train/eval splits\n",
    "4. Saves processing time through caching\n",
    "5. Maintains reproducibility through fixed seeds\n",
    "6. Gives visibility into the data preparation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed257295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_non_empty(example):\n",
    "    return len(example['article']) > 0 and len(example['highlights']) > 0\n",
    "\n",
    "# Function to load and prepare dataset\n",
    "def load_and_prepare_dataset(cfg, tokenizer=None):\n",
    "    \"\"\"\n",
    "    Load and prepare dataset with progress tracking and caching support.\n",
    "    Args:\n",
    "        cfg: Configuration object containing dataset parameters\n",
    "        tokenizer: Optional tokenizer for length filtering\n",
    "    Returns:\n",
    "        dict: Contains 'train' and 'eval' datasets\n",
    "    \"\"\"\n",
    "\n",
    "    # Check cache first\n",
    "    cache_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'cache')\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    cache_file = os.path.join(\n",
    "        cache_dir, \n",
    "        f\"{cfg.dataset.name}_{cfg.dataset.sample_size if cfg.dataset.sample_size else 'full'}.cache\"\n",
    "    )\n",
    "\n",
    "    # Try loading from cache\n",
    "    if os.path.exists(cache_file):\n",
    "        logger.info(f\"Loading dataset from cache: {cache_file}\")\n",
    "        return torch.load(cache_file)\n",
    "\n",
    "    logger.info(f\"Loading dataset: {cfg.dataset.name}\")\n",
    "    ds = load_dataset(cfg.dataset.name, split=f\"train[:{cfg.dataset.sample_size}]\") if cfg.dataset.sample_size \\\n",
    "         else load_dataset(cfg.dataset.name)['train']\n",
    "\n",
    "    # Log initial dataset size\n",
    "    logger.info(f\"Initial dataset size: {len(ds)} examples\")\n",
    "\n",
    "    # Validate columns\n",
    "    for col in [cfg.dataset.text_col, cfg.dataset.summary_col]:\n",
    "        if col not in ds.column_names:\n",
    "            raise ValueError(f\"Column '{col}' not found in dataset\")\n",
    "\n",
    "    # Rename to standard\n",
    "    ds = ds.rename_columns({cfg.dataset.text_col: \"article\", cfg.dataset.summary_col: \"highlights\"})\n",
    "    original_len = len(ds)\n",
    "\n",
    "    # Filter empty examples with progress tracking\n",
    "    logger.info(\"Filtering empty examples...\")\n",
    "    ds = ds.filter(\n",
    "        filter_non_empty,\n",
    "        desc=\"Filtering empty examples\",\n",
    "        load_from_cache_file=True\n",
    "    )\n",
    "    logger.info(f\"Removed {original_len - len(ds)} empty examples\")\n",
    "\n",
    "    if tokenizer and cfg.dataset.get('filter_by_length', True):\n",
    "        logger.info(\"Filtering by token length...\")\n",
    "        def within_token_limit(example):\n",
    "            return len(tokenizer.encode(example['article'])) < cfg.dataset.max_input_tokens and \\\n",
    "                   len(tokenizer.encode(example['highlights'])) < cfg.dataset.max_target_tokens\n",
    "        \n",
    "        length_filtered_len = len(ds)\n",
    "        ds = ds.filter(\n",
    "            within_token_limit,\n",
    "            desc=\"Filtering by length\",\n",
    "            load_from_cache_file=True\n",
    "        )\n",
    "        logger.info(f\"Removed {length_filtered_len - len(ds)} examples exceeding token limits\")\n",
    "\n",
    "    # Shuffle and split\n",
    "    logger.info(\"Shuffling and splitting dataset...\")\n",
    "    ds = ds.shuffle(seed=cfg.seed) # 42 is the default seed\n",
    "    train_size = int(cfg.split.train_frac * len(ds))\n",
    "    \n",
    "    # Create the split datasets\n",
    "    dataset_dict = {\n",
    "        \"train\": ds.select(range(train_size)),\n",
    "        \"eval\": ds.select(range(train_size, len(ds)))\n",
    "    }\n",
    "\n",
    "    # Log split sizes\n",
    "    logger.info(f\"Train set size: {len(dataset_dict['train'])} examples\")\n",
    "    logger.info(f\"Eval set size: {len(dataset_dict['eval'])} examples\")\n",
    "\n",
    "    # Cache the processed dataset\n",
    "    logger.info(f\"Caching processed dataset to: {cache_file}\")\n",
    "    torch.save(dataset_dict, cache_file)\n",
    "\n",
    "    return dataset_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89572de2",
   "metadata": {},
   "source": [
    "## setup_model_and_tokenizer function:\n",
    "- Loads the Flan-T5-base model and tokenizer from Hugging Face\n",
    "- Configures the model for LoRA training\n",
    "- Returns the model and tokenizer objects\n",
    "\n",
    "### This function is important because it:\n",
    "1. Minimizes memory usage through LoRA\n",
    "2. Provides extensive configuration options\n",
    "3. Handles common edge cases\n",
    "4. Gives clear visibility into the model setup process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5c32f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model_and_tokenizer(cfg):\n",
    "    logger.info(f\"Loading tokenizer from {cfg.model.name}\") \n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.model.name)\n",
    "    tokenizer.padding_side = cfg.tokenizer.get(\"padding_side\", \"right\")\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        logger.warning(\"Tokenizer has no pad token. Setting pad_token = eos_token.\")\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    logger.info(f\"Loading base model from {cfg.model.name}\")\n",
    "    low_cpu_mem = getattr(cfg.model.loading_args, \"low_cpu_mem_usage\", True)\n",
    "    device_map = getattr(cfg.model.loading_args, \"device_map\", \"auto\")\n",
    "    \n",
    "    # model loading\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        cfg.model.name,\n",
    "        torch_dtype=torch.float16 if cfg.training.fp16 else None,\n",
    "        low_cpu_mem_usage=low_cpu_mem,\n",
    "        device_map=device_map\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Model class: {model.__class__.__name__}\")\n",
    "    \n",
    "    # setup LoRA\n",
    "    lora_cfg = LoraConfig(\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "        r=cfg.lora.r,\n",
    "        lora_alpha=cfg.lora.alpha,\n",
    "        target_modules=cfg.lora.target_modules,\n",
    "        lora_dropout=cfg.lora.dropout,\n",
    "        bias=cfg.lora.bias\n",
    "    )\n",
    "\n",
    "    logger.info(f\"LoRA config: {lora_cfg}\")\n",
    "    logger.info(f\"LoRA targeting modules: {lora_cfg.target_modules}\")\n",
    "\n",
    "    # Apply LoRA\n",
    "    logger.info(\"Applying LoRA to the model...\")\n",
    "    peft_model = get_peft_model(model, lora_cfg)\n",
    "    logger.info(\"LoRA applied successfully.\")\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\n",
    "    \n",
    "    logger.info(f\"Total parameters: {total_params:,}\")\n",
    "    logger.info(f\"Trainable parameters: {trainable_params:,} ({trainable_params / total_params * 100:.2f}%)\") \n",
    "\n",
    "    return tokenizer, peft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569c8c79",
   "metadata": {},
   "source": [
    "## Preprocessing Function\n",
    "The `preprocess_datasets` function handles:\n",
    "- Tokenization of input texts and summaries\n",
    "- Proper padding and truncation\n",
    "- Handling of special tokens\n",
    "- Batch processing for efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bde6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_datasets(tokenizer, datasets, cfg):\n",
    "    \"\"\"\n",
    "    Preprocess datasets by tokenizing inputs and targets.\n",
    "    Args:\n",
    "        tokenizer: HuggingFace tokenizer\n",
    "        datasets: Dictionary containing train and eval datasets\n",
    "        cfg: Configuration object\n",
    "    Returns:\n",
    "        trainer: Trained Seq2SeqTrainer object\n",
    "    \"\"\"\n",
    "    # Setup output directory with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    if cfg.add_timestamp_to_output:\n",
    "        cfg.output_dir = f\"{cfg.output_dir}_{timestamp}\"\n",
    "    os.makedirs(cfg.output_dir, exist_ok=True)\n",
    "\n",
    "    # Save configuration\n",
    "    config_path = os.path.join(cfg.output_dir, 'config.yaml')\n",
    "    OmegaConf.save(cfg, config_path)\n",
    "    logger.info(f\"Configuration saved to {config_path}\")\n",
    "\n",
    "    # Initialize accelerator\n",
    "    accelerator = Accelerator()\n",
    "    logger.info(f\"Using accelerator: {accelerator.device}\")\n",
    "    print_memory_usage()\n",
    "\n",
    "    # Load and prepare data\n",
    "    logger.info(\"Loading and preparing datasets...\")\n",
    "    datasets = load_and_prepare_dataset(cfg)\n",
    "    tokenizer, peft_model = setup_model_and_tokenizer(cfg)\n",
    "    processed_datasets = preprocess_datasets(tokenizer, datasets, cfg)\n",
    "\n",
    "    # Setup data collator\n",
    "    logger.info(\"Setting up data collator...\")\n",
    "    collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer=tokenizer,\n",
    "        model=peft_model,\n",
    "        padding='longest',\n",
    "        pad_to_multiple_of=8,\n",
    "            max_length=cfg.dataset.max_input_tokens\n",
    "    )\n",
    "\n",
    "    # Setup ROUGE metrics\n",
    "    logger.info(\"Setting up evaluation metrics...\")\n",
    "    rouge = evaluate.load('rouge')\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        \"\"\"Compute ROUGE metrics for evaluation.\"\"\"\n",
    "        preds, labels = eval_pred\n",
    "        \n",
    "        # Handle padding in labels\n",
    "        decoded_labels = []\n",
    "        for label_seq in labels:\n",
    "            label_seq = [l if l != -100 else tokenizer.pad_token_id for l in label_seq]\n",
    "            decoded_labels.append(tokenizer.decode(label_seq, skip_special_tokens=True).strip())\n",
    "        \n",
    "        # Decode predictions\n",
    "        decoded_preds = [tokenizer.decode(pred, skip_special_tokens=True).strip() \n",
    "                        for pred in preds]\n",
    "        \n",
    "        # Compute ROUGE scores\n",
    "        res = rouge.compute(\n",
    "            predictions=decoded_preds, \n",
    "            references=decoded_labels, \n",
    "            use_stemmer=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'rougeL_f1': res['rougeL'].mid.fmeasure,\n",
    "            'rouge1_f1': res['rouge1'].mid.fmeasure,\n",
    "            'rouge2_f1': res['rouge2'].mid.fmeasure,\n",
    "        }\n",
    "\n",
    "    # Setup training arguments\n",
    "    logger.info(\"Configuring training arguments...\")\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=cfg.output_dir,\n",
    "        per_device_train_batch_size=cfg.training.batch_size,\n",
    "        per_device_eval_batch_size=cfg.training.eval_batch_size,\n",
    "        gradient_accumulation_steps=cfg.training.grad_accum_steps,\n",
    "        learning_rate=cfg.training.lr,\n",
    "        num_train_epochs=cfg.training.epochs,\n",
    "        weight_decay=cfg.training.weight_decay,\n",
    "        fp16=cfg.training.fp16,\n",
    "        logging_steps=cfg.training.logging_steps,\n",
    "        save_strategy=cfg.training.save_strategy,\n",
    "        save_steps=cfg.training.save_steps,\n",
    "        evaluation_strategy=cfg.training.evaluation_strategy,\n",
    "        eval_steps=cfg.training.eval_steps,\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=cfg.dataset.max_target_tokens,\n",
    "        load_best_model_at_end=cfg.training.load_best_model_at_end,\n",
    "        metric_for_best_model=cfg.training.metric_for_best,\n",
    "        greater_is_better=cfg.training.greater_is_better,\n",
    "        warmup_steps=cfg.training.warmup_steps,\n",
    "        report_to=cfg.training.report_to,\n",
    "        push_to_hub=False,\n",
    "        gradient_checkpointing=cfg.training.gradient_checkpointing,\n",
    "        label_names=[\"labels\"]\n",
    "    )\n",
    "\n",
    "    # Setup callbacks\n",
    "    logger.info(\"Setting up training callbacks...\")\n",
    "    callbacks = [\n",
    "        RogerReportCallback(cfg.output_dir, cfg),\n",
    "        MetricsTrackingCallback(cfg.output_dir)\n",
    "    ]\n",
    "\n",
    "    # Initialize trainer\n",
    "    logger.info(\"Initializing trainer...\")\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=peft_model,\n",
    "        args=training_args,\n",
    "        train_dataset=processed_datasets[\"train\"],\n",
    "        eval_dataset=processed_datasets[\"eval\"],\n",
    "        data_collator=collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    logger.info(\"Starting training...\")\n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    # Save final model\n",
    "    logger.info(\"Saving final model...\")\n",
    "    trainer.save_model(cfg.output_dir)\n",
    "    \n",
    "    # Log and save training results\n",
    "    metrics = train_result.metrics\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    \n",
    "    # Final evaluation\n",
    "    logger.info(\"Running final evaluation...\")\n",
    "    eval_metrics = trainer.evaluate(\n",
    "        max_length=cfg.dataset.max_target_tokens,\n",
    "        num_beams=cfg.generation.num_beams,\n",
    "        metric_key_prefix=\"eval\"\n",
    "    )\n",
    "    trainer.log_metrics(\"eval\", eval_metrics)\n",
    "    trainer.save_metrics(\"eval\", eval_metrics)\n",
    "    \n",
    "    logger.info(\"Training completed successfully!\")\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b845bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(cfg):\n",
    "    \"\"\"\n",
    "    Main training function that orchestrates the fine-tuning process.\n",
    "    Args:\n",
    "        cfg: Configuration object containing all training parameters\n",
    "    Returns:\n",
    "        trainer: Trained Seq2SeqTrainer object\n",
    "    \"\"\"\n",
    "    # Setup output directory with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    if cfg.add_timestamp_to_output:\n",
    "        cfg.output_dir = f\"{cfg.output_dir}_{timestamp}\"\n",
    "    os.makedirs(cfg.output_dir, exist_ok=True)\n",
    "\n",
    "    # Save configuration\n",
    "    config_path = os.path.join(cfg.output_dir, 'config.yaml')\n",
    "    OmegaConf.save(cfg, config_path)\n",
    "    logger.info(f\"Configuration saved to {config_path}\")\n",
    "\n",
    "    # Initialize accelerator\n",
    "    accelerator = Accelerator()\n",
    "    logger.info(f\"Using accelerator: {accelerator.device}\")\n",
    "    print_memory_usage()\n",
    "\n",
    "    # Load and prepare data\n",
    "    logger.info(\"Loading and preparing datasets...\")\n",
    "    datasets = load_and_prepare_dataset(cfg)\n",
    "    tokenizer, peft_model = setup_model_and_tokenizer(cfg)\n",
    "    processed_datasets = preprocess_datasets(tokenizer, datasets, cfg)\n",
    "\n",
    "    # Setup data collator\n",
    "    logger.info(\"Setting up data collator...\")\n",
    "    collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer=tokenizer,\n",
    "        model=peft_model,\n",
    "        padding='longest',\n",
    "        pad_to_multiple_of=8,\n",
    "        max_length=cfg.dataset.max_input_tokens\n",
    "    )\n",
    "\n",
    "    # Setup ROUGE metrics\n",
    "    logger.info(\"Setting up evaluation metrics...\")\n",
    "    rouge = evaluate.load('rouge')\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        \"\"\"Compute ROUGE metrics for evaluation.\"\"\"\n",
    "        preds, labels = eval_pred\n",
    "        \n",
    "        # Handle padding in labels\n",
    "        decoded_labels = []\n",
    "        for label_seq in labels:\n",
    "            label_seq = [l if l != -100 else tokenizer.pad_token_id for l in label_seq]\n",
    "            decoded_labels.append(tokenizer.decode(label_seq, skip_special_tokens=True).strip())\n",
    "        \n",
    "        # Decode predictions\n",
    "        decoded_preds = [tokenizer.decode(pred, skip_special_tokens=True).strip() \n",
    "                        for pred in preds]\n",
    "        \n",
    "        # Compute ROUGE scores\n",
    "        res = rouge.compute(\n",
    "            predictions=decoded_preds, \n",
    "            references=decoded_labels, \n",
    "            use_stemmer=True\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'rougeL_f1': res['rougeL'].mid.fmeasure,\n",
    "            'rouge1_f1': res['rouge1'].mid.fmeasure,\n",
    "            'rouge2_f1': res['rouge2'].mid.fmeasure,\n",
    "        }\n",
    "\n",
    "    # Setup training arguments\n",
    "    logger.info(\"Configuring training arguments...\")\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=cfg.output_dir,\n",
    "        per_device_train_batch_size=cfg.training.batch_size,\n",
    "        per_device_eval_batch_size=cfg.training.eval_batch_size,\n",
    "        gradient_accumulation_steps=cfg.training.grad_accum_steps,\n",
    "        learning_rate=cfg.training.lr,\n",
    "        num_train_epochs=cfg.training.epochs,\n",
    "        weight_decay=cfg.training.weight_decay,\n",
    "        fp16=cfg.training.fp16,\n",
    "        logging_steps=cfg.training.logging_steps,\n",
    "        save_strategy=cfg.training.save_strategy,\n",
    "        save_steps=cfg.training.save_steps,\n",
    "        evaluation_strategy=cfg.training.evaluation_strategy,\n",
    "        eval_steps=cfg.training.eval_steps,\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=cfg.dataset.max_target_tokens,\n",
    "        load_best_model_at_end=cfg.training.load_best_model_at_end,\n",
    "        metric_for_best_model=cfg.training.metric_for_best,\n",
    "        greater_is_better=cfg.training.greater_is_better,\n",
    "        warmup_steps=cfg.training.warmup_steps,\n",
    "        report_to=cfg.training.report_to,\n",
    "        push_to_hub=False,\n",
    "        gradient_checkpointing=cfg.training.gradient_checkpointing,\n",
    "        label_names=[\"labels\"]\n",
    "    )\n",
    "\n",
    "    # Setup callbacks\n",
    "    logger.info(\"Setting up training callbacks...\")\n",
    "    callbacks = [\n",
    "        RogerReportCallback(cfg.output_dir, cfg),\n",
    "        MetricsTrackingCallback(cfg.output_dir)\n",
    "    ]\n",
    "\n",
    "    # Initialize trainer\n",
    "    logger.info(\"Initializing trainer...\")\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=peft_model,\n",
    "        args=training_args,\n",
    "        train_dataset=processed_datasets[\"train\"],\n",
    "        eval_dataset=processed_datasets[\"eval\"],\n",
    "        data_collator=collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    logger.info(\"Starting training...\")\n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    # Save final model\n",
    "    logger.info(\"Saving final model...\")\n",
    "    trainer.save_model(cfg.output_dir)\n",
    "    \n",
    "    # Log and save training results\n",
    "    metrics = train_result.metrics\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    \n",
    "    # Final evaluation\n",
    "    logger.info(\"Running final evaluation...\")\n",
    "    eval_metrics = trainer.evaluate(\n",
    "        max_length=cfg.dataset.max_target_tokens,\n",
    "        num_beams=cfg.generation.num_beams,\n",
    "        metric_key_prefix=\"eval\"\n",
    "    )\n",
    "    trainer.log_metrics(\"eval\", eval_metrics)\n",
    "    trainer.save_metrics(\"eval\", eval_metrics)\n",
    "    \n",
    "    logger.info(\"Training completed successfully!\")\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5187be",
   "metadata": {},
   "source": [
    "## Main Execution\n",
    "Load the configuration and start the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc147d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-15 18:04:43,947 - INFO - ✅ Loaded configuration from ../configs/billsum.yaml\n",
      "2025-05-15 18:04:43,957 - INFO - 📦 Model: google/flan-t5-base\n",
      "2025-05-15 18:04:43,958 - INFO - 📊 Dataset: billsum\n",
      "2025-05-15 18:04:43,959 - INFO - 📁 Output dir: lora_billsum\n",
      "2025-05-15 18:04:43,959 - INFO - 📈 Epochs: 1\n",
      "2025-05-15 18:04:43,962 - INFO - Configuration saved to lora_billsum_20250515_180443/config.yaml\n",
      "2025-05-15 18:04:44,135 - INFO - Using accelerator: mps\n",
      "2025-05-15 18:04:44,136 - INFO - RAM usage: 0.06 GB\n",
      "2025-05-15 18:04:44,136 - INFO - Total memory: 8.59 GB\n",
      "2025-05-15 18:04:44,137 - INFO - Loading and preparing datasets...\n",
      "2025-05-15 18:04:44,137 - ERROR - ❌ Training failed: name '__file__' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'traceback' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 20\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# 4. Launch training\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🏁 Training completed successfully! Artifacts at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[21], line 27\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     26\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading and preparing datasets...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m datasets \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_prepare_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m tokenizer, peft_model \u001b[38;5;241m=\u001b[39m setup_model_and_tokenizer(cfg)\n",
      "Cell \u001b[0;32mIn[20], line 16\u001b[0m, in \u001b[0;36mload_and_prepare_dataset\u001b[0;34m(cfg, tokenizer)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Check cache first\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m cache_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;18;43m__file__\u001b[39;49m)), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcache\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(cache_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     28\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m❌ Training failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m     traceback_str \u001b[38;5;241m=\u001b[39m \u001b[43mtraceback\u001b[49m\u001b[38;5;241m.\u001b[39mformat_exc()\n\u001b[1;32m     30\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📉 Full traceback:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtraceback_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m     sys\u001b[38;5;241m.\u001b[39mexit(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'traceback' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    config_path = \"../configs/billsum.yaml\"\n",
    "    \n",
    "    try:\n",
    "        # 1. Load configuration\n",
    "        cfg = OmegaConf.load(config_path)\n",
    "        logger.info(f\"✅ Loaded configuration from {config_path}\")\n",
    "        logger.debug(f\"🔧 Full config:\\n{OmegaConf.to_yaml(cfg)}\")\n",
    "\n",
    "        # 2. Log core info\n",
    "        logger.info(f\"📦 Model: {cfg.model.name}\")\n",
    "        logger.info(f\"📊 Dataset: {cfg.dataset.name}\")\n",
    "        logger.info(f\"📁 Output dir: {cfg.output_dir}\")\n",
    "        logger.info(f\"📈 Epochs: {cfg.training.epochs}\")\n",
    "        \n",
    "        # 3. Create output directory early\n",
    "        os.makedirs(cfg.output_dir, exist_ok=True)\n",
    "\n",
    "        # 4. Launch training\n",
    "        output_dir = train(cfg)\n",
    "        logger.info(f\"🏁 Training completed successfully! Artifacts at: {output_dir}\")\n",
    "\n",
    "        # 5. Final memory log\n",
    "        print_memory_usage()\n",
    "        sys.exit(0)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Training failed: {str(e)}\")\n",
    "        traceback_str = traceback.format_exc()\n",
    "        logger.error(f\"📉 Full traceback:\\n{traceback_str}\")\n",
    "        sys.exit(1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
